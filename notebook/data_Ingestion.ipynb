{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb240b6",
   "metadata": {},
   "source": [
    "## RAG pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd8f0c",
   "metadata": {},
   "source": [
    "#### 1- Data ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9074edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895fbc1",
   "metadata": {},
   "source": [
    "Reading the pdf files inside the folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0723286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pdf files found: 3 files\n",
      "processing the file:bgDataFundamentals.pdf\n",
      "loaded 20 pages\n",
      "processing the file:kafka.pdf\n",
      "loaded 50 pages\n",
      "processing the file:rapport_de_stage.pdf\n",
      "loaded 26 pages\n",
      "Total of loaded documents:96\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    all_documents = []\n",
    "    pdf_dir_path=Path(pdf_directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir_path.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"number of pdf files found: {len(pdf_files)} files\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"processing the file:{pdf_file.name}\")\n",
    "        try:\n",
    "            loader=PyPDFLoader(str(pdf_file))\n",
    "            documents=loader.load()\n",
    "\n",
    "            #adding source info to metadata:\n",
    "            for document in documents:\n",
    "                document.metadata['source_file'] = pdf_file.name\n",
    "                document.metadata['file_type']='pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"loaded {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading files:{e}\")\n",
    "    print(f\"Total of loaded documents:{len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# process all pdf files inside data folder\n",
    "all_pdf_docs=process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2924b12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Mohamed El Marouani\\nTDIA 2\\nLes fondements du Big Data'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='2\\n1. Qu’est ce que les données?\\n2. Types des données\\n3. Impact des données\\n4. Caractéristiques des données (les V)\\n5. Data Journey\\n6. Qu’est ce que Big Data?\\n7. Big Data: cas d’utilisation\\n8. Evolution du Big Data\\n9. Paysage du Big Data'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 2, 'page_label': '3', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Qu’est ce que les données ?\\n\\uf0a7 Les données (Data) sont toutes les informations que vous collectez et \\nqui ont été organisées et structurées de manière à pouvoir être \\nanalysées. \\n« Data are a collection of discrete or continuous values that convey \\ninformation, describing the quantity, quality, fact, statistics, other basic \\nunits of meaning, or simply sequences of symbols that may be further \\ninterpreted formally “ - Wikipedia\\n\\uf0a7 Les données sont collectées à chaque fois que vous effectuez un \\nachat, que vous naviguez sur un site web, que vous voyagez, que \\nvous passez un appel téléphonique ou que vous publiez un message \\nsur un site de médias sociaux. \\n\\uf0a7 Les données peuvent provenir de nombreuses sources, notamment \\nde capteurs, d'enquêtes, d'expériences, d'observations ou \\nd'enregistrements existants (données historiques), comme les \\ntransactions financières. \\n 3\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Qu’est ce que les données ?\\n\\uf0a7 Les organisations modernes considèrent les données comme leur actif le plus précieux, car elles fournissent des \\ninformations sur le comportement des clients, les tendances du marché, les performances des produits, etc. qui \\naident à prendre des décisions éclairées sur l'affectation des ressources.\\n\\uf0a7 La théorie de l'information a poussé le concept de données beaucoup plus loin (Shannon, 1948). La théorie de \\nl'information est un domaine d'étude qui cherche à comprendre la nature et l'origine de l'information et, selon \\ncette étude, tout peut être considéré comme des données. Cela inclut les objets physiques et les concepts \\nabstraits tels que les idées ou les émotions. En outre, les données sont définies comme tout ensemble de symboles \\nqui transmettent un sens lorsqu'ils sont interprétés par un récepteur. Par conséquent, tout ce qui a une forme de \\nreprésentation symbolique (par exemple, des séquences d'ADN, des mots, des nombres) peut être classé comme \\ndonnées dans ce contexte.\\n4\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 4, 'page_label': '5', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Types des données\\nD'un point de vue purement statistique, les données peuvent être classées en deux grandes catégories en fonction \\nde leur valeur :\\n\\uf0a7 Les données quantitatives (numériques) : il s'agit de toute information qui peut être exprimée, mesurée et \\ncomparée à l'aide de valeurs numériques, telles que des nombres entiers ou des nombres réels. Parmi les exemples \\nde données quantitatives, on peut citer la taille, le poids, la longueur, les relevés de température, la taille d'une \\npopulation ou des éléments dénombrables tels que le nombre d'élèves dans une salle de classe. \\nCe type de données peut être subdivisé en données discrètes (nombres entiers) ou continues (décimales).\\n• Données continues : données quantitatives qui peuvent être divisées de manière significative en niveaux plus fins. Elles peuvent être \\nmesurées sur une échelle ou un continuum. Elles peuvent avoir presque n'importe quelle valeur numérique : n'importe quelle va leur \\ndans un intervalle fini ou infini (intervalle) ou une valeur qui compare deux nombres ou plus (rapport). Les exemples incluen t la taille, le \\npoids, la température, la vitesse, l'IMC et le temps.\\n• Données discrètes : consistent en des valeurs finies, numériques et dénombrables. Les valeurs discrètes ne peuvent pas être divisées en \\nparties. Les variables discrètes comprennent les dénombrements (par exemple, le nombre d'enfants dans un ménage), le nombre t otal \\nde produits ou les indicateurs binaires (oui/non, vrai/faux).\\n5\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 5, 'page_label': '6', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Types des données\\n\\uf0a7 Données qualitatives (catégorielles) : il s'agit d'informations non numériques \\ntelles que les opinions, les sentiments, les perceptions et les attitudes. Ces \\ndonnées peuvent répondre à des questions telles que : « Comment cela \\ns'est-il produit ? » ou “Pourquoi cela s'est-il produit ?”. Parmi les exemples de \\ndonnées qualitatives, on peut citer le sexe, les classements, les \\ndénombrements, etc. Ce type de données peut être divisé en deux \\ncatégories : les données nominales et les données ordinales.\\n• Données nominales : un type de données catégoriques qui n'a pas de valeur \\nnumérique ou d'ordre. Il s'agit de noms, d'étiquettes ou de catégories qui classent et \\norganisent les informations en groupes distincts. Les exemples incluent le sexe \\n(homme/femme), la nationalité (marocain/français) et les couleurs (vert/bleu).\\n• Données ordinales : ce type de données est associé à un ordre ou à un classement. \\nLes exemples incluent les classements tels que 1er, 2ème et 3ème ; les notes telles que \\nA+, B- et C/D ; et les notes élevées, moyennes et basses.\\n6\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 6, 'page_label': '7', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Impact des données\\n7\\nLe modèle DIKW décrit la relation entre les données, \\nl'information, la connaissance et la sagesse.\\n\\uf0a7 Données / Data :\\n• Considérées comme la matière première d'une prise de \\ndécision avisée.\\n• Fournissent une base objective pour tirer des conclusions \\nou prendre des décisions.\\n\\uf0a7 Information / Information :\\n• Issue de l'analyse des données à l'aide de méthodes \\ncomme les statistiques ou l'apprentissage automatique.\\n• Permet de découvrir des schémas auparavant non \\névidents.\\n\\uf0a7 Connaissance / Knowledge :\\n• Résulte de la transformation des informations en savoir \\nstructuré.\\n• Sert de base aux processus de prise de décision.\\n\\uf0a7 Sagesse / Wisdom :\\n• Implique l'application des connaissances avec \\nexpérience et jugement.\\n• Permet de prendre des décisions éclairées sur les \\nstratégies et actions futures.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 7, 'page_label': '8', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Caractéristiques des données (les V) \\nLes cinq caractéristiques principales et innées des données sont :\\n\\uf0a7 Volume : La quantité de données qu'une organisation génère et stocke.\\n\\uf0a7 Vélocité : la vitesse à laquelle les données sont générées et la vitesse à laquelle elles \\nse déplacent et peuvent être traitées pour en tirer des informations exploitables.\\n\\uf0a7 Variété : la diversité des données. Les organisations peuvent collecter des données à \\npartir de sources multiples, dont le format peut varier. Les données collectées \\npeuvent être structurées, semi-structurées ou non structurées.\\n\\uf0a7 Véracité : fait référence au niveau de confiance et de fiabilité des données \\ncollectées. En d'autres termes, il s'agit de la qualité et de l'exactitude des données. \\nLes données collectées peuvent comporter des éléments manquants, être inexactes \\nou ne pas être en mesure de fournir une valeur réelle.\\n\\uf0a7 Valeur : se réfère à la valeur que les données peuvent apporter sur ce que les \\norganisations peuvent en faire. Cette caractéristique est directement liée à la \\nsignification et au contexte qu'une organisation peut donner aux données collectées.\\n8\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 8, 'page_label': '9', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Caractéristiques des données (les V) \\nDans le domaine du marketing, les experts en la matière ont commencé à utiliser deux caractéristiques \\nsupplémentaires qui ne sont pas innées aux données mais qui peuvent avoir un impact significatif sur les informations \\ngénérées à partir de celles-ci. Ces deux caractéristiques sont les suivantes\\n\\uf0a7 Variabilité : une mesure de la variation des valeurs dans chaque variante de données. Ce concept est lié au contexte des données \\net à la signification qui leur est donnée. Dans une organisation, la signification peut changer constamment, ce qui a un impa ct \\nsignificatif sur l'homogénéisation des données. Ce concept diffère de celui de variété : Imaginez un café qui propose six mél anges \\nde café différents (c'est la variété), mais si vous prenez le même mélange tous les jours. Il a un goût différent chaque jour ; c'est la \\nvariabilité.\\n\\uf0a7 Visualisation : La visualisation est essentielle dans le monde d'aujourd'hui. L'utilisation de tableaux et de graphiques pour visualiser de \\ngrandes quantités de données complexes est beaucoup plus efficace pour transmettre du sens que des données brutes dans des \\nfeuilles de calcul remplies de chiffres et de formules.\\n9\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 9, 'page_label': '10', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Data Journey\\nData Journey (parcours des données) se fait en plusieurs étapes. Les principales sont l'ingestion, le stockage, le traitement et la distribution. \\nChaque étape comporte son propre ensemble d'activités et de considérations. Le parcours des données comporte également une no tion \\nd'activités « sous-jacentes », c'est-à-dire des activités critiques tout au long du cycle de vie. Il s'agit notamment de la sécurité, de la gestion \\ndes données, du DataOps, de l'orchestration et de l'ingénierie logicielle.\\n10\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 10, 'page_label': '11', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Data Journey\\n1 - L'ingestion des données\\nL'ingestion des données est la première étape du cycle de vie des données. C'est à ce stade que les données \\nsont collectées à partir de diverses sources internes telles que les bases de données, les systèmes de gestion de la \\nrelation client (CRM), les systèmes d'information de gestion (ERP), les systèmes existants, les sources externes telles \\nque les enquêtes et les fournisseurs tiers. Il est important de s'assurer que les données acquises sont exactes et à \\njour afin de pouvoir les utiliser efficacement dans les étapes suivantes du cycle.\\nÀ ce stade, les données brutes sont extraites d'une ou de plusieurs sources de données, répliquées, puis intégrées \\ndans un support de stockage d'atterrissage. Ensuite, vous devez prendre en compte les caractéristiques des \\ndonnées que vous souhaitez acquérir pour vous assurer que l'étape d'ingestion des données dispose de la \\ntechnologie et des processus adéquats pour atteindre ses objectifs.\\n11\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 11, 'page_label': '12', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Data Journey\\n2 - Stockage des données\\nLe stockage des données désigne la manière dont les informations sont conservées après leur acquisition. Il \\nrepose sur des plateformes sécurisées et fiables, intégrant des mécanismes de sauvegarde essentiels à la reprise \\naprès sinistre. Par ailleurs, des contrôles d'accès stricts doivent être mis en place afin de protéger les données \\nsensibles contre toute tentative d’accès non autorisé ou malveillant.\\nLe choix d’une solution de stockage est une étape déterminante du cycle de vie des données, bien qu’il s’agisse \\nd’un processus complexe influencé par plusieurs facteurs. Parmi les principales caractéristiques du stockage, on \\ndistingue :\\n\\uf0a7 Le cycle de vie des données : la manière dont elles évoluent au fil du temps.\\n\\uf0a7 Les options de stockage : les différentes méthodes permettant d’optimiser leur conservation.\\n\\uf0a7 Les couches de stockage : la structuration des données en fonction de leur importance et de leur \\naccessibilité.\\n\\uf0a7 Les formats de stockage : le mode d’organisation des données selon leur fréquence d’accès.\\n\\uf0a7 Les technologies de stockage : les infrastructures sur lesquelles reposent les données.\\nBien que le stockage constitue une phase distincte du parcours des données, il s’intègre étroitement aux autres \\nétapes clés, telles que l’ingestion, la transformation et la mise à disposition des données. Il intervient à différents \\npoints du pipeline de traitement, se connectant aux systèmes sources et influençant la manière dont les données \\nsont exploitées à chaque phase. Ainsi, la stratégie de stockage adoptée a un impact direct sur l’efficacité \\nglobale du cycle de vie des données.\\n12\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 12, 'page_label': '13', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Data Journey\\n3 - Traitement des données\\nUne fois les données saisies et stockées, elles doivent être exploitées pour devenir véritablement utiles. L’étape suivante \\ndu cycle de vie des données est la transformation, qui consiste à convertir les données brutes en informations \\nexploitables pour les différents cas d’utilisation en aval.\\nLe traitement des données repose sur une série de transformations de base, essentielles pour garantir la cohérence et \\nl’exactitude des informations. Ces transformations incluent :\\n\\uf0a7 La conversion des types de données : transformation des chaînes de caractères (dates, valeurs numériques) en \\ntypes de données adaptés.\\n\\uf0a7 La normalisation des enregistrements : harmonisation des formats et structuration des données.\\n\\uf0a7 L’élimination des erreurs : suppression des entrées incorrectes ou incohérentes.\\nÀ mesure que le pipeline de traitement progresse, des transformations plus avancées peuvent être nécessaires, telles \\nque :\\n\\uf0a7 L’adaptation ou la normalisation du schéma des données, pour assurer une compatibilité avec les systèmes en \\naval.\\n\\uf0a7 L’agrégation de données à grande échelle, notamment pour les besoins de reporting.\\n\\uf0a7 La transformation des données en vecteurs de caractéristiques (embeddings), pour les intégrer à des modèles \\nd’apprentissage automatique.\\nL’un des principaux défis de cette phase réside dans la précision et l’efficacité du traitement, qui nécessite une \\npuissance de calcul importante. Sans stratégies d’optimisation adéquates, ce processus peut s’avérer coûteux à long \\nterme. Ainsi, une gestion efficace des ressources et des performances est essentielle pour garantir un traitement rapide \\net fiable des données.\\n13'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 13, 'page_label': '14', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Data Journey\\n4 - Servir les données\\nVous avez atteint la dernière étape du parcours des données. Après avoir été ingérées, stockées et \\ntransformées en structures cohérentes et exploitables, il est temps d’en extraire toute la valeur.\\nLe service de données est l’étape où les informations prennent tout leur sens. C’est ici que les ingénieurs BI, \\nles ingénieurs en machine learning et les data scientists appliquent des techniques avancées pour générer \\ndes insights pertinents. Parmi les approches les plus courantes, on retrouve :\\n\\uf0a7 L’analyse des données : interprétation et exploration des informations stockées afin d’orienter la \\nprise de décision et d’anticiper les tendances futures.\\n\\uf0a7 La visualisation des données : utilisation d’outils spécialisés pour représenter graphiquement les \\nrésultats et faciliter leur compréhension.\\nCette phase constitue l’aboutissement du cycle de vie des données, permettant de transformer des \\nensembles bruts en ressources stratégiques et exploitables pour l’entreprise.\\n14'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 14, 'page_label': '15', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Qu’est ce que Big Data?\\nLe Big Data, ou données massives, désigne des ensembles de données \\nsi volumineux, variés et générés à une telle vitesse qu\\'ils dépassent les \\ncapacités des outils traditionnels de gestion et d\\'analyse de données. \\nCes caractéristiques sont souvent résumées par les \"3V\" :\\n\\uf0a7 Volume : Quantité massive de données générées.\\n\\uf0a7 Variété : Diversité des types de données (structurées, non \\nstructurées, semi-structurées).\\n\\uf0a7 Vélocité : Vitesse à laquelle ces données sont produites et \\ndoivent être traitées.\\nCertaines définitions ajoutent deux autres \"V\" :\\n\\uf0a7 Véracité : Fiabilité et qualité des données.\\n\\uf0a7 Valeur : Potentiel des données à générer des informations utiles.\\n15'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 15, 'page_label': '16', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Big Data: cas d’utilisations\\nSecteur financier (Détection de la fraude)\\n\\uf0a7 Descriptif : Les institutions financières analysent les comportements de transactions pour détecter et prévenir les \\nfraudes en temps réel.\\n\\uf0a7 Solution :\\n• Analyse des modèles de transactions avec des algorithmes de détection d'anomalies.\\n• Utilisation du Machine Learning pour reconnaître des comportements suspects.\\n• Mise en place de systèmes de scoring en temps réel (ex. systèmes de scoring de carte bancaire).\\n\\uf0a7 Challenges :\\n• Faux positifs qui peuvent impacter l’expérience client.\\n• Besoin de traitements en temps réel pour bloquer rapidement les fraudes.\\n• Complexité liée aux volumes de données générés par les transactions globales\\n16\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 16, 'page_label': '17', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Big Data: cas d’utilisations\\nSmart Cities (Gestion intelligente du trafic urbain)\\n\\uf0a7 Descriptif : Les villes intelligentes utilisent le Big Data pour optimiser la circulation et réduire les embouteillages.\\n\\uf0a7 Solution :\\n• Analyse des flux de circulation en temps réel à partir des capteurs et caméras.\\n• Modélisation des itinéraires optimaux en fonction des conditions actuelles.\\n• Intégration avec des applications mobiles (ex. Google Maps).\\n\\uf0a7 Challenges :\\n• Traitement et synchronisation de données en temps réel provenant de différentes sources.\\n• Sécurité et protection contre les cyberattaques.\\n• Acceptation par les citoyens et respect de leur vie privée.\\n17'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 17, 'page_label': '18', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Big Data: cas d’utilisations\\nSecteur de la santé (Prédiction des maladies et personnalisation des traitements)\\n\\uf0a7 Descriptif : L'analyse de grandes quantités de données médicales (dossiers patients, imageries médicales, \\ndonnées génétiques) permet de détecter des tendances et de proposer des traitements personnalisés.\\n\\uf0a7 Solution :\\n• Utilisation d'algorithmes d'apprentissage automatique pour identifier des modèles dans les données de \\nsanté.\\n• Intégration de données issues de capteurs portables (montres connectées, bracelets de santé).\\n• Plateformes de stockage et de traitement cloud (ex. AWS, Google Cloud Healthcare).\\n\\uf0a7 Challenges :\\n• Protection des données sensibles et conformité avec les réglementations (RGPD, HIPAA).\\n• Interopérabilité des différents systèmes hospitaliers.\\n• Fiabilité des algorithmes et des recommandations médicales. 18\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Evolution du Big Data\\nBien que le concept de Big Data soit relativement nouveau, la nécessité de gérer des jeux de données volumineux \\nremonte aux années 1960 et 70, avec les premiers data centers et le développement des bases de données \\nrelationnelles.\\n\\uf0a7 Passé: En 2005, on assista à une prise de conscience de la quantité de données que les utilisateurs généraient sur \\nFacebook, YouTube et autres services en ligne. Apache Hadoop, une infrastructure open source créée \\nspécifiquement pour stocker et analyser de grands jeux de données, fut développé cette même année. NoSQL\\ncommença également à être de plus en plus utilisé à cette époque.\\n\\uf0a7 Présent: Le développement d’infrastructures open source, telles qu'Apache Hadoop et, plus récemment, Apache \\nSpark, a été primordial pour la croissance du Big Data, car celles-ci facilitent l’utilisation du Big Data et réduisent les \\ncoûts de stockage. Depuis, le volume du Big Data a explosé. Les utilisateurs génèrent toujours d’énormes quantités \\nde données, mais ce ne sont pas seulement les humains qui les utilisent.\\nAvec l’avènement de l’Internet of Things (IoT), de plus en plus d’objets et de terminaux sont connectés à Internet, \\ncollectant des données sur les habitudes d’utilisation des clients et les performances des produits. L’émergence \\ndu Machine Learning a produit encore plus de données.\\n\\uf0a7 Futur: Alors que le Big Data a fait des progrès, sa valeur continue de croître à mesure que l'IA générative et \\nl'utilisation du Cloud Computing se développent dans les entreprises. Le cloud offre une évolutivité considérable, \\nles développeurs peuvent simplement faire fonctionner rapidement des clusters dédiés pour tester un sous-\\nensemble de données. En outre, les bases de données graphiques deviennent de plus en plus importantes, avec \\nleur capacité à afficher d'énormes quantités de données de manière à rendre les analyses rapides et complètes.\\n19\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 19, 'page_label': '20', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Le paysage Big Data\\n20'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 0, 'page_label': '1', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Mohamed El Marouani\\nTDIA 2\\nApache Kafka\\n1'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 1, 'page_label': '2', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Event Streaming\\n2\\nDans un monde de plus en plus connecté, les systèmes doivent traiter des flux continus de données en temps réel.\\nEvents Streaming est une approche qui permet de collecter, traiter et analyser des événements au fur et à \\nmesure qu’ils se produisent.\\nCette technologie est au cœur des architectures modernes réactives, scalables et orientées données, utilisée \\ndans des domaines comme :\\n\\uf0a7 Les plateformes de streaming vidéo. \\n\\uf0a7 La surveillance en cybersécurité.\\n\\uf0a7 Le traitement des transactions financières. \\n\\uf0a7 L’IoT et les objets connectés.\\nElle repose sur des solutions comme Apache Kafka, Pulsar ou Kinesis, et change notre manière de penser les \\nsystèmes : de statiques à dynamiques, de batch à stream.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 2, 'page_label': '3', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"Kafka: Introduction\\n3\\n\\uf0a7 Apache Kafka est un framework open-source distribué conçu pour le streaming d’événements à grande \\néchelle.\\n\\uf0a7 Initialement développé par LinkedIn en 2011, il est aujourd’hui un standard industriel pour la gestion de flux de \\ndonnées en temps réel.\\n\\uf0a7 Kafka réunit trois capacités clés dans une seule solution éprouvée :\\no Publier et consommer des flux d’événements, y compris l’import/export continu de données depuis \\nd'autres systèmes.\\no Stocker ces flux de manière fiable et durable, aussi longtemps que nécessaire.\\no Traiter les événements en temps réel ou a posteriori.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 3, 'page_label': '4', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Kafka: Introduction\\n4\\n\\uf0a7 Kafka offre une infrastructure distribuée, scalable, élastique, tolérante aux pannes et sécurisée.\\n\\uf0a7 Kafka peut être déployé :\\no sur des serveurs physiques, machines virtuelles ou conteneurs \\no en local (on-premise) ou dans le cloud \\no en mode autogéré ou via des services managés'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 4, 'page_label': '5', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Kafka APIs\\n5\\nKafka fournit plusieurs APIs permettant d’interagir avec les flux de données à chaque étape du pipeline :\\n\\uf0a7 Producer API\\no Permet d’envoyer des événements dans des topics Kafka.\\no Utilisé par les applications génératrices de données (ex. logs, capteurs IoT, applications web).\\n\\uf0a7 Consumer API\\no Permet de lire des événements depuis des topics.\\no Utilisé pour construire des systèmes de traitement ou de réaction aux événements.\\n\\uf0a7 Streams API\\no API de traitement intégré au client Kafka, orientée microservices.\\no Permet de transformer, agréger, filtrer et joindre des flux en temps réel.\\no Basée sur une logique déclarative avec une sémantique \"event-at-a-time\".'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 5, 'page_label': '6', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='6\\n\\uf0a7 Kafka Connect API\\no Facilite la connexion de Kafka à des bases de données, systèmes de fichiers, cloud services, etc.\\no Repose sur des connecteurs prêts à l’emploi (source ou sink).\\n\\uf0a7 Admin API\\no Permet de créer, configurer et gérer des topics, des quotas, etc.\\no Utile pour l’automatisation et la supervision de l’infrastructure Kafka.\\nKafka APIs'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 6, 'page_label': '7', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Kafka: Concepts et terminologie\\n7\\n\\uf0a7 Event\\no Unité de donnée transmise dans Kafka.\\no Composé généralement d’une clé, d’une valeur, d’un horodatage et de métadonnées.\\no Exemple : {\"user_id\": 123, \"action\": \"login\"}\\n\\uf0a7 Topic\\no Canal de communication nommé dans lequel les événements sont publiés.\\no Divisé en partitions pour la scalabilité et la parallélisation.\\no Les events sont ordonnés par partition.\\n\\uf0a7 Partition\\no Une sous-division d’un topic ; chaque partition contient une séquence ordonnée d’événements.\\no Permet le traitement parallèle et la montée en charge.\\n\\uf0a7 Offset\\no Identifiant unique de chaque événement dans une partition.\\no Utilisé pour garder la position de lecture du consommateur.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 7, 'page_label': '8', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='8\\n\\uf0a7 Le modèle Pub/Sub\\nLe modèle Pub/Sub est un style de communication asynchrone dans lequel les producteurs (appelés publishers) \\nenvoient des messages à un canal nommé (ex. un topic) sans se soucier de qui les recevra.\\nLes consommateurs (ou subscribers) s’abonnent à ces canaux pour recevoir les messages pertinents, de manière \\ndécouplée.\\nKafka implémente le modèle Pub/Sub de façon distribuée, avec persistance des messages et gestion avancée \\ndes offsets.\\nKafka: Concepts et terminologie'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 8, 'page_label': '9', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"Kafka: Architecture\\n9\\nKafka repose sur une architecture distribuée composée de deux grandes catégories d’acteurs :\\n\\uf0a7 Serveurs (Côté Cluster Kafka)\\no Brokers : Nœuds qui reçoivent, stockent et distribuent les événements.\\no Topics & Partitions : Les données sont organisées par sujets, découpés en partitions pour la scalabilité.\\no ZooKeeper / KRaft : Gère la coordination, l’équilibrage de charge et la tolérance aux pannes (ZooKeeper\\nremplacé progressivement par KRaft).\\n\\uf0a7 Clients (Côté Applications)\\no Producers : Publient des événements dans un topic Kafka.\\no Consumers : Lisent les événements d’un ou plusieurs topics.\\no Kafka Streams : Clients intelligents qui consomment, traitent et republient des flux.\\no Kafka Connect : Connecte automatiquement Kafka à des systèmes externes (BD, fichiers, cloud, etc.).\\nKafka permet de découpler producteurs et consommateurs, et d'assurer une haute disponibilité grâce à la \\nréplication et à une architecture résiliente.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 9, 'page_label': '10', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"10\\nCritère Apache Kafka RabbitMQ\\nModèle Distributed Log (log distribué \\nd'événements)\\nMessage Broker (file d'attente orientée \\nmessage)\\nPersistance Stocke les messages durablement sur \\ndisque (log) Messages stockés mais orienté queue, non log\\nRétention des messages Messages conservés pour une durée \\ndéfinie\\nSupprimés dès qu’ils sont consommés (par \\ndéfaut)\\nPerformance Très haut débit (millions de msg/sec) Débit plus limité, latence faible\\nScalabilité Conçu pour la scalabilité horizontale Scalabilité plus difficile sans plugins\\nModèle Pub/Sub Pub/Sub natif avec topics & partitions Pub/Sub via exchanges et bindings\\nCas d’usage typique Event streaming, ETL temps réel, traitement \\nde logs\\nFile d’attente, communication interservices \\n(RPC)\\nAPI & Clients APIs Java natives, Kafka Streams, Connect Clients AMQP dans plusieurs langages\\nÉcosystème Kafka Connect, Kafka Streams, KSQL, \\nConfluent, Flink... Plugins AMQP, Shovel, Federation, UI intégrée\\nGestion de l’état Suivi de l’offset côté consommateur Suivi de l’état géré par le broker\\nApache Kafka vs RabbitMQ\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 10, 'page_label': '11', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='11\\nKafka: Producers\\n\\uf0a7 Lorsqu’on envoie un message à Kafka via un Producer, on \\ncommence par créer un ProducerRecord. Ce record doit\\nobligatoirement contenir :\\no le topic cible,\\no une valeur (le message).\\n\\uf0a7 On peut aussi ajouter optionnellement :\\no une clé,\\no une partition spécifique,\\no un horodatage,\\no et/ou des en-têtes personnalisés.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 11, 'page_label': '12', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='12\\nKafka: Producers\\nÉtapes principales :\\n1. Sérialisation :\\n\\uf0a7 Kafka convertit la clé et la valeur en tableaux d’octets pour les transmettre via le réseau.\\n2. Choix de la partition :\\n\\uf0a7 Si aucune partition n’est définie, un partitionneur détermine automatiquement à quelle partition envoyer\\nle message (souvent basé sur la clé).\\n3. Batching :\\n\\uf0a7 Kafka groupe les messages destinés à la même partition dans un batch avant de les envoyer.\\n4. Transmission :\\n\\uf0a7 Un thread séparé se charge d’envoyer ces batches aux brokers Kafka.\\n5. Réponse du broker :\\n\\uf0a7 En cas de succès → le broker renvoie un objet RecordMetadata (contenant le topic, la partition et \\nl’offset).\\n\\uf0a7 En cas d’échec → le broker renvoie une erreur, et le producer peut effectuer des tentatives de renvoi\\n(retries) avant d’abandonner.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 12, 'page_label': '13', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='13\\nProducers: Construction\\nAvant d’envoyer des messages à Kafka, il faut créer un objet producer avec une configuration adaptée. Il existe trois \\npropriétés obligatoires :\\n1. bootstrap.servers\\n\\uf0a7 Liste d’adresses hôte:port des brokers Kafka pour établir une première connexion.\\n\\uf0a7 Il suffit d’en fournir une partie, car le producer découvre les autres après connexion.\\n\\uf0a7 Il est conseillé d’en mettre au moins deux pour assurer une tolérance aux pannes.\\n2. key.serializer\\n\\uf0a7 Classe responsable de sérialiser les clés des messages en byte arrays.\\n\\uf0a7 Kafka attend des tableaux d’octets, mais on peut envoyer n’importe quel objet Java.\\n\\uf0a7 Le producer doit donc savoir comment convertir l’objet clé.\\n\\uf0a7 Il faut indiquer une classe qui implémente l’interface Serializer de Kafka.\\n\\uf0a7 Exemples inclus : StringSerializer, IntegerSerializer, ByteArraySerializer, etc.\\n\\uf0a7 Même si vous n’envoyez pas de clé, vous devez quand même définir cette propriété (ex. avec VoidSerializer).'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 13, 'page_label': '14', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='14\\nProducers: Construction\\n3. value.serializer\\n\\uf0a7 Même principe que pour key.serializer, mais pour la valeur du message.\\n\\uf0a7 Permet de transformer l’objet à envoyer en byte array.\\n\\uf0a7 Doit être défini pour garantir que Kafka puisse comprendre les messages produits.\\nNote : Ces propriétés sont essentielles pour garantir la bonne communication entre l’application et le cluster Kafka.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 14, 'page_label': '15', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='15\\nProducers: Construction\\nUne fois le producer Kafka instancié, on peut envoyer des messages de trois façons principales :\\n1. Fire-and-Forget (Envoyer sans retour)\\n\\uf0a7 Le message est envoyé sans attendre de confirmation.\\n\\uf0a7 Simple et rapide, mais aucune garantie de livraison en cas d’erreur non-récupérable ou de timeout.\\n\\uf0a7 Kafka est très disponible, donc cela fonctionne bien dans la majorité des cas.\\n\\uf0a7 Les erreurs ne sont pas remontées à l’application.\\n2. Synchronous Send (Envoi synchrone)\\n\\uf0a7 Le producer reste asynchrone en interne, mais on appelle .get() sur le Future retourné par send() pour \\nattendre le résultat.\\n\\uf0a7 Cela permet de savoir si le message a bien été délivré avant de passer au suivant.\\n\\uf0a7 Fiable, mais plus lent que l’envoi asynchrone.\\n3. Asynchronous Send with Callback (Envoi asynchrone avec rappel)\\n\\uf0a7 On appelle send() en fournissant une fonction de rappel (callback).\\n\\uf0a7 Cette fonction est exécutée dès que Kafka répond (succès ou échec).\\n\\uf0a7 Permet une exécution non bloquante tout en gérant les erreurs proprement.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 15, 'page_label': '16', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='16\\nProducers: Construction'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 16, 'page_label': '17', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='17\\nEnvoi synchrone d’un message Kafka:\\nL’envoi synchrone permet au producer :\\n\\uf0a7 de capturer les exceptions (ex. : erreurs de Kafka ou échec après plusieurs tentatives),\\n\\uf0a7 mais il présente un inconvénient majeur : la performance.\\n\\uf0a7 Inconvénient principal :\\no Le thread reste bloqué en attendant la réponse du broker Kafka (de 2 ms à plusieurs secondes selon la \\ncharge).\\no Pendant ce temps, il ne peut rien faire d’autre (pas même envoyer d’autres messages).\\no Cela entraîne une performance très faible, raison pour laquelle cette méthode n’est pas utilisée en \\nproduction, mais très fréquente dans les exemples pédagogiques.\\nProducers: Construction'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 17, 'page_label': '18', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='18\\nEnvoi asynchrone dans Kafka:\\nL’envoi asynchrone est rapide et efficace, surtout lorsque l’on n’a pas besoin d’attendre les réponses de Kafka.\\n\\uf0a7 Comparaison avec l’envoi synchrone :\\no Si le temps réseau aller-retour est de 10 ms, envoyer 100 messages en attendant à chaque fois prend ~1 \\nseconde.\\no Si on envoie tout sans attendre, c’est quasi instantané.\\no En général, l’application n’a pas besoin de la réponse (topic, partition, offset) mais doit savoir si une erreur \\nest survenue.\\n\\uf0a7 Solution : Ajouter un callback\\no Permet d’envoyer les messages sans blocage tout en gérant les erreurs.\\no Le callback est exécuté à la réception de la réponse Kafka, avec ou sans erreur.\\nProducers: Construction'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 18, 'page_label': '19', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='19\\nProducers: Temps de livraison des messages\\nDepuis Apache Kafka 2.1, ce temps est divisé en deux phases distinctes :\\n\\uf0a7 Temps jusqu’au retour de l’appel asynchrone à send() :\\no Pendant cette phase, le thread de l’application est bloqué (le temps de mise en lot du message, par ex.).\\n\\uf0a7 Temps entre le retour de send() et le déclenchement du callback :\\no Correspond au délai entre le moment où le message a été placé dans un batch à envoyer, et la réponse\\ndu broker Kafka :\\n\\uf0fc succès ,\\n\\uf0fc erreur non-récupérable ,\\n\\uf0fc ou expiration du délai d’envoi maximal configuré.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 19, 'page_label': '20', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='20\\nProducers: Serializers\\nKafka requiert des sérialiseurs pour convertir les objets Java en tableaux d’octets avant envoi. Kafka fournit des \\nsérialiseurs par défaut (ex : chaînes, entiers, tableaux d’octets), mais pour des objets plus complexes, vous devrez \\ncréer votre propre sérialiseur ou utiliser une bibliothèque de sérialisation.\\nCustom Serializer\\nPrenons une classe simple Customer avec un ID et un nom. Pour l’envoyer via Kafka, on crée un sérialiseur\\npersonnalisé : CustomerSerializer.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 20, 'page_label': '21', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='21\\nProducers: Serializers\\nExtrait du fonctionnement :\\n\\uf0a7 L’objet Customer est converti en un tableau d’octets (byte[]) via un ByteBuffer.\\n\\uf0a7 Format de sérialisation :\\no 4 octets : ID client\\no 4 octets : taille du nom (en UTF-8)\\no N octets : le nom lui-même\\nLimites :\\n\\uf0a7 Le code est fragile : tout changement dans la classe (ex : ajouter un champ) casse la compatibilité.\\n\\uf0a7 Le débogage des problèmes de compatibilité entre versions est complexe.\\n\\uf0a7 Si plusieurs équipes utilisent Kafka, elles doivent toutes maintenir la même logique de \\nsérialisation/désérialisation.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 21, 'page_label': '22', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"22\\nProducers: Serializers\\nRecommandations:\\n\\uf0a7 Éviter les sérialiseurs personnalisés quand possible.\\n\\uf0a7 Préférer des bibliothèques standardisées et robustes \\ncomme Avro, Protobuf, Thrift ou JSON.\\n\\uf0a7 Avro est fortement recommandé : il gère le \\nschéma, l'évolution de données, la compatibilité et \\npermet une meilleure interopérabilité.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 22, 'page_label': '23', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"23\\nProducers: Sérialisation avec Apache Avro\\nApache Avro est un format de sérialisation de données neutre vis-à-vis du langage. Conçu par Doug Cutting\\n(créateur de Hadoop), il facilite le partage de fichiers de données avec un grand nombre d’utilisateurs.\\n\\uf0a7 Les données sont décrites à l’aide d’un schéma JSON.\\n\\uf0a7 La sérialisation est binaire (ou JSON si besoin).\\n\\uf0a7 Le schéma est inclus avec les données pour permettre lecture/écriture sans ambiguïté.\\nPourquoi Avro avec Kafka ?\\nAvro est particulièrement adapté à Kafka car il gère l’évolution du schéma. Lorsqu’un producteur change de \\nschéma de manière compatible, les consommateurs n'ont pas besoin d’être mis à jour pour continuer à traiter les \\nmessages.\\nOriginal schema New schema\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 23, 'page_label': '24', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='24\\nCompatibilité des applications\\n\\uf0a7 Ancienne application : peut lire les nouveaux messages, mais getFaxNumber() renverra null car ce\\nchamp n’existe plus.\\n\\uf0a7 Nouvelle application : peut lire les anciens messages, mais getEmail() renverra null car l’email n’était pas \\nprésent à l’époque.\\n\\uf0a7 Résultat : aucune erreur bloquante, même si les schémas sont différents mais compatibles.\\nProducers: Sérialisation avec Apache Avro'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 24, 'page_label': '25', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='25\\nProducers: Sérialisation avec Apache Avro\\n\\uf0a7 Problème : stocker le schéma dans chaque message\\no Dans un fichier Avro, inclure le schéma complet ne pose pas trop de problème.\\nMais dans Kafka, inclure le schéma dans chaque message doublerait sa taille, ce qui est inefficace.\\no Solution : utiliser un Schema Registry\\n\\uf0a7 Schema Registry – Comment ça marche ?\\no Le Schema Registry est un service externe (pas intégré à Kafka) qui stocke tous les schémas utilisés.\\no Lorsqu’un message est produit :\\nOn n’envoie pas le schéma complet, mais simplement un identifiant du schéma.\\no Le consommateur utilise cet identifiant pour récupérer le schéma dans le registry et désérialiser le message.\\no Toute cette logique est gérée automatiquement par les sérialiseurs/désérialiseurs Avro. Le producteur Kafka \\nn’a rien à faire de spécial, il utilise juste un AvroSerializer comme un sérialiseur classique.\\n\\uf0a7 Exemple utilisé : Confluent Schema Registry\\no Outil open-source (ou intégré à la plateforme Confluent).\\no Permet de stocker, versionner et gérer les schémas utilisés pour Kafka.\\no La documentation Confluent est recommandée pour la mise en œuvre.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 25, 'page_label': '26', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"26\\nKafka: Consumers\\n\\uf0a7 L'API Kafka Consumers est une interface fournie par Apache Kafka pour consommer des messages publiés \\ndans des topics Kafka. Un consumer est un client Kafka qui se connecte à un ou plusieurs topics, lit les \\nmessages disponibles dans ces topics et les traite.\\n\\uf0a7 Problème : Si le flux de messages est élevé, un consommateur unique peut être submergé et accumuler un \\nretard important.\\n\\uf0a7 Les consumers sont regroupés dans des consumer groups. Chaque consumer d'un groupe lit les messages d'un \\nensemble de partitions spécifique, garantissant ainsi que chaque message est consommé par un seul \\nconsumer du groupe.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 26, 'page_label': '27', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"27\\nKafka Consumers: Concepts\\n\\uf0a7 Consumer Groups:\\no Un groupe de consommateurs permet de distribuer le traitement des messages entre plusieurs \\nconsommateurs.\\no Chaque consommateur du groupe reçoit les messages d'un sous-ensemble des partitions d'un topic.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 27, 'page_label': '28', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"28\\nKafka Consumers: Concepts\\n\\uf0a7 Pour augmenter la capacité de traitement, on peut ajouter des consommateurs à un groupe.\\n\\uf0a7 Limite : Le nombre de partitions d'un topic détermine le nombre maximal de consommateurs actifs.\\n\\uf0a7 Plus le nombre de partitions est élevé, plus il est possible d'ajouter des consommateurs.\\n\\uf0a7 Exemple : Un topic avec 8 partitions peut supporter jusqu'à 8 consommateurs actifs dans un groupe.\\n\\uf0a7 Attention : Si un groupe a plus de consommateurs que de partitions, certains resteront inactifs.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 28, 'page_label': '29', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"29\\nKafka Consumers: Concepts\\n\\uf0a7 Créer des groupes de consommateurs distincts pour chaque application qui doit traiter tous les messages.\\n\\uf0a7 Exemple :\\no Application A utilise le groupe G1 pour traiter le topic T1.\\no Application B utilise le groupe G2 pour traiter le même topic T1.\\no Les consommateurs de G1 et G2 travaillent indépendamment sans interférer.\\n\\uf0a7 Avantage : Permet d'isoler le traitement des données pour différentes applications tout en partageant le \\nmême topic.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 29, 'page_label': '30', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='30\\nKafka Consumers: Rebalancement des partitions\\n\\uf0a7 Lorsqu’un consommateur rejoint ou quitte un groupe, les partitions sont réassignées automatiquement.\\n\\uf0a7 Objectif : Garantir la haute disponibilité et l’équilibre des charges.\\n\\uf0a7 Types de rebalancements :\\n\\uf0a7 Eager Rebalance : Tous les consommateurs stoppent, libèrent leurs partitions, puis récupèrent de nouvelles \\npartitions. Risque de latence accrue.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 30, 'page_label': '31', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='31\\n\\uf0a7 Cooperative Rebalance : Redistribution progressive des partitions pour éviter les interruptions complètes. \\nRecommandé pour les grands groupes.\\nKafka Consumers: Rebalancement des partitions'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 31, 'page_label': '32', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"32\\n\\uf0a7 Les consommateurs envoient des heartbeats pour signaler qu'ils sont actifs.\\n\\uf0a7 Le coordinateur de groupe surveille ces heartbeats pour détecter les pannes.\\n\\uf0a7 Si un consommateur cesse d'envoyer des heartbeats, le coordinateur le considère comme inactif et \\ndéclenche un rebalancement.\\n\\uf0a7 Paramètres essentiels :\\n\\uf0a7 heartbeat.interval.ms : Intervalle entre deux heartbeats.\\n\\uf0a7 session.timeout.ms : Durée maximale sans heartbeat avant qu'un consommateur soit considéré comme \\nmort.\\nKafka Consumers: Rebalancement des partitions\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 32, 'page_label': '33', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='33\\nKafka Consumers: Adhésion statique des groupes\\n\\uf0a7 Par défaut, un consommateur est identifié de manière temporaire dans un groupe.\\n\\uf0a7 Avec group.instance.id, un consommateur devient un membre statique :\\n\\uf0a7 Ses partitions ne sont pas réassignées après un redémarrage.\\n\\uf0a7 Il conserve ses partitions tant que le délai session.timeout.ms n’est pas dépassé.\\n\\uf0a7 Avantage : Maintien de l’état local (cache, traitement en cours).\\n\\uf0a7 Inconvénient : En cas de redémarrage long, risque de retard important.\\n\\uf0a7 Recommendations :\\no Utiliser cooperative rebalance pour minimiser les interruptions.\\no Configurer group.instance.id pour les applications nécessitant des états locaux persistants.\\no Prévoir un nombre de partitions suffisant pour permettre une montée en charge progressive.\\no Surveiller les heartbeats et ajuster les timeouts pour équilibrer réactivité et robustesse.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 33, 'page_label': '34', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"34\\nCréation d’un Kafka Consumer\\nPour consommer des enregistrements dans Kafka, la première étape consiste à créer une instance de \\nKafkaConsumer. La création d'un KafkaConsumer est similaire à celle d'un KafkaProducer : il faut d'abord créer\\nune instance de Properties pour définir les propriétés à utiliser. Les trois propriétés obligatoires pour démarrer sont :\\n1. bootstrap.servers : la chaîne de connexion au cluster Kafka (comme pour le producteur).\\n2. key.deserializer : classe pour convertir les clés des enregistrements de tableau d'octets en objets Java.\\n3. value.deserializer : classe pour convertir les valeurs des enregistrements de tableau d'octets en objets\\nJava.\\nUne quatrième propriété, non obligatoire mais couramment utilisée, est group.id, qui indique le groupe de \\nconsommateurs auquel appartient l'instance de KafkaConsumer.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 34, 'page_label': '35', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"35\\nConsumers: Souscription à un topic\\n\\uf0a7 Après avoir créé un KafkaConsumer, l'étape suivante consiste à s'abonner à un ou plusieurs topics.\\nLa méthode subscribe() accepte une liste de topics en paramètre. \\n\\uf0a7 Il est également possible d'utiliser une expression régulière avec subscribe(). Cela permet de s'abonner à \\nplusieurs topics dont les noms correspondent au motif spécifié. Si un nouveau topic correspondant au motif est\\ncréé, un rééquilibrage se produit immédiatement et les consommateurs commencent à consommer les \\ndonnées de ce nouveau topic. \\n\\uf0a7 Cette méthode est particulièrement utile pour les applications qui consomment des données de plusieurs \\ntopics ou pour les applications de traitement de flux.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 35, 'page_label': '36', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"36\\nConsumers: la boucle poll()\\nAu cœur de l'API Consumer se trouve une boucle simple qui interroge le serveur pour obtenir des données. Le \\ncode typique d'un consommateur Kafka ressemble à ceci :\\nCette boucle est infinie car les consommateurs sont généralement des applications longues durées qui \\nconsomment continuellement des données de Kafka.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 36, 'page_label': '37', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"37\\n\\uf0a7 Importance de poll() :\\no Les consommateurs doivent appeler poll() en permanence pour être considérés comme actifs.\\no Si poll() n'est pas invoqué pendant plus de max.poll.interval.ms, le consommateur est marqué comme\\ninactif et ses partitions sont réassignées à un autre consommateur.\\no Le paramètre timeout détermine le temps d'attente si aucun enregistrement n'est disponible dans le buffer.\\n\\uf0a7 Fonctionnement de poll() :\\no poll() retourne une liste de ConsumerRecords.\\no Chaque enregistrement contient : le topic, la partition, l'offset, la clé et la valeur.\\no Le traitement des enregistrements consiste généralement à écrire les résultats dans une base de données\\nou à mettre à jour un enregistrement existant.\\n\\uf0a7 Gestion des rééquilibrages : Le premier appel à poll() gère :\\no La recherche du GroupCoordinator.\\no L'adhésion au groupe de consommateurs.\\no L'assignation des partitions.\\no La gestion des rééquilibrages.\\nConsumers: la boucle poll()\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 37, 'page_label': '38', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"38\\nConfiguration des consommateurs\\nLes consommateurs Kafka peuvent être configurés à l'aide de plusieurs propriétés. Voici les principales \\nconfigurations :\\n\\uf0a7 fetch.min.bytes :\\no Définit la quantité minimale de données que le consommateur souhaite recevoir du broker lors d'un \\npoll().\\no Par défaut : 1 octet.\\no Augmenter cette valeur peut réduire la charge CPU en cas de faible activité sur le topic, mais peut\\négalement augmenter la latence.\\n\\uf0a7 fetch.max.wait.ms :\\no Temps d'attente maximal pour obtenir fetch.min.bytes.\\no Par défaut : 500 ms.\\no Un compromis entre latence et volume de données récupérées.\\no Exemple : Si fetch.min.bytes = 1 MB et fetch.max.wait.ms = 100 ms, Kafka renverra les données soit après 1 \\nMB de données, soit après 100 ms, selon la première condition atteinte.\\n\\uf0a7 fetch.max.bytes :\\no Quantité maximale de données retournées par poll().\\no Par défaut : 50 MB.\\no Contrôle la mémoire utilisée par le consommateur.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 38, 'page_label': '39', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"39\\n\\uf0a7 max.poll.records :\\no Nombre maximal de messages traités par appel à poll().\\no Permet de limiter le traitement des données lors de chaque itération de la boucle poll().\\n\\uf0a7 max.partition.fetch.bytes :\\no Limite la quantité de données par partition lors d'un poll().\\no Par défaut : 1 MB.\\no Recommandé d'utiliser fetch.max.bytes pour un meilleur contrôle.\\nConfiguration des consommateurs\\nGestion des temps et des délais:\\n\\uf0a7 session.timeout.ms : Temps pendant lequel le consommateur peut rester inactif sans être considéré comme\\nmort.\\n\\uf0a7 heartbeat.interval.ms : Fréquence d'envoi des heartbeats.\\n\\uf0a7 max.poll.interval.ms : Durée maximale entre deux appels poll() avant que le consommateur ne soit considéré\\ncomme mort.\\n\\uf0a7 default.api.timeout.ms : Délai par défaut pour toutes les requêtes API du consommateur.\\n\\uf0a7 request.timeout.ms : Délai maximal pour attendre une réponse du broker.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 39, 'page_label': '40', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='40\\nGestion des offsets :\\n\\uf0a7 auto.offset.reset : Comportement lorsque le consommateur ne trouve pas d\\'offset valide :\\no \"latest\" (par défaut) : commence à lire les nouveaux messages.\\no \"earliest\" : commence à lire depuis le début du topic.\\no \"none\" : lève une exception si l\\'offset est invalide.\\n\\uf0a7 enable.auto.commit :\\no Contrôle le commit automatique des offsets.\\no Par défaut : true.\\no Pour un contrôle manuel des offsets, définir sur false.\\nConfiguration des consommateurs'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 40, 'page_label': '41', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"41\\nConfiguration des consommateurs\\nStratégies d'assignation des partitions\\n\\uf0a7 Range :\\no Attribue les partitions de manière consécutive.\\no Peut créer des déséquilibres si le nombre de partitions n'est pas divisible par le nombre de \\nconsommateurs.\\n\\uf0a7 RoundRobin :\\no Attribue les partitions de manière circulaire, garantissant une répartition plus équilibrée.\\n\\uf0a7 Sticky :\\no Objectif : minimiser le déplacement des partitions lors des rééquilibrages.\\n\\uf0a7 Cooperative Sticky :\\no Identique à Sticky, mais permet aux consommateurs de continuer à consommer les partitions non \\nréassignées.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 41, 'page_label': '42', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"42\\nConfiguration des consommateurs\\nAutres propriétés importantes\\n\\uf0a7 client.id : Identifiant du consommateur, utilisé dans les logs et les métriques.\\n\\uf0a7 client.rack : Permet d'indiquer le datacenter ou la zone où se trouve le consommateur.\\n\\uf0a7 group.instance.id : Identifiant unique d'un consommateur pour une adhésion statique au groupe.\\n\\uf0a7 receive.buffer.bytes / send.buffer.bytes : Taille des buffers TCP. À ajuster pour des communications inter-\\ndatacenters.\\n\\uf0a7 offsets.retention.minutes : Durée pendant laquelle les offsets sont conservés après que le groupe soit \\ndevenu inactif.\\n\\uf0a7 Par défaut : 7 jours.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 42, 'page_label': '43', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"43\\nConsumers: Commits & Offsets\\nLorsque le consommateur appelle poll(), il reçoit les enregistrements non encore lus par le groupe de \\nconsommateurs. Kafka permet aux consommateurs de suivre leur position dans chaque partition à l'aide des offsets, \\nmais il ne gère pas les accusés de réception.\\nL'action d'actualiser la position dans une partition est appelée commit d'offset. Les offsets sont stockés dans le topic \\nspécial __consumer_offsets. En cas de crash ou d'ajout d'un nouveau consommateur, un rééquilibrage se produit et \\nles consommateurs se basent sur les offsets précédemment commités pour reprendre la consommation.\\nRisques :\\no Si le dernier offset commité est inférieur à l'offset du dernier message traité, certains messages seront traités deux\\nfois.\\no Si le dernier offset commité est supérieur à l'offset du dernier message traité, certains messages seront perdus.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 43, 'page_label': '44', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"44\\nConsumers: Commits & Offsets\\n1. Commit automatique :\\n\\uf0a7 En activant enable.auto.commit=true, le consommateur commit automatiquement l'offset toutes les 5 \\nsecondes (par défaut).\\n\\uf0a7 Problème : Si le consommateur plante avant le prochain commit, des messages peuvent être traités\\ndeux fois.\\n2. Commit manuel des offsets actuels :\\nPour plus de contrôle, il est possible de désactiver le commit automatique (enable.auto.commit=false) et de \\ncommiter manuellement à l'aide de commitSync().\\nExemple:\\nLimite : Si le commit est effectué avant le traitement complet des messages, certains d'entre eux peuvent être \\nignorés en cas de crash.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 44, 'page_label': '45', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"45\\nConsumers: Commits & Offsets\\n3. Commit asynchrone:\\nPour éviter le blocage de l'application pendant le commit, Kafka propose commitAsync().\\n\\uf0a7 Avantage : Non bloquant, améliore le débit.\\n\\uf0a7 Inconvénient : En cas de défaillance, il ne réessaie pas, ce qui peut causer des duplications ou des \\npertes de messages.\\n\\uf0a7 Exemple avec callback :\\n4. Combinaison des commits synchrone et asynchrone:\\nIl est recommandé de combiner commitAsync() pour le traitement normal et commitSync() juste avant la \\nfermeture du consommateur pour garantir la persistance des offsets.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 45, 'page_label': '46', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"46\\nConsumers: Commits & Offsets\\n5. Commit d'offsets spécifiques:\\nPour des cas où il est nécessaire de commiter des offsets avant la fin d'un batch, il est possible de passer une \\nmap d'offsets spécifiques :\\n\\uf0a7 Exemple:\\nCette approche permet un contrôle fin des engagements et réduit le risque de pertes ou de duplications de \\nmessages.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 46, 'page_label': '47', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"47\\nConsumers: Deserializers\\n\\uf0a7 Dans Kafka, les producteurs utilisent des sérialiseurs pour convertir des objets en tableaux d'octets avant de \\nles envoyer à Kafka. De même, les consommateurs utilisent des désérialiseurs pour convertir ces tableaux \\nd'octets en objets Java.\\n\\uf0a7 Dans les exemples précédents, nous avons utilisé le StringDeserializer par défaut pour les clés et les valeurs des \\nmessages. Cependant, pour des objets personnalisés, il est nécessaire de créer des désérialiseurs personnalisés\\nou d'utiliser des formats standardisés comme Avro.\\n\\uf0a7 Il est essentiel que le sérialiseur utilisé par le producteur corresponde au désérialiseur utilisé par le \\nconsommateur. Par exemple, un message sérialisé avec IntSerializer ne pourra pas être désérialisé\\ncorrectement avec StringDeserializer.\\no Avro et le Schema Registry permettent de gérer cette compatibilité en validant les schémas des \\nmessages produits et consommés.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 47, 'page_label': '48', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='48\\nConsumers: Deserializers\\nVoici un exemple de désérialiseur personnalisé pour un objet Customer'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 48, 'page_label': '49', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='49\\nConsumers: Deserializers\\nLimite :\\nLes désérialiseurs personnalisés sont fragiles car ils dépendent fortement de la structure des objets. Les erreurs de \\ncompatibilité peuvent survenir si les schémas changent.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 49, 'page_label': '50', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"50\\nConsumers: Deserializers\\nL'utilisation d'Avro permet de standardiser les messages et d'assurer la compatibilité des schémas.\\nExemple de désérialisation avec Avro :\\n\\uf0a7 schema.registry.url : Indique l'emplacement du Schema Registry.\\n\\uf0a7 KafkaAvroDeserializer : Désérialise les messages Avro.\\n\\uf0a7 specific.avro.reader : Permet d'utiliser des objets Avro générés.\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 0, 'page_label': '1', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='UNIVERSITE ABDELMALEK\\nESSAADI\\nECOLE NATIONALE DES\\nSCIENCES APPLIQUEES\\nD’AL HOCEIMA\\nRapport de Stage de Fin d’Année\\nApplication de Gestion Centralisée\\ndes Imprimantes de la production\\nRéalisé par :Achbab Mohammed\\nEncadré par :Mr.Bouya Amine\\nDépartement :IT\\nFilière :Transformation Digitale et Intelligence Artificielle\\nNiveau :2eme année cycle ingénieur\\nDurée :Du 01/07/2025 au 1/09/2025\\nAnnée Universitaire : 2024 – 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 1, 'page_label': '1', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Remerciements\\nJe tiens à exprimer ma profonde gratitude à toutes les personnes qui ont contribué,\\nde près ou de loin, à la réussite de ce stage et à la réalisation de ce projet.\\nMes remerciements les plus sincères s’adressent à mon encadrantMr. Bouya\\nAminepour son accompagnement constant, ses conseils précieux et sa patience.\\nSa supervision a été déterminante pour l’accomplissement de mon projet et\\nl’acquisition de nouvelles compétences.\\nJ’exprime ma gratitude à l’ensemble du personnel de l’entreprise Yazaki pour leur\\naccueil chaleureux, leur disponibilité et les connaissances qu’ils ont bien voulu partager\\navec moi.\\nEnfin, un grand merci à ma famille et à mes amis pour leur soutien et leurs encoura-\\ngements tout au long de ce parcours.\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 2, 'page_label': '2', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Table des matières\\nRemerciements 1\\nListe des figures 4\\nListe des tableaux 5\\nIntroduction 6\\n1 Contexte et Objectifs du Stage 7\\n1.1 Présentation de l’Entreprise . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.2 Contexte du Projet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.3 Objectifs du Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2 Analyse des Besoins et Conception 9\\n2.1 Analyse Fonctionnelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Analyse Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3 Conception UML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3.1 Diagramme de Cas d’Utilisation . . . . . . . . . . . . . . . . . . . . 12\\n2.3.2 Diagramme de Classes . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3 Réalisation Technique 14\\n3.1 Environnement de Développement . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2 Architecture Détaillée . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2.1 Backend Spring Boot . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2.2 Frontend React . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2.3 Base de Données PostgreSQL . . . . . . . . . . . . . . . . . . . . . 15\\n3.3 Implémentation des Fonctionnalités Principales . . . . . . . . . . . . . . . 15\\n3.3.1 Découverte Automatique des Imprimantes . . . . . . . . . . . . . . 15\\n3.3.2 Communication SNMP . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.3.3 Interface Utilisateur React . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.4 Gestion de la Qualité et Tests . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4 Déploiement et Exploitation 17\\n4.1 Conteneurisation avec Docker . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.2 Déploiement en Production . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.3 Monitoring et Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n5 Bilan et Perspectives 18\\n5.1 Bilan du Projet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n5.2 Difficultés Rencontrées et Solutions . . . . . . . . . . . . . . . . . . . . . . 18\\n5.3 Perspectives d’Évolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n5.4 Apports Personnels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nConclusion 21\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 3, 'page_label': '3', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='TABLE DES MATIÈRES 3\\nBibliographie 22\\nA Annexes 23\\nA.1 Code Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\nA.2 Manuel d’Utilisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\nA.3 Captures d’Écran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 4, 'page_label': '4', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Table des figures\\n1.1 Vue d’une desccription de l’histoire de Yazaki . . . . . . . . . . . . . . . . 7\\n2.1 Diagramme de Cas d’Utilisation . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.2 Diagramme de classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nA.1 home page de l’application . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\nA.2 Accès simplifié aux imprimantes via le tableau de bord . . . . . . . . . . . 24\\nA.3 Interface de découverte d’imprimantes . . . . . . . . . . . . . . . . . . . . 25\\nA.4 Interface d’export des données . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 5, 'page_label': '5', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Liste des tableaux\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 6, 'page_label': '6', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Introduction\\nCe stage de deux mois réalisé au sein de Yazaki Meknes avait pour objectif\\nprincipal le développement d’une application complète de gestion d’imprimantes\\nréseau. Dans un contexte où la gestion des ressources d’impression constitue un\\nenjeu opérationnel majeur, cette application vise à centraliser le monitoring, la\\ndécouverte automatique et la gestion des imprimantes connectées au réseau de\\nl’entreprise.\\nDans le cadre de ma formation en Transformation Digitale et Intelligence Artificielle,\\nj’ai effectué un stage de deux mois au sein de l’entreprise Yazaki, spécialisée dans la\\nconception et la fabrication de systèmes électriques et électroniques pour l’industrie au-\\ntomobile. Ce stage avait pour objectif principal la conception et le développement d’une\\napplication de gestion d’imprimantes réseau.\\nLagestionduparcd’imprimantesreprésenteunenjeuimportantpourlesentreprisesde\\ntaille moyenne à grande. Le suivi des compteurs de pages, de la connectivitée, des niveaux\\nde toner,et l’état général des imprimantes sont essentiels pour assurer la continuité des\\nservices d’impression et anticiper les besoins de maintenance.\\nDurantcestage,j’aidéveloppéuneapplicationwebcomplètepermettantladécouverte,\\nle monitoring et la gestion des imprimantes connectées au réseau d’entreprise. L’outil offre\\nla possibilité de suivre en temps réel différents indicateurs tels que l’état de connexion\\n(imprimante en ligne ou hors ligne), le nombre total de pages imprimées, le niveau de\\ntoner ainsi que l’état général des périphériques. La découverte des imprimantes se fait en\\nrenseignant l’adresse d’un ou de plusieurs sous-réseaux, ce qui permet d’élargir la portée\\nde la supervision et d’obtenir une vision centralisée du parc d’impression.\\nCe rapport présente en détail le travail réalisé, depuis l’analyse des besoins jusqu’au\\ndéploiement de l’application, en passant par la conception technique et le développement.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 7, 'page_label': '7', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 1\\nContexte et Objectifs du Stage\\n1.1 Présentation de l’Entreprise\\nYazaki est un leader mondial dans la fabrication de systèmes de câblage\\nautomobile et de instruments de mesure. Fondée en 1929 au Japon, l’entreprise\\nemploie plus de 200 000 personnes dans 45 pays et possède une forte présence dans\\nle secteur automobile avec des innovations technologiques constantes.\\nYazaki est une entreprise spécialisée dans la conception et la fabrication de systèmes\\nélectriques et électroniques pour l’industrie automobile. Fondée en 1941 au Japon, elle\\nemploie aujourd’hui plus de 190 000 collaborateurs et possède une présence internationale\\navec des implantations dans plus de 45 pays. Les activités principales de Yazaki incluent la\\nproduction de faisceaux de câblage, de systèmes de distribution électrique, de composants\\nélectroniques et de solutions pour la gestion de l’énergie dans les véhicules.\\nFigure 1.1– Vue d’une desccription de l’histoire de Yazaki\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 8, 'page_label': '8', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 1. CONTEXTE ET OBJECTIFS DU STAGE 8\\n1.2 Contexte du Projet\\nLa gestion du parc d’imprimantes chez Yazaki Meknes représentait un défi\\nopérationnel significatif avec plus de 200 imprimantes réparties sur différents sites,\\nnécessitant une surveillance manuelle fastidieuse et sujette à des oublis ou retards\\ndans la maintenance préventive.\\nAu sein de Yazaki, la gestion du parc d’imprimantes était jusqu’alors réalisée manuel-\\nlement par les équipes informatiques. Cette approche présentait plusieurs limitations :\\n— Absence de centralisation des informations sur l’état des imprimantes\\n— Difficulté à anticiper les pannes et les besoins en consommables\\n— Processus fastidieux pour inventorier les nouvelles imprimantes sur le réseau\\n— Manque de reporting automatisé sur l’utilisation des imprimantes\\nFace à ces constats, il a été décidé de développer une application interne de gestion\\nd’imprimantes réseau permettant de résoudre ces problématiques.\\n1.3 Objectifs du Stage\\nLes objectifs techniques principaux incluaient la maîtrise du protocole SNMP, le\\ndéveloppement full-stack avec des technologies modernes, et la conteneurisation de\\nl’application pour un déploiement simplifié en environnement de production.\\nLes objectifs principaux de ce stage étaient :\\n1. Analyser les besoins fonctionnels et techniques de l’application\\n2. Concevoir l’architecture technique de la solution\\n3. Développer une application web complète avec frontend et backend\\n4. Implémenter le protocole SNMP pour la communication avec les imprimantes\\n5. Réaliser des fonctionnalités de découverte automatique des imprimantes sur le ré-\\nseau\\n6. Mettre en place un système de reporting et d’export de données\\n7. Déployer l’application à l’aide de Docker'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 9, 'page_label': '9', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 2\\nAnalyse des Besoins et Conception\\n2.1 Analyse Fonctionnelle\\nL’analyse fonctionnelle a identifié quatre domaines critiques : la découverte\\nautomatique, la surveillance en temps réel, la gestion des alertes et la génération\\nde rapports, répondant aux besoins spécifiques des équipes techniques de Yazaki.\\nL’analyse des besoins a permis d’identifier les fonctionnalités principales suivantes :\\nFonctionnalités Administratives\\n— Découverte automatique des imprimantes sur le réseau\\n— Inventaire centralisé des imprimantes avec leurs caractéristiques\\n— Surveillance en temps réel des niveaux de toner et des compteurs de pages\\n— Gestion des statuts des imprimantes (en ligne, hors ligne, en impression, etc.)\\nFonctionnalités de Reporting\\n— Tableau de bord avec indicateurs visuels\\n— Export des données au format Excel\\n— Filtres avancés pour la recherche d’imprimantes\\n— Alertes automatiques pour les imprimantes nécessitant une intervention\\n2.2 Analyse Technique\\nL’architecture technique retenue privilégie une approche microservices avec une\\nAPI RESTful, permettant une séparation claire des responsabilités et une\\névolutivité optimale pour les futures extensions du système.\\nL’analyse technique a conduit aux choix suivants :\\nArchitecture\\n— Architecture microservices avec séparation frontend/backend\\n— API RESTful pour la communication entre les composants\\n— Base de données relationnelle pour la persistance des données\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 10, 'page_label': '10', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 10\\nTechnologies\\nLogo Technologie Description\\nSpring Boot (Java) Utilisé pour le backend grâce à sa robustesse et\\nson large écosystème.\\nReact.js + Tail-\\nwind CSS\\nDéveloppement du frontend avec une interface\\nmoderne, responsive et modulaire.\\nPostgreSQL Base de données relationnelle fiable, performante\\net open-source.\\nSNMP Protocole réseau utilisé pour le monitoring et la\\ncommunication avec les imprimantes.\\nDocker Conteneurisation pour simplifier le déploiement\\net isoler les services.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 11, 'page_label': '11', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 12, 'page_label': '12', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 12\\n2.3 Conception UML\\n2.3.1 Diagramme de Cas d’Utilisation\\nFigure 2.1– Diagramme de Cas d’Utilisation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 13, 'page_label': '13', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 13\\n2.3.2 Diagramme de Classes\\nFigure 2.2– Diagramme de classes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 14, 'page_label': '14', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 3\\nRéalisation Technique\\n3.1 Environnement de Développement\\nL’environnement de développement a été configuré avec une approche DevOps,\\nintégrant le versioning Git, une pipeline CI/CD naissante, et des outils de qualité\\nde code pour assurer la maintenabilité du projet sur le long terme.\\nL’environnement de développement mis en place comprenait :\\n—IDE: IntelliJ IDEA pour le backend Java, VS Code pour le frontend React\\n—Versioning: Git avec repository GitLab d’entreprise\\n—Gestion de projet: Méthodologie Agile avec des sprints de deux semaines\\n—Tests: JUnit pour le backend, Jest et React Testing Library pour le frontend\\n3.2 Architecture Détaillée\\n3.2.1 Backend Spring Boot\\nL’architecture backend repose sur le pattern MVC avec une couche de service\\nrobuste gérant la logique métier, une couche de repository pour l’accès aux\\ndonnées, et un contrôleur exposant une API RESTful documentée et sécurisée.\\nLe backend a été développé avec Spring Boot, offrant une structure robuste et modu-\\nlaire. Le contrôleur principal expose une API RESTful avec les endpoints suivants :\\n—GET /printers- Lister les imprimantes avec filtres\\n—POST /discoverPrinters- Découvrir de nouvelles imprimantes\\n—POST /refreshAllPrinters- Actualiser les données des imprimantes\\n—GET /printers/download/excel- Exporter les données en Excel\\n—GET /discoveryProgress- Suivre la progression de la découverte\\n3.2.2 Frontend React\\nL’interface utilisateur a été conçue avec une approche component-first, favorisant\\nla réutilisabilité et la maintenabilité. L’utilisation de Tailwind CSS a permis de\\ncréer une interface moderne et responsive sans dépendre à des bibliothèques de\\ncomposants externes.\\nLe frontend utilise React avec une architecture composée de plusieurs modules. L’in-\\nterface utilisateur est divisée en plusieurs vues :\\n—Tableau de bord: Vue d’ensemble avec indicateurs visuels\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 15, 'page_label': '15', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 3. RÉALISATION TECHNIQUE 15\\n—Liste des imprimantes: Tableau interactif avec fonctionnalités de filtrage\\n—Découverte: Interface pour lancer la découverte d’imprimantes\\n—Export: Gestion de l’export des données\\n3.2.3 Base de Données PostgreSQL\\nLe schéma de base de données a été optimisé pour les requêtes de reporting tout\\nen maintenant l’intégrité des données. Des index stratégiques ont été implémentés\\nsur les champs fréquemment interrogés comme l’adresse IP et le statut des\\nimprimantes.\\nLa base de données PostgreSQL stocke les informations des imprimantes avec une\\nstructure relationnelle normalisée permettant des requêtes complexes et des jointures ef-\\nficaces pour la génération de rapports.\\n3.3 Implémentation des Fonctionnalités Principales\\n3.3.1 Découverte Automatique des Imprimantes\\nL’algorithme de découverte combine un scan réseau par ping avec une\\ninterrogation SNMP ciblée, utilisant un pool de threads pour paralléliser les\\nrequêtes et réduire significativement le temps de scan d’un réseau complet.\\nLa découverte automatique utilise une combinaison de ping réseau et d’interrogation\\nSNMP. Le processus s’exécute en plusieurs phases : scan du réseau pour identifier les ap-\\npareils actifs, interrogation SNMP pour identifier les imprimantes, et enfin enregistrement\\nen base de données avec toutes les informations techniques recueillies.\\n3.3.2 Communication SNMP\\nL’implémentation SNMP gère les particularités des différents fabricants\\nd’imprimantes via un système d’OIDs (Object Identifiers) spécifiques, avec des\\nmécanismes de fallback pour assurer une compatibilité maximale avec le parc\\nhétérogène d’imprimantes.\\nLe service SNMP implémente la communication avec les imprimantes en utilisant la\\nbibliothèque SNMP4J. Cette implémentation gère les timeouts, les retries et les erreurs de\\ncommunication spécifiques au protocole SNMP, avec une gestion robuste des exceptions.\\n3.3.3 Interface Utilisateur React\\nL’interface utilise des indicateurs visuels avancés avec des graphiques\\nsemi-circulaires pour représenter l’état des consommables, et un système de\\nfiltrage dynamique permettant aux utilisateurs de créer des vues personnalisées\\nsur le parc d’imprimantes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 16, 'page_label': '16', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 3. RÉALISATION TECHNIQUE 16\\nL’interface utilisateur offre une expérience moderne et réactive avec des mises à jour\\nen temps réel de l’état des imprimantes, des indicateurs visuels colorés pour les niveaux\\nde toner critiques, et des mécanismes de tri et filtrage avancés.\\n3.4 Gestion de la Qualité et Tests\\nLa stratégie de test inclut des tests unitaires pour la logique métier, des tests\\nd’intégration pour les appels SNMP, et des tests end-to-end pour les scénarios\\nutilisateurs critiques, avec un objectif de couverture de code supérieur à 80%.\\nDes tests unitaires et d’intégration ont été implémentés pour assurer la qualité du\\ncode. La couverture de test couvre les aspects critiques comme la communication SNMP,\\nla gestion des erreurs réseau, et la génération des rapports Excel.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 17, 'page_label': '17', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 4\\nDéploiement et Exploitation\\n4.1 Conteneurisation avec Docker\\nLa conteneurisation Docker permet un déploiement cohérent entre les\\nenvironnements de développement, test et production, avec une configuration par\\nvariables d’environnement et une orchestration simplifiée des services\\ninterdépendants.\\nL’application a été conteneurisée à l’aide de Docker, avec une configuration multi-\\nconteneurs via Docker Compose. Cette approche permet d’isoler les services, de gérer leurs\\ndépendances respectives et de simplifier le déploiement sur différentes environnements.\\n4.2 Déploiement en Production\\nLe déploiement en production suit une procédure standardisée avec validation en\\npré-production, rollback automatisé en cas d’échec, et monitoring des\\nperformances après déploiement pour détecter rapidement d’éventuels problèmes.\\nLe déploiement en production a suivi les étapes suivantes :\\n1. Construction des images Docker pour chaque service\\n2. Configuration des variables d’environnement pour la production\\n3. Déploiement sur un serveur dédié avec Docker Engine\\n4. Configuration du réseau pour permettre l’accès aux imprimantes\\n5. Mise en place de la persistance des données PostgreSQL\\n6. Tests de charge et validation des performances\\n4.3 Monitoring et Maintenance\\nLe système de monitoring inclut la journalisation centralisée des événements, la\\nsurveillance de la santé des services via des endpoints dédiés, et des alertes\\nproactives pour les imprimantes nécessitant une intervention technique préventive.\\nDes mécanismes de monitoring ont été implémentés :\\n— Journalisation centralisée des événements de l’application\\n— Surveillance de la santé des services avec des endpoints de health-check\\n— Alertes automatiques par email pour les imprimantes critiques\\n— Rapports planifiés générés automatiquement\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 18, 'page_label': '18', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 5\\nBilan et Perspectives\\n5.1 Bilan du Projet\\nLe projet a atteint ses objectifs principaux avec une application pleinement\\nfonctionnelle déployée en production, offrant une réduction de 70% du temps\\nconsacré à la gestion manuelle des imprimantes et une meilleure anticipation des\\nbesoins de maintenance.\\nLe projet a permis de développer une application complète de gestion d’imprimantes\\nréseau qui répond aux objectifs initiaux :\\nObjectifs Atteints\\n— Découverte automatique des imprimantes sur le réseau\\n— Surveillance en temps réel des consommables\\n— Interface web moderne et intuitive\\n— Export des données au format Excel\\n— Déploiement containerisé avec Docker\\nRésultats Techniques\\n— Backend Spring Boot robuste avec API RESTful\\n— Frontend React performant avec Tailwind CSS\\n— Base de données PostgreSQL optimisée\\n— Implémentation complète du protocole SNMP\\n— Automatisation du déploiement avec Docker\\n5.2 Difficultés Rencontrées et Solutions\\nLes principaux défis techniques ont inclus la gestion de l’hétérogénéité des\\nimplémentations SNMP selon les fabricants, l’optimisation des performances du\\nscan réseau, et la création d’une interface utilisateur intuitive pour une gestion\\nefficace d’un grand parc d’imprimantes.\\nPlusieurs défis techniques ont été rencontrés durant le développement :\\nHétérogénéité des Imprimantes\\n—Problème: Les différentes marques et modèles d’imprimantes implémentent le\\nprotocole SNMP de manière variable\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 19, 'page_label': '19', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 5. BILAN ET PERSPECTIVES 19\\n—Solution: Implémentation d’un système de détection de modèle avec des OIDs\\nspécifiques pour chaque fabricant\\nPerformances du Scan Réseau\\n—Problème: Le scan séquentiel des adresses IP était trop lent\\n—Solution: Implémentation d’un système multi-threadé avec un pool de threads\\npour paralléliser les requêtes\\nGestion des États dans l’Interface\\n—Problème: Complexité de la gestion d’état entre les différents composants React\\n—Solution: Utilisation du contexte React et custom hooks pour une gestion d’état\\ncentralisée\\n5.3 Perspectives d’Évolution\\nLes évolutions envisagées incluent l’intégration avec les systèmes de ticketing\\nexistants, la mise en œuvre de notifications temps réel, l’ajout de capacités\\nprédictives pour la maintenance, et la migration vers une architecture Kubernetes\\npour une meilleure scalabilité.\\nPlusieurs axes d’amélioration et d’évolution sont envisageables :\\nFonctionnalités à Ajouter\\n— Intégration avec des systèmes de ticketing (ServiceNow, Jira)\\n— Alertes temps réel via notifications push\\n— Historique des consommables et prédiction des besoins\\n— Gestion des droits d’accès multi-utilisateurs\\nAméliorations Techniques\\n— Migration vers une architecture microservices plus découpée\\n— Implémentation de caching Redis pour améliorer les performances\\n— Ajout de tests de charge et d’intégration continus\\n— Refactoring vers une PWA (Progressive Web App) pour une expérience mobile\\nnative\\nDéploiement et DevOps\\n— Mise en place d’une pipeline CI/CD complète\\n— Déploiement sur Kubernetes pour une meilleure scalabilité\\n— Monitoring avancé avec Prometheus et Grafana\\n— Sécurisation renforcée avec gestion centralisée des secrets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 20, 'page_label': '20', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 5. BILAN ET PERSPECTIVES 20\\n5.4 Apports Personnels\\nCe stage a permis l’acquisition de compétences techniques pointues en\\ndéveloppement full-stack et en gestion de projet, tout en développant des capacités\\nd’analyse et de résolution de problèmes techniques complexes dans un\\nenvironnement professionnel exigeant.\\nCe stage a été l’occasion de développer plusieurs compétences techniques et profes-\\nsionnelles :\\nCompétences Techniques Acquises\\n— Maîtrise approfondie de Spring Boot et l’écosystème Java\\n— Expérience significative avec React et les hooks avancés\\n— Première expérience avec le protocole SNMP et la communication réseau\\n— Pratique de la conteneurisation avec Docker et Docker Compose\\n— Gestion de base de données PostgreSQL avec Spring Data JPA\\nCompétences Méthodologiques\\n— Gestion de projet selon les méthodologies Agile\\n— Conception technique et architecture logicielle\\n— Rédaction de documentation technique complète\\n— Travail en équipe et collaboration avec les autres services'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 21, 'page_label': '21', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Conclusion\\nCe stage chez Yazaki a représenté une opportunité exceptionnelle de concevoir et\\ndévelopper une application complète from scratch, en intégrant des technologies\\nmodernes et en répondant à un besoin métier concret. L’application déployée\\napporte une valeur opérationnelle significative et ouvre la voie à de nombreuses\\névolutions futures.\\nCe stage de deux mois au sein de Yazaki a été une expérience extrêmement enrichis-\\nsante tant sur le plan technique que professionnel. Le développement de cette application\\nde gestion d’imprimantes réseau m’a permis de mettre en pratique les connaissances ac-\\nquises durant ma formation et d’acquérir de nouvelles compétences dans des technologies\\nmodernes.\\nLe projet a abouti à la création d’une application complète, allant de la découverte\\nautomatiquedesimprimantessurleréseauàlagénérationderapportsdétaillés,enpassant\\npar une interface utilisateur moderne et intuitive. L’utilisation de Spring Boot pour le\\nbackend, React pour le frontend, et Docker pour le déploiement a démontré l’efficacité de\\nces technologies pour développer des applications enterprise robustes et maintenables.\\nLes défis techniques rencontrés, particulièrement concernant l’hétérogénéité des impri-\\nmantes et l’optimisation des performances, ont été formateurs et m’ont appris à rechercher\\net implémenter des solutions adaptées à des problèmes complexes.\\nCette expérience chez Yazaki a confirmé mon intérêt pour le développement d’applica-\\ntions enterprise et m’a donné une vision concrète des enjeux techniques et organisationnels\\ndans un environnement professionnel. Je suis convaincu que les compétences acquises du-\\nrant ce stage seront un atout précieux pour ma future carrière dans le développement\\nlogiciel.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 22, 'page_label': '22', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Bibliographie\\n[1] Spring Boot Documentation.\\nhttps://spring.io/projects/spring-boot\\n[2] React Documentation.\\nhttps://reactjs.org/docs/getting-started.html\\n[3] Tailwind CSS Documentation.\\nhttps://tailwindcss.com/docs\\n[4] PostgreSQL Documentation.\\nhttps://www.postgresql.org/docs/\\n[5] Docker Documentation.\\nhttps://docs.docker.com/\\n[6] SNMP4J Documentation.\\nhttp://www.snmp4j.org/\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 23, 'page_label': '23', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Annexe A\\nAnnexes\\nA.1 Code Source\\nLe code source complet est disponible dans github avec des procédures de\\ndéploiement pour les environnements de développement et de production.\\nLe code source complet de l’application est disponible dans le lien github suivant :\\nhttps://github.com/medachbab/PrintersManagement\\nA.2 Manuel d’Utilisation\\nLe manuel d’utilisation couvre l’ensemble des fonctionnalités avec des captures\\nd’écran, des procédures pas à pas pour les opérations courantes, et un guide de\\ndépannage pour les problèmes techniques les plus fréquents.\\nUn manuel d’utilisation détaillé a été rédigé pour les administrateurs et utilisateurs\\nfinaux, couvrant :\\n— Installation et configuration de l’application\\n— Guide d’utilisation des différentes fonctionnalités\\n— Dépannage des problèmes courants\\n— Bonnes pratiques pour la gestion des imprimantes\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 24, 'page_label': '24', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='ANNEXE A. ANNEXES 24\\nA.3 Captures d’Écran\\nFigure A.1– home page de l’application\\nFigure A.2– Accès simplifié aux imprimantes via le tableau de bord'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 25, 'page_label': '25', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='ANNEXE A. ANNEXES 25\\nFigure A.3– Interface de découverte d’imprimantes\\nFigure A.4– Interface d’export des données')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb72e61",
   "metadata": {},
   "source": [
    "##### step2: chunking using text splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ce2c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter= RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    split_docs=text_splitter.split_documents(documents)\n",
    "    print(f\"splitted {len(documents)} into {len(split_docs)} chunks\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6c7205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted 96 into 125 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 0, 'page_label': '1', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Mohamed El Marouani\\nTDIA 2\\nLes fondements du Big Data'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 1, 'page_label': '2', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='2\\n1. Qu’est ce que les données?\\n2. Types des données\\n3. Impact des données\\n4. Caractéristiques des données (les V)\\n5. Data Journey\\n6. Qu’est ce que Big Data?\\n7. Big Data: cas d’utilisation\\n8. Evolution du Big Data\\n9. Paysage du Big Data'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 2, 'page_label': '3', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Qu’est ce que les données ?\\n\\uf0a7 Les données (Data) sont toutes les informations que vous collectez et \\nqui ont été organisées et structurées de manière à pouvoir être \\nanalysées. \\n« Data are a collection of discrete or continuous values that convey \\ninformation, describing the quantity, quality, fact, statistics, other basic \\nunits of meaning, or simply sequences of symbols that may be further \\ninterpreted formally “ - Wikipedia\\n\\uf0a7 Les données sont collectées à chaque fois que vous effectuez un \\nachat, que vous naviguez sur un site web, que vous voyagez, que \\nvous passez un appel téléphonique ou que vous publiez un message \\nsur un site de médias sociaux. \\n\\uf0a7 Les données peuvent provenir de nombreuses sources, notamment \\nde capteurs, d'enquêtes, d'expériences, d'observations ou \\nd'enregistrements existants (données historiques), comme les \\ntransactions financières. \\n 3\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Qu’est ce que les données ?\\n\\uf0a7 Les organisations modernes considèrent les données comme leur actif le plus précieux, car elles fournissent des \\ninformations sur le comportement des clients, les tendances du marché, les performances des produits, etc. qui \\naident à prendre des décisions éclairées sur l'affectation des ressources.\\n\\uf0a7 La théorie de l'information a poussé le concept de données beaucoup plus loin (Shannon, 1948). La théorie de \\nl'information est un domaine d'étude qui cherche à comprendre la nature et l'origine de l'information et, selon \\ncette étude, tout peut être considéré comme des données. Cela inclut les objets physiques et les concepts \\nabstraits tels que les idées ou les émotions. En outre, les données sont définies comme tout ensemble de symboles \\nqui transmettent un sens lorsqu'ils sont interprétés par un récepteur. Par conséquent, tout ce qui a une forme de \\nreprésentation symbolique (par exemple, des séquences d'ADN, des mots, des nombres) peut être classé comme\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 3, 'page_label': '4', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"représentation symbolique (par exemple, des séquences d'ADN, des mots, des nombres) peut être classé comme \\ndonnées dans ce contexte.\\n4\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 4, 'page_label': '5', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Types des données\\nD'un point de vue purement statistique, les données peuvent être classées en deux grandes catégories en fonction \\nde leur valeur :\\n\\uf0a7 Les données quantitatives (numériques) : il s'agit de toute information qui peut être exprimée, mesurée et \\ncomparée à l'aide de valeurs numériques, telles que des nombres entiers ou des nombres réels. Parmi les exemples \\nde données quantitatives, on peut citer la taille, le poids, la longueur, les relevés de température, la taille d'une \\npopulation ou des éléments dénombrables tels que le nombre d'élèves dans une salle de classe. \\nCe type de données peut être subdivisé en données discrètes (nombres entiers) ou continues (décimales).\\n• Données continues : données quantitatives qui peuvent être divisées de manière significative en niveaux plus fins. Elles peuvent être \\nmesurées sur une échelle ou un continuum. Elles peuvent avoir presque n'importe quelle valeur numérique : n'importe quelle va leur\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 4, 'page_label': '5', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"mesurées sur une échelle ou un continuum. Elles peuvent avoir presque n'importe quelle valeur numérique : n'importe quelle va leur \\ndans un intervalle fini ou infini (intervalle) ou une valeur qui compare deux nombres ou plus (rapport). Les exemples incluen t la taille, le \\npoids, la température, la vitesse, l'IMC et le temps.\\n• Données discrètes : consistent en des valeurs finies, numériques et dénombrables. Les valeurs discrètes ne peuvent pas être divisées en \\nparties. Les variables discrètes comprennent les dénombrements (par exemple, le nombre d'enfants dans un ménage), le nombre t otal \\nde produits ou les indicateurs binaires (oui/non, vrai/faux).\\n5\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 5, 'page_label': '6', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Types des données\\n\\uf0a7 Données qualitatives (catégorielles) : il s'agit d'informations non numériques \\ntelles que les opinions, les sentiments, les perceptions et les attitudes. Ces \\ndonnées peuvent répondre à des questions telles que : « Comment cela \\ns'est-il produit ? » ou “Pourquoi cela s'est-il produit ?”. Parmi les exemples de \\ndonnées qualitatives, on peut citer le sexe, les classements, les \\ndénombrements, etc. Ce type de données peut être divisé en deux \\ncatégories : les données nominales et les données ordinales.\\n• Données nominales : un type de données catégoriques qui n'a pas de valeur \\nnumérique ou d'ordre. Il s'agit de noms, d'étiquettes ou de catégories qui classent et \\norganisent les informations en groupes distincts. Les exemples incluent le sexe \\n(homme/femme), la nationalité (marocain/français) et les couleurs (vert/bleu).\\n• Données ordinales : ce type de données est associé à un ordre ou à un classement.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 5, 'page_label': '6', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='(homme/femme), la nationalité (marocain/français) et les couleurs (vert/bleu).\\n• Données ordinales : ce type de données est associé à un ordre ou à un classement. \\nLes exemples incluent les classements tels que 1er, 2ème et 3ème ; les notes telles que \\nA+, B- et C/D ; et les notes élevées, moyennes et basses.\\n6'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 6, 'page_label': '7', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Impact des données\\n7\\nLe modèle DIKW décrit la relation entre les données, \\nl'information, la connaissance et la sagesse.\\n\\uf0a7 Données / Data :\\n• Considérées comme la matière première d'une prise de \\ndécision avisée.\\n• Fournissent une base objective pour tirer des conclusions \\nou prendre des décisions.\\n\\uf0a7 Information / Information :\\n• Issue de l'analyse des données à l'aide de méthodes \\ncomme les statistiques ou l'apprentissage automatique.\\n• Permet de découvrir des schémas auparavant non \\névidents.\\n\\uf0a7 Connaissance / Knowledge :\\n• Résulte de la transformation des informations en savoir \\nstructuré.\\n• Sert de base aux processus de prise de décision.\\n\\uf0a7 Sagesse / Wisdom :\\n• Implique l'application des connaissances avec \\nexpérience et jugement.\\n• Permet de prendre des décisions éclairées sur les \\nstratégies et actions futures.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 7, 'page_label': '8', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Caractéristiques des données (les V) \\nLes cinq caractéristiques principales et innées des données sont :\\n\\uf0a7 Volume : La quantité de données qu'une organisation génère et stocke.\\n\\uf0a7 Vélocité : la vitesse à laquelle les données sont générées et la vitesse à laquelle elles \\nse déplacent et peuvent être traitées pour en tirer des informations exploitables.\\n\\uf0a7 Variété : la diversité des données. Les organisations peuvent collecter des données à \\npartir de sources multiples, dont le format peut varier. Les données collectées \\npeuvent être structurées, semi-structurées ou non structurées.\\n\\uf0a7 Véracité : fait référence au niveau de confiance et de fiabilité des données \\ncollectées. En d'autres termes, il s'agit de la qualité et de l'exactitude des données. \\nLes données collectées peuvent comporter des éléments manquants, être inexactes \\nou ne pas être en mesure de fournir une valeur réelle.\\n\\uf0a7 Valeur : se réfère à la valeur que les données peuvent apporter sur ce que les\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 7, 'page_label': '8', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"ou ne pas être en mesure de fournir une valeur réelle.\\n\\uf0a7 Valeur : se réfère à la valeur que les données peuvent apporter sur ce que les \\norganisations peuvent en faire. Cette caractéristique est directement liée à la \\nsignification et au contexte qu'une organisation peut donner aux données collectées.\\n8\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 8, 'page_label': '9', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Caractéristiques des données (les V) \\nDans le domaine du marketing, les experts en la matière ont commencé à utiliser deux caractéristiques \\nsupplémentaires qui ne sont pas innées aux données mais qui peuvent avoir un impact significatif sur les informations \\ngénérées à partir de celles-ci. Ces deux caractéristiques sont les suivantes\\n\\uf0a7 Variabilité : une mesure de la variation des valeurs dans chaque variante de données. Ce concept est lié au contexte des données \\net à la signification qui leur est donnée. Dans une organisation, la signification peut changer constamment, ce qui a un impa ct \\nsignificatif sur l'homogénéisation des données. Ce concept diffère de celui de variété : Imaginez un café qui propose six mél anges \\nde café différents (c'est la variété), mais si vous prenez le même mélange tous les jours. Il a un goût différent chaque jour ; c'est la \\nvariabilité.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 8, 'page_label': '9', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"de café différents (c'est la variété), mais si vous prenez le même mélange tous les jours. Il a un goût différent chaque jour ; c'est la \\nvariabilité.\\n\\uf0a7 Visualisation : La visualisation est essentielle dans le monde d'aujourd'hui. L'utilisation de tableaux et de graphiques pour visualiser de \\ngrandes quantités de données complexes est beaucoup plus efficace pour transmettre du sens que des données brutes dans des \\nfeuilles de calcul remplies de chiffres et de formules.\\n9\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 9, 'page_label': '10', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Data Journey\\nData Journey (parcours des données) se fait en plusieurs étapes. Les principales sont l'ingestion, le stockage, le traitement et la distribution. \\nChaque étape comporte son propre ensemble d'activités et de considérations. Le parcours des données comporte également une no tion \\nd'activités « sous-jacentes », c'est-à-dire des activités critiques tout au long du cycle de vie. Il s'agit notamment de la sécurité, de la gestion \\ndes données, du DataOps, de l'orchestration et de l'ingénierie logicielle.\\n10\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 10, 'page_label': '11', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Data Journey\\n1 - L'ingestion des données\\nL'ingestion des données est la première étape du cycle de vie des données. C'est à ce stade que les données \\nsont collectées à partir de diverses sources internes telles que les bases de données, les systèmes de gestion de la \\nrelation client (CRM), les systèmes d'information de gestion (ERP), les systèmes existants, les sources externes telles \\nque les enquêtes et les fournisseurs tiers. Il est important de s'assurer que les données acquises sont exactes et à \\njour afin de pouvoir les utiliser efficacement dans les étapes suivantes du cycle.\\nÀ ce stade, les données brutes sont extraites d'une ou de plusieurs sources de données, répliquées, puis intégrées \\ndans un support de stockage d'atterrissage. Ensuite, vous devez prendre en compte les caractéristiques des \\ndonnées que vous souhaitez acquérir pour vous assurer que l'étape d'ingestion des données dispose de la \\ntechnologie et des processus adéquats pour atteindre ses objectifs.\\n11\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 11, 'page_label': '12', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Data Journey\\n2 - Stockage des données\\nLe stockage des données désigne la manière dont les informations sont conservées après leur acquisition. Il \\nrepose sur des plateformes sécurisées et fiables, intégrant des mécanismes de sauvegarde essentiels à la reprise \\naprès sinistre. Par ailleurs, des contrôles d'accès stricts doivent être mis en place afin de protéger les données \\nsensibles contre toute tentative d’accès non autorisé ou malveillant.\\nLe choix d’une solution de stockage est une étape déterminante du cycle de vie des données, bien qu’il s’agisse \\nd’un processus complexe influencé par plusieurs facteurs. Parmi les principales caractéristiques du stockage, on \\ndistingue :\\n\\uf0a7 Le cycle de vie des données : la manière dont elles évoluent au fil du temps.\\n\\uf0a7 Les options de stockage : les différentes méthodes permettant d’optimiser leur conservation.\\n\\uf0a7 Les couches de stockage : la structuration des données en fonction de leur importance et de leur \\naccessibilité.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 11, 'page_label': '12', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='\\uf0a7 Les couches de stockage : la structuration des données en fonction de leur importance et de leur \\naccessibilité.\\n\\uf0a7 Les formats de stockage : le mode d’organisation des données selon leur fréquence d’accès.\\n\\uf0a7 Les technologies de stockage : les infrastructures sur lesquelles reposent les données.\\nBien que le stockage constitue une phase distincte du parcours des données, il s’intègre étroitement aux autres \\nétapes clés, telles que l’ingestion, la transformation et la mise à disposition des données. Il intervient à différents \\npoints du pipeline de traitement, se connectant aux systèmes sources et influençant la manière dont les données \\nsont exploitées à chaque phase. Ainsi, la stratégie de stockage adoptée a un impact direct sur l’efficacité \\nglobale du cycle de vie des données.\\n12'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 12, 'page_label': '13', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Data Journey\\n3 - Traitement des données\\nUne fois les données saisies et stockées, elles doivent être exploitées pour devenir véritablement utiles. L’étape suivante \\ndu cycle de vie des données est la transformation, qui consiste à convertir les données brutes en informations \\nexploitables pour les différents cas d’utilisation en aval.\\nLe traitement des données repose sur une série de transformations de base, essentielles pour garantir la cohérence et \\nl’exactitude des informations. Ces transformations incluent :\\n\\uf0a7 La conversion des types de données : transformation des chaînes de caractères (dates, valeurs numériques) en \\ntypes de données adaptés.\\n\\uf0a7 La normalisation des enregistrements : harmonisation des formats et structuration des données.\\n\\uf0a7 L’élimination des erreurs : suppression des entrées incorrectes ou incohérentes.\\nÀ mesure que le pipeline de traitement progresse, des transformations plus avancées peuvent être nécessaires, telles \\nque :'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 12, 'page_label': '13', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='À mesure que le pipeline de traitement progresse, des transformations plus avancées peuvent être nécessaires, telles \\nque :\\n\\uf0a7 L’adaptation ou la normalisation du schéma des données, pour assurer une compatibilité avec les systèmes en \\naval.\\n\\uf0a7 L’agrégation de données à grande échelle, notamment pour les besoins de reporting.\\n\\uf0a7 La transformation des données en vecteurs de caractéristiques (embeddings), pour les intégrer à des modèles \\nd’apprentissage automatique.\\nL’un des principaux défis de cette phase réside dans la précision et l’efficacité du traitement, qui nécessite une \\npuissance de calcul importante. Sans stratégies d’optimisation adéquates, ce processus peut s’avérer coûteux à long \\nterme. Ainsi, une gestion efficace des ressources et des performances est essentielle pour garantir un traitement rapide \\net fiable des données.\\n13'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 13, 'page_label': '14', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Data Journey\\n4 - Servir les données\\nVous avez atteint la dernière étape du parcours des données. Après avoir été ingérées, stockées et \\ntransformées en structures cohérentes et exploitables, il est temps d’en extraire toute la valeur.\\nLe service de données est l’étape où les informations prennent tout leur sens. C’est ici que les ingénieurs BI, \\nles ingénieurs en machine learning et les data scientists appliquent des techniques avancées pour générer \\ndes insights pertinents. Parmi les approches les plus courantes, on retrouve :\\n\\uf0a7 L’analyse des données : interprétation et exploration des informations stockées afin d’orienter la \\nprise de décision et d’anticiper les tendances futures.\\n\\uf0a7 La visualisation des données : utilisation d’outils spécialisés pour représenter graphiquement les \\nrésultats et faciliter leur compréhension.\\nCette phase constitue l’aboutissement du cycle de vie des données, permettant de transformer des'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 13, 'page_label': '14', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='résultats et faciliter leur compréhension.\\nCette phase constitue l’aboutissement du cycle de vie des données, permettant de transformer des \\nensembles bruts en ressources stratégiques et exploitables pour l’entreprise.\\n14'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 14, 'page_label': '15', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Qu’est ce que Big Data?\\nLe Big Data, ou données massives, désigne des ensembles de données \\nsi volumineux, variés et générés à une telle vitesse qu\\'ils dépassent les \\ncapacités des outils traditionnels de gestion et d\\'analyse de données. \\nCes caractéristiques sont souvent résumées par les \"3V\" :\\n\\uf0a7 Volume : Quantité massive de données générées.\\n\\uf0a7 Variété : Diversité des types de données (structurées, non \\nstructurées, semi-structurées).\\n\\uf0a7 Vélocité : Vitesse à laquelle ces données sont produites et \\ndoivent être traitées.\\nCertaines définitions ajoutent deux autres \"V\" :\\n\\uf0a7 Véracité : Fiabilité et qualité des données.\\n\\uf0a7 Valeur : Potentiel des données à générer des informations utiles.\\n15'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 15, 'page_label': '16', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Big Data: cas d’utilisations\\nSecteur financier (Détection de la fraude)\\n\\uf0a7 Descriptif : Les institutions financières analysent les comportements de transactions pour détecter et prévenir les \\nfraudes en temps réel.\\n\\uf0a7 Solution :\\n• Analyse des modèles de transactions avec des algorithmes de détection d'anomalies.\\n• Utilisation du Machine Learning pour reconnaître des comportements suspects.\\n• Mise en place de systèmes de scoring en temps réel (ex. systèmes de scoring de carte bancaire).\\n\\uf0a7 Challenges :\\n• Faux positifs qui peuvent impacter l’expérience client.\\n• Besoin de traitements en temps réel pour bloquer rapidement les fraudes.\\n• Complexité liée aux volumes de données générés par les transactions globales\\n16\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 16, 'page_label': '17', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Big Data: cas d’utilisations\\nSmart Cities (Gestion intelligente du trafic urbain)\\n\\uf0a7 Descriptif : Les villes intelligentes utilisent le Big Data pour optimiser la circulation et réduire les embouteillages.\\n\\uf0a7 Solution :\\n• Analyse des flux de circulation en temps réel à partir des capteurs et caméras.\\n• Modélisation des itinéraires optimaux en fonction des conditions actuelles.\\n• Intégration avec des applications mobiles (ex. Google Maps).\\n\\uf0a7 Challenges :\\n• Traitement et synchronisation de données en temps réel provenant de différentes sources.\\n• Sécurité et protection contre les cyberattaques.\\n• Acceptation par les citoyens et respect de leur vie privée.\\n17'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 17, 'page_label': '18', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Big Data: cas d’utilisations\\nSecteur de la santé (Prédiction des maladies et personnalisation des traitements)\\n\\uf0a7 Descriptif : L'analyse de grandes quantités de données médicales (dossiers patients, imageries médicales, \\ndonnées génétiques) permet de détecter des tendances et de proposer des traitements personnalisés.\\n\\uf0a7 Solution :\\n• Utilisation d'algorithmes d'apprentissage automatique pour identifier des modèles dans les données de \\nsanté.\\n• Intégration de données issues de capteurs portables (montres connectées, bracelets de santé).\\n• Plateformes de stockage et de traitement cloud (ex. AWS, Google Cloud Healthcare).\\n\\uf0a7 Challenges :\\n• Protection des données sensibles et conformité avec les réglementations (RGPD, HIPAA).\\n• Interopérabilité des différents systèmes hospitaliers.\\n• Fiabilité des algorithmes et des recommandations médicales. 18\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"Evolution du Big Data\\nBien que le concept de Big Data soit relativement nouveau, la nécessité de gérer des jeux de données volumineux \\nremonte aux années 1960 et 70, avec les premiers data centers et le développement des bases de données \\nrelationnelles.\\n\\uf0a7 Passé: En 2005, on assista à une prise de conscience de la quantité de données que les utilisateurs généraient sur \\nFacebook, YouTube et autres services en ligne. Apache Hadoop, une infrastructure open source créée \\nspécifiquement pour stocker et analyser de grands jeux de données, fut développé cette même année. NoSQL\\ncommença également à être de plus en plus utilisé à cette époque.\\n\\uf0a7 Présent: Le développement d’infrastructures open source, telles qu'Apache Hadoop et, plus récemment, Apache \\nSpark, a été primordial pour la croissance du Big Data, car celles-ci facilitent l’utilisation du Big Data et réduisent les \\ncoûts de stockage. Depuis, le volume du Big Data a explosé. Les utilisateurs génèrent toujours d’énormes quantités\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"coûts de stockage. Depuis, le volume du Big Data a explosé. Les utilisateurs génèrent toujours d’énormes quantités \\nde données, mais ce ne sont pas seulement les humains qui les utilisent.\\nAvec l’avènement de l’Internet of Things (IoT), de plus en plus d’objets et de terminaux sont connectés à Internet, \\ncollectant des données sur les habitudes d’utilisation des clients et les performances des produits. L’émergence \\ndu Machine Learning a produit encore plus de données.\\n\\uf0a7 Futur: Alors que le Big Data a fait des progrès, sa valeur continue de croître à mesure que l'IA générative et \\nl'utilisation du Cloud Computing se développent dans les entreprises. Le cloud offre une évolutivité considérable, \\nles développeurs peuvent simplement faire fonctionner rapidement des clusters dédiés pour tester un sous-\\nensemble de données. En outre, les bases de données graphiques deviennent de plus en plus importantes, avec\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 18, 'page_label': '19', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content=\"ensemble de données. En outre, les bases de données graphiques deviennent de plus en plus importantes, avec \\nleur capacité à afficher d'énormes quantités de données de manière à rendre les analyses rapides et complètes.\\n19\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-02-16T12:01:23+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-02-16T12:01:23+01:00', 'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf', 'total_pages': 20, 'page': 19, 'page_label': '20', 'source_file': 'bgDataFundamentals.pdf', 'file_type': 'pdf'}, page_content='Le paysage Big Data\\n20'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 0, 'page_label': '1', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Mohamed El Marouani\\nTDIA 2\\nApache Kafka\\n1'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 1, 'page_label': '2', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Event Streaming\\n2\\nDans un monde de plus en plus connecté, les systèmes doivent traiter des flux continus de données en temps réel.\\nEvents Streaming est une approche qui permet de collecter, traiter et analyser des événements au fur et à \\nmesure qu’ils se produisent.\\nCette technologie est au cœur des architectures modernes réactives, scalables et orientées données, utilisée \\ndans des domaines comme :\\n\\uf0a7 Les plateformes de streaming vidéo. \\n\\uf0a7 La surveillance en cybersécurité.\\n\\uf0a7 Le traitement des transactions financières. \\n\\uf0a7 L’IoT et les objets connectés.\\nElle repose sur des solutions comme Apache Kafka, Pulsar ou Kinesis, et change notre manière de penser les \\nsystèmes : de statiques à dynamiques, de batch à stream.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 2, 'page_label': '3', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"Kafka: Introduction\\n3\\n\\uf0a7 Apache Kafka est un framework open-source distribué conçu pour le streaming d’événements à grande \\néchelle.\\n\\uf0a7 Initialement développé par LinkedIn en 2011, il est aujourd’hui un standard industriel pour la gestion de flux de \\ndonnées en temps réel.\\n\\uf0a7 Kafka réunit trois capacités clés dans une seule solution éprouvée :\\no Publier et consommer des flux d’événements, y compris l’import/export continu de données depuis \\nd'autres systèmes.\\no Stocker ces flux de manière fiable et durable, aussi longtemps que nécessaire.\\no Traiter les événements en temps réel ou a posteriori.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 3, 'page_label': '4', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Kafka: Introduction\\n4\\n\\uf0a7 Kafka offre une infrastructure distribuée, scalable, élastique, tolérante aux pannes et sécurisée.\\n\\uf0a7 Kafka peut être déployé :\\no sur des serveurs physiques, machines virtuelles ou conteneurs \\no en local (on-premise) ou dans le cloud \\no en mode autogéré ou via des services managés'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 4, 'page_label': '5', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Kafka APIs\\n5\\nKafka fournit plusieurs APIs permettant d’interagir avec les flux de données à chaque étape du pipeline :\\n\\uf0a7 Producer API\\no Permet d’envoyer des événements dans des topics Kafka.\\no Utilisé par les applications génératrices de données (ex. logs, capteurs IoT, applications web).\\n\\uf0a7 Consumer API\\no Permet de lire des événements depuis des topics.\\no Utilisé pour construire des systèmes de traitement ou de réaction aux événements.\\n\\uf0a7 Streams API\\no API de traitement intégré au client Kafka, orientée microservices.\\no Permet de transformer, agréger, filtrer et joindre des flux en temps réel.\\no Basée sur une logique déclarative avec une sémantique \"event-at-a-time\".'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 5, 'page_label': '6', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='6\\n\\uf0a7 Kafka Connect API\\no Facilite la connexion de Kafka à des bases de données, systèmes de fichiers, cloud services, etc.\\no Repose sur des connecteurs prêts à l’emploi (source ou sink).\\n\\uf0a7 Admin API\\no Permet de créer, configurer et gérer des topics, des quotas, etc.\\no Utile pour l’automatisation et la supervision de l’infrastructure Kafka.\\nKafka APIs'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 6, 'page_label': '7', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Kafka: Concepts et terminologie\\n7\\n\\uf0a7 Event\\no Unité de donnée transmise dans Kafka.\\no Composé généralement d’une clé, d’une valeur, d’un horodatage et de métadonnées.\\no Exemple : {\"user_id\": 123, \"action\": \"login\"}\\n\\uf0a7 Topic\\no Canal de communication nommé dans lequel les événements sont publiés.\\no Divisé en partitions pour la scalabilité et la parallélisation.\\no Les events sont ordonnés par partition.\\n\\uf0a7 Partition\\no Une sous-division d’un topic ; chaque partition contient une séquence ordonnée d’événements.\\no Permet le traitement parallèle et la montée en charge.\\n\\uf0a7 Offset\\no Identifiant unique de chaque événement dans une partition.\\no Utilisé pour garder la position de lecture du consommateur.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 7, 'page_label': '8', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='8\\n\\uf0a7 Le modèle Pub/Sub\\nLe modèle Pub/Sub est un style de communication asynchrone dans lequel les producteurs (appelés publishers) \\nenvoient des messages à un canal nommé (ex. un topic) sans se soucier de qui les recevra.\\nLes consommateurs (ou subscribers) s’abonnent à ces canaux pour recevoir les messages pertinents, de manière \\ndécouplée.\\nKafka implémente le modèle Pub/Sub de façon distribuée, avec persistance des messages et gestion avancée \\ndes offsets.\\nKafka: Concepts et terminologie'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 8, 'page_label': '9', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"Kafka: Architecture\\n9\\nKafka repose sur une architecture distribuée composée de deux grandes catégories d’acteurs :\\n\\uf0a7 Serveurs (Côté Cluster Kafka)\\no Brokers : Nœuds qui reçoivent, stockent et distribuent les événements.\\no Topics & Partitions : Les données sont organisées par sujets, découpés en partitions pour la scalabilité.\\no ZooKeeper / KRaft : Gère la coordination, l’équilibrage de charge et la tolérance aux pannes (ZooKeeper\\nremplacé progressivement par KRaft).\\n\\uf0a7 Clients (Côté Applications)\\no Producers : Publient des événements dans un topic Kafka.\\no Consumers : Lisent les événements d’un ou plusieurs topics.\\no Kafka Streams : Clients intelligents qui consomment, traitent et republient des flux.\\no Kafka Connect : Connecte automatiquement Kafka à des systèmes externes (BD, fichiers, cloud, etc.).\\nKafka permet de découpler producteurs et consommateurs, et d'assurer une haute disponibilité grâce à la \\nréplication et à une architecture résiliente.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 9, 'page_label': '10', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"10\\nCritère Apache Kafka RabbitMQ\\nModèle Distributed Log (log distribué \\nd'événements)\\nMessage Broker (file d'attente orientée \\nmessage)\\nPersistance Stocke les messages durablement sur \\ndisque (log) Messages stockés mais orienté queue, non log\\nRétention des messages Messages conservés pour une durée \\ndéfinie\\nSupprimés dès qu’ils sont consommés (par \\ndéfaut)\\nPerformance Très haut débit (millions de msg/sec) Débit plus limité, latence faible\\nScalabilité Conçu pour la scalabilité horizontale Scalabilité plus difficile sans plugins\\nModèle Pub/Sub Pub/Sub natif avec topics & partitions Pub/Sub via exchanges et bindings\\nCas d’usage typique Event streaming, ETL temps réel, traitement \\nde logs\\nFile d’attente, communication interservices \\n(RPC)\\nAPI & Clients APIs Java natives, Kafka Streams, Connect Clients AMQP dans plusieurs langages\\nÉcosystème Kafka Connect, Kafka Streams, KSQL, \\nConfluent, Flink... Plugins AMQP, Shovel, Federation, UI intégrée\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 9, 'page_label': '10', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='Écosystème Kafka Connect, Kafka Streams, KSQL, \\nConfluent, Flink... Plugins AMQP, Shovel, Federation, UI intégrée\\nGestion de l’état Suivi de l’offset côté consommateur Suivi de l’état géré par le broker\\nApache Kafka vs RabbitMQ'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 10, 'page_label': '11', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='11\\nKafka: Producers\\n\\uf0a7 Lorsqu’on envoie un message à Kafka via un Producer, on \\ncommence par créer un ProducerRecord. Ce record doit\\nobligatoirement contenir :\\no le topic cible,\\no une valeur (le message).\\n\\uf0a7 On peut aussi ajouter optionnellement :\\no une clé,\\no une partition spécifique,\\no un horodatage,\\no et/ou des en-têtes personnalisés.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 11, 'page_label': '12', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='12\\nKafka: Producers\\nÉtapes principales :\\n1. Sérialisation :\\n\\uf0a7 Kafka convertit la clé et la valeur en tableaux d’octets pour les transmettre via le réseau.\\n2. Choix de la partition :\\n\\uf0a7 Si aucune partition n’est définie, un partitionneur détermine automatiquement à quelle partition envoyer\\nle message (souvent basé sur la clé).\\n3. Batching :\\n\\uf0a7 Kafka groupe les messages destinés à la même partition dans un batch avant de les envoyer.\\n4. Transmission :\\n\\uf0a7 Un thread séparé se charge d’envoyer ces batches aux brokers Kafka.\\n5. Réponse du broker :\\n\\uf0a7 En cas de succès → le broker renvoie un objet RecordMetadata (contenant le topic, la partition et \\nl’offset).\\n\\uf0a7 En cas d’échec → le broker renvoie une erreur, et le producer peut effectuer des tentatives de renvoi\\n(retries) avant d’abandonner.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 12, 'page_label': '13', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='13\\nProducers: Construction\\nAvant d’envoyer des messages à Kafka, il faut créer un objet producer avec une configuration adaptée. Il existe trois \\npropriétés obligatoires :\\n1. bootstrap.servers\\n\\uf0a7 Liste d’adresses hôte:port des brokers Kafka pour établir une première connexion.\\n\\uf0a7 Il suffit d’en fournir une partie, car le producer découvre les autres après connexion.\\n\\uf0a7 Il est conseillé d’en mettre au moins deux pour assurer une tolérance aux pannes.\\n2. key.serializer\\n\\uf0a7 Classe responsable de sérialiser les clés des messages en byte arrays.\\n\\uf0a7 Kafka attend des tableaux d’octets, mais on peut envoyer n’importe quel objet Java.\\n\\uf0a7 Le producer doit donc savoir comment convertir l’objet clé.\\n\\uf0a7 Il faut indiquer une classe qui implémente l’interface Serializer de Kafka.\\n\\uf0a7 Exemples inclus : StringSerializer, IntegerSerializer, ByteArraySerializer, etc.\\n\\uf0a7 Même si vous n’envoyez pas de clé, vous devez quand même définir cette propriété (ex. avec VoidSerializer).'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 13, 'page_label': '14', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='14\\nProducers: Construction\\n3. value.serializer\\n\\uf0a7 Même principe que pour key.serializer, mais pour la valeur du message.\\n\\uf0a7 Permet de transformer l’objet à envoyer en byte array.\\n\\uf0a7 Doit être défini pour garantir que Kafka puisse comprendre les messages produits.\\nNote : Ces propriétés sont essentielles pour garantir la bonne communication entre l’application et le cluster Kafka.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 14, 'page_label': '15', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='15\\nProducers: Construction\\nUne fois le producer Kafka instancié, on peut envoyer des messages de trois façons principales :\\n1. Fire-and-Forget (Envoyer sans retour)\\n\\uf0a7 Le message est envoyé sans attendre de confirmation.\\n\\uf0a7 Simple et rapide, mais aucune garantie de livraison en cas d’erreur non-récupérable ou de timeout.\\n\\uf0a7 Kafka est très disponible, donc cela fonctionne bien dans la majorité des cas.\\n\\uf0a7 Les erreurs ne sont pas remontées à l’application.\\n2. Synchronous Send (Envoi synchrone)\\n\\uf0a7 Le producer reste asynchrone en interne, mais on appelle .get() sur le Future retourné par send() pour \\nattendre le résultat.\\n\\uf0a7 Cela permet de savoir si le message a bien été délivré avant de passer au suivant.\\n\\uf0a7 Fiable, mais plus lent que l’envoi asynchrone.\\n3. Asynchronous Send with Callback (Envoi asynchrone avec rappel)\\n\\uf0a7 On appelle send() en fournissant une fonction de rappel (callback).\\n\\uf0a7 Cette fonction est exécutée dès que Kafka répond (succès ou échec).'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 14, 'page_label': '15', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='\\uf0a7 On appelle send() en fournissant une fonction de rappel (callback).\\n\\uf0a7 Cette fonction est exécutée dès que Kafka répond (succès ou échec).\\n\\uf0a7 Permet une exécution non bloquante tout en gérant les erreurs proprement.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 15, 'page_label': '16', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='16\\nProducers: Construction'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 16, 'page_label': '17', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='17\\nEnvoi synchrone d’un message Kafka:\\nL’envoi synchrone permet au producer :\\n\\uf0a7 de capturer les exceptions (ex. : erreurs de Kafka ou échec après plusieurs tentatives),\\n\\uf0a7 mais il présente un inconvénient majeur : la performance.\\n\\uf0a7 Inconvénient principal :\\no Le thread reste bloqué en attendant la réponse du broker Kafka (de 2 ms à plusieurs secondes selon la \\ncharge).\\no Pendant ce temps, il ne peut rien faire d’autre (pas même envoyer d’autres messages).\\no Cela entraîne une performance très faible, raison pour laquelle cette méthode n’est pas utilisée en \\nproduction, mais très fréquente dans les exemples pédagogiques.\\nProducers: Construction'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 17, 'page_label': '18', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='18\\nEnvoi asynchrone dans Kafka:\\nL’envoi asynchrone est rapide et efficace, surtout lorsque l’on n’a pas besoin d’attendre les réponses de Kafka.\\n\\uf0a7 Comparaison avec l’envoi synchrone :\\no Si le temps réseau aller-retour est de 10 ms, envoyer 100 messages en attendant à chaque fois prend ~1 \\nseconde.\\no Si on envoie tout sans attendre, c’est quasi instantané.\\no En général, l’application n’a pas besoin de la réponse (topic, partition, offset) mais doit savoir si une erreur \\nest survenue.\\n\\uf0a7 Solution : Ajouter un callback\\no Permet d’envoyer les messages sans blocage tout en gérant les erreurs.\\no Le callback est exécuté à la réception de la réponse Kafka, avec ou sans erreur.\\nProducers: Construction'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 18, 'page_label': '19', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='19\\nProducers: Temps de livraison des messages\\nDepuis Apache Kafka 2.1, ce temps est divisé en deux phases distinctes :\\n\\uf0a7 Temps jusqu’au retour de l’appel asynchrone à send() :\\no Pendant cette phase, le thread de l’application est bloqué (le temps de mise en lot du message, par ex.).\\n\\uf0a7 Temps entre le retour de send() et le déclenchement du callback :\\no Correspond au délai entre le moment où le message a été placé dans un batch à envoyer, et la réponse\\ndu broker Kafka :\\n\\uf0fc succès ,\\n\\uf0fc erreur non-récupérable ,\\n\\uf0fc ou expiration du délai d’envoi maximal configuré.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 19, 'page_label': '20', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='20\\nProducers: Serializers\\nKafka requiert des sérialiseurs pour convertir les objets Java en tableaux d’octets avant envoi. Kafka fournit des \\nsérialiseurs par défaut (ex : chaînes, entiers, tableaux d’octets), mais pour des objets plus complexes, vous devrez \\ncréer votre propre sérialiseur ou utiliser une bibliothèque de sérialisation.\\nCustom Serializer\\nPrenons une classe simple Customer avec un ID et un nom. Pour l’envoyer via Kafka, on crée un sérialiseur\\npersonnalisé : CustomerSerializer.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 20, 'page_label': '21', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='21\\nProducers: Serializers\\nExtrait du fonctionnement :\\n\\uf0a7 L’objet Customer est converti en un tableau d’octets (byte[]) via un ByteBuffer.\\n\\uf0a7 Format de sérialisation :\\no 4 octets : ID client\\no 4 octets : taille du nom (en UTF-8)\\no N octets : le nom lui-même\\nLimites :\\n\\uf0a7 Le code est fragile : tout changement dans la classe (ex : ajouter un champ) casse la compatibilité.\\n\\uf0a7 Le débogage des problèmes de compatibilité entre versions est complexe.\\n\\uf0a7 Si plusieurs équipes utilisent Kafka, elles doivent toutes maintenir la même logique de \\nsérialisation/désérialisation.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 21, 'page_label': '22', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"22\\nProducers: Serializers\\nRecommandations:\\n\\uf0a7 Éviter les sérialiseurs personnalisés quand possible.\\n\\uf0a7 Préférer des bibliothèques standardisées et robustes \\ncomme Avro, Protobuf, Thrift ou JSON.\\n\\uf0a7 Avro est fortement recommandé : il gère le \\nschéma, l'évolution de données, la compatibilité et \\npermet une meilleure interopérabilité.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 22, 'page_label': '23', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"23\\nProducers: Sérialisation avec Apache Avro\\nApache Avro est un format de sérialisation de données neutre vis-à-vis du langage. Conçu par Doug Cutting\\n(créateur de Hadoop), il facilite le partage de fichiers de données avec un grand nombre d’utilisateurs.\\n\\uf0a7 Les données sont décrites à l’aide d’un schéma JSON.\\n\\uf0a7 La sérialisation est binaire (ou JSON si besoin).\\n\\uf0a7 Le schéma est inclus avec les données pour permettre lecture/écriture sans ambiguïté.\\nPourquoi Avro avec Kafka ?\\nAvro est particulièrement adapté à Kafka car il gère l’évolution du schéma. Lorsqu’un producteur change de \\nschéma de manière compatible, les consommateurs n'ont pas besoin d’être mis à jour pour continuer à traiter les \\nmessages.\\nOriginal schema New schema\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 23, 'page_label': '24', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='24\\nCompatibilité des applications\\n\\uf0a7 Ancienne application : peut lire les nouveaux messages, mais getFaxNumber() renverra null car ce\\nchamp n’existe plus.\\n\\uf0a7 Nouvelle application : peut lire les anciens messages, mais getEmail() renverra null car l’email n’était pas \\nprésent à l’époque.\\n\\uf0a7 Résultat : aucune erreur bloquante, même si les schémas sont différents mais compatibles.\\nProducers: Sérialisation avec Apache Avro'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 24, 'page_label': '25', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='25\\nProducers: Sérialisation avec Apache Avro\\n\\uf0a7 Problème : stocker le schéma dans chaque message\\no Dans un fichier Avro, inclure le schéma complet ne pose pas trop de problème.\\nMais dans Kafka, inclure le schéma dans chaque message doublerait sa taille, ce qui est inefficace.\\no Solution : utiliser un Schema Registry\\n\\uf0a7 Schema Registry – Comment ça marche ?\\no Le Schema Registry est un service externe (pas intégré à Kafka) qui stocke tous les schémas utilisés.\\no Lorsqu’un message est produit :\\nOn n’envoie pas le schéma complet, mais simplement un identifiant du schéma.\\no Le consommateur utilise cet identifiant pour récupérer le schéma dans le registry et désérialiser le message.\\no Toute cette logique est gérée automatiquement par les sérialiseurs/désérialiseurs Avro. Le producteur Kafka \\nn’a rien à faire de spécial, il utilise juste un AvroSerializer comme un sérialiseur classique.\\n\\uf0a7 Exemple utilisé : Confluent Schema Registry\\no Outil open-source (ou intégré à la plateforme Confluent).'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 24, 'page_label': '25', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='\\uf0a7 Exemple utilisé : Confluent Schema Registry\\no Outil open-source (ou intégré à la plateforme Confluent).\\no Permet de stocker, versionner et gérer les schémas utilisés pour Kafka.\\no La documentation Confluent est recommandée pour la mise en œuvre.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 25, 'page_label': '26', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"26\\nKafka: Consumers\\n\\uf0a7 L'API Kafka Consumers est une interface fournie par Apache Kafka pour consommer des messages publiés \\ndans des topics Kafka. Un consumer est un client Kafka qui se connecte à un ou plusieurs topics, lit les \\nmessages disponibles dans ces topics et les traite.\\n\\uf0a7 Problème : Si le flux de messages est élevé, un consommateur unique peut être submergé et accumuler un \\nretard important.\\n\\uf0a7 Les consumers sont regroupés dans des consumer groups. Chaque consumer d'un groupe lit les messages d'un \\nensemble de partitions spécifique, garantissant ainsi que chaque message est consommé par un seul \\nconsumer du groupe.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 26, 'page_label': '27', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"27\\nKafka Consumers: Concepts\\n\\uf0a7 Consumer Groups:\\no Un groupe de consommateurs permet de distribuer le traitement des messages entre plusieurs \\nconsommateurs.\\no Chaque consommateur du groupe reçoit les messages d'un sous-ensemble des partitions d'un topic.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 27, 'page_label': '28', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"28\\nKafka Consumers: Concepts\\n\\uf0a7 Pour augmenter la capacité de traitement, on peut ajouter des consommateurs à un groupe.\\n\\uf0a7 Limite : Le nombre de partitions d'un topic détermine le nombre maximal de consommateurs actifs.\\n\\uf0a7 Plus le nombre de partitions est élevé, plus il est possible d'ajouter des consommateurs.\\n\\uf0a7 Exemple : Un topic avec 8 partitions peut supporter jusqu'à 8 consommateurs actifs dans un groupe.\\n\\uf0a7 Attention : Si un groupe a plus de consommateurs que de partitions, certains resteront inactifs.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 28, 'page_label': '29', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"29\\nKafka Consumers: Concepts\\n\\uf0a7 Créer des groupes de consommateurs distincts pour chaque application qui doit traiter tous les messages.\\n\\uf0a7 Exemple :\\no Application A utilise le groupe G1 pour traiter le topic T1.\\no Application B utilise le groupe G2 pour traiter le même topic T1.\\no Les consommateurs de G1 et G2 travaillent indépendamment sans interférer.\\n\\uf0a7 Avantage : Permet d'isoler le traitement des données pour différentes applications tout en partageant le \\nmême topic.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 29, 'page_label': '30', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='30\\nKafka Consumers: Rebalancement des partitions\\n\\uf0a7 Lorsqu’un consommateur rejoint ou quitte un groupe, les partitions sont réassignées automatiquement.\\n\\uf0a7 Objectif : Garantir la haute disponibilité et l’équilibre des charges.\\n\\uf0a7 Types de rebalancements :\\n\\uf0a7 Eager Rebalance : Tous les consommateurs stoppent, libèrent leurs partitions, puis récupèrent de nouvelles \\npartitions. Risque de latence accrue.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 30, 'page_label': '31', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='31\\n\\uf0a7 Cooperative Rebalance : Redistribution progressive des partitions pour éviter les interruptions complètes. \\nRecommandé pour les grands groupes.\\nKafka Consumers: Rebalancement des partitions'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 31, 'page_label': '32', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"32\\n\\uf0a7 Les consommateurs envoient des heartbeats pour signaler qu'ils sont actifs.\\n\\uf0a7 Le coordinateur de groupe surveille ces heartbeats pour détecter les pannes.\\n\\uf0a7 Si un consommateur cesse d'envoyer des heartbeats, le coordinateur le considère comme inactif et \\ndéclenche un rebalancement.\\n\\uf0a7 Paramètres essentiels :\\n\\uf0a7 heartbeat.interval.ms : Intervalle entre deux heartbeats.\\n\\uf0a7 session.timeout.ms : Durée maximale sans heartbeat avant qu'un consommateur soit considéré comme \\nmort.\\nKafka Consumers: Rebalancement des partitions\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 32, 'page_label': '33', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='33\\nKafka Consumers: Adhésion statique des groupes\\n\\uf0a7 Par défaut, un consommateur est identifié de manière temporaire dans un groupe.\\n\\uf0a7 Avec group.instance.id, un consommateur devient un membre statique :\\n\\uf0a7 Ses partitions ne sont pas réassignées après un redémarrage.\\n\\uf0a7 Il conserve ses partitions tant que le délai session.timeout.ms n’est pas dépassé.\\n\\uf0a7 Avantage : Maintien de l’état local (cache, traitement en cours).\\n\\uf0a7 Inconvénient : En cas de redémarrage long, risque de retard important.\\n\\uf0a7 Recommendations :\\no Utiliser cooperative rebalance pour minimiser les interruptions.\\no Configurer group.instance.id pour les applications nécessitant des états locaux persistants.\\no Prévoir un nombre de partitions suffisant pour permettre une montée en charge progressive.\\no Surveiller les heartbeats et ajuster les timeouts pour équilibrer réactivité et robustesse.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 33, 'page_label': '34', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"34\\nCréation d’un Kafka Consumer\\nPour consommer des enregistrements dans Kafka, la première étape consiste à créer une instance de \\nKafkaConsumer. La création d'un KafkaConsumer est similaire à celle d'un KafkaProducer : il faut d'abord créer\\nune instance de Properties pour définir les propriétés à utiliser. Les trois propriétés obligatoires pour démarrer sont :\\n1. bootstrap.servers : la chaîne de connexion au cluster Kafka (comme pour le producteur).\\n2. key.deserializer : classe pour convertir les clés des enregistrements de tableau d'octets en objets Java.\\n3. value.deserializer : classe pour convertir les valeurs des enregistrements de tableau d'octets en objets\\nJava.\\nUne quatrième propriété, non obligatoire mais couramment utilisée, est group.id, qui indique le groupe de \\nconsommateurs auquel appartient l'instance de KafkaConsumer.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 34, 'page_label': '35', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"35\\nConsumers: Souscription à un topic\\n\\uf0a7 Après avoir créé un KafkaConsumer, l'étape suivante consiste à s'abonner à un ou plusieurs topics.\\nLa méthode subscribe() accepte une liste de topics en paramètre. \\n\\uf0a7 Il est également possible d'utiliser une expression régulière avec subscribe(). Cela permet de s'abonner à \\nplusieurs topics dont les noms correspondent au motif spécifié. Si un nouveau topic correspondant au motif est\\ncréé, un rééquilibrage se produit immédiatement et les consommateurs commencent à consommer les \\ndonnées de ce nouveau topic. \\n\\uf0a7 Cette méthode est particulièrement utile pour les applications qui consomment des données de plusieurs \\ntopics ou pour les applications de traitement de flux.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 35, 'page_label': '36', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"36\\nConsumers: la boucle poll()\\nAu cœur de l'API Consumer se trouve une boucle simple qui interroge le serveur pour obtenir des données. Le \\ncode typique d'un consommateur Kafka ressemble à ceci :\\nCette boucle est infinie car les consommateurs sont généralement des applications longues durées qui \\nconsomment continuellement des données de Kafka.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 36, 'page_label': '37', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"37\\n\\uf0a7 Importance de poll() :\\no Les consommateurs doivent appeler poll() en permanence pour être considérés comme actifs.\\no Si poll() n'est pas invoqué pendant plus de max.poll.interval.ms, le consommateur est marqué comme\\ninactif et ses partitions sont réassignées à un autre consommateur.\\no Le paramètre timeout détermine le temps d'attente si aucun enregistrement n'est disponible dans le buffer.\\n\\uf0a7 Fonctionnement de poll() :\\no poll() retourne une liste de ConsumerRecords.\\no Chaque enregistrement contient : le topic, la partition, l'offset, la clé et la valeur.\\no Le traitement des enregistrements consiste généralement à écrire les résultats dans une base de données\\nou à mettre à jour un enregistrement existant.\\n\\uf0a7 Gestion des rééquilibrages : Le premier appel à poll() gère :\\no La recherche du GroupCoordinator.\\no L'adhésion au groupe de consommateurs.\\no L'assignation des partitions.\\no La gestion des rééquilibrages.\\nConsumers: la boucle poll()\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 37, 'page_label': '38', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"38\\nConfiguration des consommateurs\\nLes consommateurs Kafka peuvent être configurés à l'aide de plusieurs propriétés. Voici les principales \\nconfigurations :\\n\\uf0a7 fetch.min.bytes :\\no Définit la quantité minimale de données que le consommateur souhaite recevoir du broker lors d'un \\npoll().\\no Par défaut : 1 octet.\\no Augmenter cette valeur peut réduire la charge CPU en cas de faible activité sur le topic, mais peut\\négalement augmenter la latence.\\n\\uf0a7 fetch.max.wait.ms :\\no Temps d'attente maximal pour obtenir fetch.min.bytes.\\no Par défaut : 500 ms.\\no Un compromis entre latence et volume de données récupérées.\\no Exemple : Si fetch.min.bytes = 1 MB et fetch.max.wait.ms = 100 ms, Kafka renverra les données soit après 1 \\nMB de données, soit après 100 ms, selon la première condition atteinte.\\n\\uf0a7 fetch.max.bytes :\\no Quantité maximale de données retournées par poll().\\no Par défaut : 50 MB.\\no Contrôle la mémoire utilisée par le consommateur.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 38, 'page_label': '39', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"39\\n\\uf0a7 max.poll.records :\\no Nombre maximal de messages traités par appel à poll().\\no Permet de limiter le traitement des données lors de chaque itération de la boucle poll().\\n\\uf0a7 max.partition.fetch.bytes :\\no Limite la quantité de données par partition lors d'un poll().\\no Par défaut : 1 MB.\\no Recommandé d'utiliser fetch.max.bytes pour un meilleur contrôle.\\nConfiguration des consommateurs\\nGestion des temps et des délais:\\n\\uf0a7 session.timeout.ms : Temps pendant lequel le consommateur peut rester inactif sans être considéré comme\\nmort.\\n\\uf0a7 heartbeat.interval.ms : Fréquence d'envoi des heartbeats.\\n\\uf0a7 max.poll.interval.ms : Durée maximale entre deux appels poll() avant que le consommateur ne soit considéré\\ncomme mort.\\n\\uf0a7 default.api.timeout.ms : Délai par défaut pour toutes les requêtes API du consommateur.\\n\\uf0a7 request.timeout.ms : Délai maximal pour attendre une réponse du broker.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 39, 'page_label': '40', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='40\\nGestion des offsets :\\n\\uf0a7 auto.offset.reset : Comportement lorsque le consommateur ne trouve pas d\\'offset valide :\\no \"latest\" (par défaut) : commence à lire les nouveaux messages.\\no \"earliest\" : commence à lire depuis le début du topic.\\no \"none\" : lève une exception si l\\'offset est invalide.\\n\\uf0a7 enable.auto.commit :\\no Contrôle le commit automatique des offsets.\\no Par défaut : true.\\no Pour un contrôle manuel des offsets, définir sur false.\\nConfiguration des consommateurs'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 40, 'page_label': '41', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"41\\nConfiguration des consommateurs\\nStratégies d'assignation des partitions\\n\\uf0a7 Range :\\no Attribue les partitions de manière consécutive.\\no Peut créer des déséquilibres si le nombre de partitions n'est pas divisible par le nombre de \\nconsommateurs.\\n\\uf0a7 RoundRobin :\\no Attribue les partitions de manière circulaire, garantissant une répartition plus équilibrée.\\n\\uf0a7 Sticky :\\no Objectif : minimiser le déplacement des partitions lors des rééquilibrages.\\n\\uf0a7 Cooperative Sticky :\\no Identique à Sticky, mais permet aux consommateurs de continuer à consommer les partitions non \\nréassignées.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 41, 'page_label': '42', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"42\\nConfiguration des consommateurs\\nAutres propriétés importantes\\n\\uf0a7 client.id : Identifiant du consommateur, utilisé dans les logs et les métriques.\\n\\uf0a7 client.rack : Permet d'indiquer le datacenter ou la zone où se trouve le consommateur.\\n\\uf0a7 group.instance.id : Identifiant unique d'un consommateur pour une adhésion statique au groupe.\\n\\uf0a7 receive.buffer.bytes / send.buffer.bytes : Taille des buffers TCP. À ajuster pour des communications inter-\\ndatacenters.\\n\\uf0a7 offsets.retention.minutes : Durée pendant laquelle les offsets sont conservés après que le groupe soit \\ndevenu inactif.\\n\\uf0a7 Par défaut : 7 jours.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 42, 'page_label': '43', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"43\\nConsumers: Commits & Offsets\\nLorsque le consommateur appelle poll(), il reçoit les enregistrements non encore lus par le groupe de \\nconsommateurs. Kafka permet aux consommateurs de suivre leur position dans chaque partition à l'aide des offsets, \\nmais il ne gère pas les accusés de réception.\\nL'action d'actualiser la position dans une partition est appelée commit d'offset. Les offsets sont stockés dans le topic \\nspécial __consumer_offsets. En cas de crash ou d'ajout d'un nouveau consommateur, un rééquilibrage se produit et \\nles consommateurs se basent sur les offsets précédemment commités pour reprendre la consommation.\\nRisques :\\no Si le dernier offset commité est inférieur à l'offset du dernier message traité, certains messages seront traités deux\\nfois.\\no Si le dernier offset commité est supérieur à l'offset du dernier message traité, certains messages seront perdus.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 43, 'page_label': '44', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"44\\nConsumers: Commits & Offsets\\n1. Commit automatique :\\n\\uf0a7 En activant enable.auto.commit=true, le consommateur commit automatiquement l'offset toutes les 5 \\nsecondes (par défaut).\\n\\uf0a7 Problème : Si le consommateur plante avant le prochain commit, des messages peuvent être traités\\ndeux fois.\\n2. Commit manuel des offsets actuels :\\nPour plus de contrôle, il est possible de désactiver le commit automatique (enable.auto.commit=false) et de \\ncommiter manuellement à l'aide de commitSync().\\nExemple:\\nLimite : Si le commit est effectué avant le traitement complet des messages, certains d'entre eux peuvent être \\nignorés en cas de crash.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 44, 'page_label': '45', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"45\\nConsumers: Commits & Offsets\\n3. Commit asynchrone:\\nPour éviter le blocage de l'application pendant le commit, Kafka propose commitAsync().\\n\\uf0a7 Avantage : Non bloquant, améliore le débit.\\n\\uf0a7 Inconvénient : En cas de défaillance, il ne réessaie pas, ce qui peut causer des duplications ou des \\npertes de messages.\\n\\uf0a7 Exemple avec callback :\\n4. Combinaison des commits synchrone et asynchrone:\\nIl est recommandé de combiner commitAsync() pour le traitement normal et commitSync() juste avant la \\nfermeture du consommateur pour garantir la persistance des offsets.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 45, 'page_label': '46', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"46\\nConsumers: Commits & Offsets\\n5. Commit d'offsets spécifiques:\\nPour des cas où il est nécessaire de commiter des offsets avant la fin d'un batch, il est possible de passer une \\nmap d'offsets spécifiques :\\n\\uf0a7 Exemple:\\nCette approche permet un contrôle fin des engagements et réduit le risque de pertes ou de duplications de \\nmessages.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 46, 'page_label': '47', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"47\\nConsumers: Deserializers\\n\\uf0a7 Dans Kafka, les producteurs utilisent des sérialiseurs pour convertir des objets en tableaux d'octets avant de \\nles envoyer à Kafka. De même, les consommateurs utilisent des désérialiseurs pour convertir ces tableaux \\nd'octets en objets Java.\\n\\uf0a7 Dans les exemples précédents, nous avons utilisé le StringDeserializer par défaut pour les clés et les valeurs des \\nmessages. Cependant, pour des objets personnalisés, il est nécessaire de créer des désérialiseurs personnalisés\\nou d'utiliser des formats standardisés comme Avro.\\n\\uf0a7 Il est essentiel que le sérialiseur utilisé par le producteur corresponde au désérialiseur utilisé par le \\nconsommateur. Par exemple, un message sérialisé avec IntSerializer ne pourra pas être désérialisé\\ncorrectement avec StringDeserializer.\\no Avro et le Schema Registry permettent de gérer cette compatibilité en validant les schémas des \\nmessages produits et consommés.\"),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 47, 'page_label': '48', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='48\\nConsumers: Deserializers\\nVoici un exemple de désérialiseur personnalisé pour un objet Customer'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 48, 'page_label': '49', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content='49\\nConsumers: Deserializers\\nLimite :\\nLes désérialiseurs personnalisés sont fragiles car ils dépendent fortement de la structure des objets. Les erreurs de \\ncompatibilité peuvent survenir si les schémas changent.'),\n",
       " Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2025-05-20T15:25:22+01:00', 'title': 'PowerPoint Presentation', 'author': 'elmarouani', 'moddate': '2025-05-20T15:25:22+01:00', 'source': '..\\\\data\\\\pdfs\\\\kafka.pdf', 'total_pages': 50, 'page': 49, 'page_label': '50', 'source_file': 'kafka.pdf', 'file_type': 'pdf'}, page_content=\"50\\nConsumers: Deserializers\\nL'utilisation d'Avro permet de standardiser les messages et d'assurer la compatibilité des schémas.\\nExemple de désérialisation avec Avro :\\n\\uf0a7 schema.registry.url : Indique l'emplacement du Schema Registry.\\n\\uf0a7 KafkaAvroDeserializer : Désérialise les messages Avro.\\n\\uf0a7 specific.avro.reader : Permet d'utiliser des objets Avro générés.\"),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 0, 'page_label': '1', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='UNIVERSITE ABDELMALEK\\nESSAADI\\nECOLE NATIONALE DES\\nSCIENCES APPLIQUEES\\nD’AL HOCEIMA\\nRapport de Stage de Fin d’Année\\nApplication de Gestion Centralisée\\ndes Imprimantes de la production\\nRéalisé par :Achbab Mohammed\\nEncadré par :Mr.Bouya Amine\\nDépartement :IT\\nFilière :Transformation Digitale et Intelligence Artificielle\\nNiveau :2eme année cycle ingénieur\\nDurée :Du 01/07/2025 au 1/09/2025\\nAnnée Universitaire : 2024 – 2025'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 1, 'page_label': '1', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Remerciements\\nJe tiens à exprimer ma profonde gratitude à toutes les personnes qui ont contribué,\\nde près ou de loin, à la réussite de ce stage et à la réalisation de ce projet.\\nMes remerciements les plus sincères s’adressent à mon encadrantMr. Bouya\\nAminepour son accompagnement constant, ses conseils précieux et sa patience.\\nSa supervision a été déterminante pour l’accomplissement de mon projet et\\nl’acquisition de nouvelles compétences.\\nJ’exprime ma gratitude à l’ensemble du personnel de l’entreprise Yazaki pour leur\\naccueil chaleureux, leur disponibilité et les connaissances qu’ils ont bien voulu partager\\navec moi.\\nEnfin, un grand merci à ma famille et à mes amis pour leur soutien et leurs encoura-\\ngements tout au long de ce parcours.\\n1'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 2, 'page_label': '2', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Table des matières\\nRemerciements 1\\nListe des figures 4\\nListe des tableaux 5\\nIntroduction 6\\n1 Contexte et Objectifs du Stage 7\\n1.1 Présentation de l’Entreprise . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.2 Contexte du Projet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.3 Objectifs du Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2 Analyse des Besoins et Conception 9\\n2.1 Analyse Fonctionnelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Analyse Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3 Conception UML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.3.1 Diagramme de Cas d’Utilisation . . . . . . . . . . . . . . . . . . . . 12\\n2.3.2 Diagramme de Classes . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3 Réalisation Technique 14\\n3.1 Environnement de Développement . . . . . . . . . . . . . . . . . . . . . . . 14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 2, 'page_label': '2', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='2.3.2 Diagramme de Classes . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3 Réalisation Technique 14\\n3.1 Environnement de Développement . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2 Architecture Détaillée . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2.1 Backend Spring Boot . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2.2 Frontend React . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n3.2.3 Base de Données PostgreSQL . . . . . . . . . . . . . . . . . . . . . 15\\n3.3 Implémentation des Fonctionnalités Principales . . . . . . . . . . . . . . . 15\\n3.3.1 Découverte Automatique des Imprimantes . . . . . . . . . . . . . . 15\\n3.3.2 Communication SNMP . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.3.3 Interface Utilisateur React . . . . . . . . . . . . . . . . . . . . . . . 15\\n3.4 Gestion de la Qualité et Tests . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4 Déploiement et Exploitation 17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 2, 'page_label': '2', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='3.4 Gestion de la Qualité et Tests . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4 Déploiement et Exploitation 17\\n4.1 Conteneurisation avec Docker . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.2 Déploiement en Production . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.3 Monitoring et Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n5 Bilan et Perspectives 18\\n5.1 Bilan du Projet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n5.2 Difficultés Rencontrées et Solutions . . . . . . . . . . . . . . . . . . . . . . 18\\n5.3 Perspectives d’Évolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n5.4 Apports Personnels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\nConclusion 21\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 3, 'page_label': '3', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='TABLE DES MATIÈRES 3\\nBibliographie 22\\nA Annexes 23\\nA.1 Code Source . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\nA.2 Manuel d’Utilisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\nA.3 Captures d’Écran . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 4, 'page_label': '4', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Table des figures\\n1.1 Vue d’une desccription de l’histoire de Yazaki . . . . . . . . . . . . . . . . 7\\n2.1 Diagramme de Cas d’Utilisation . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.2 Diagramme de classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nA.1 home page de l’application . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\nA.2 Accès simplifié aux imprimantes via le tableau de bord . . . . . . . . . . . 24\\nA.3 Interface de découverte d’imprimantes . . . . . . . . . . . . . . . . . . . . 25\\nA.4 Interface d’export des données . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 5, 'page_label': '5', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Liste des tableaux\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 6, 'page_label': '6', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Introduction\\nCe stage de deux mois réalisé au sein de Yazaki Meknes avait pour objectif\\nprincipal le développement d’une application complète de gestion d’imprimantes\\nréseau. Dans un contexte où la gestion des ressources d’impression constitue un\\nenjeu opérationnel majeur, cette application vise à centraliser le monitoring, la\\ndécouverte automatique et la gestion des imprimantes connectées au réseau de\\nl’entreprise.\\nDans le cadre de ma formation en Transformation Digitale et Intelligence Artificielle,\\nj’ai effectué un stage de deux mois au sein de l’entreprise Yazaki, spécialisée dans la\\nconception et la fabrication de systèmes électriques et électroniques pour l’industrie au-\\ntomobile. Ce stage avait pour objectif principal la conception et le développement d’une\\napplication de gestion d’imprimantes réseau.\\nLagestionduparcd’imprimantesreprésenteunenjeuimportantpourlesentreprisesde\\ntaille moyenne à grande. Le suivi des compteurs de pages, de la connectivitée, des niveaux'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 6, 'page_label': '6', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Lagestionduparcd’imprimantesreprésenteunenjeuimportantpourlesentreprisesde\\ntaille moyenne à grande. Le suivi des compteurs de pages, de la connectivitée, des niveaux\\nde toner,et l’état général des imprimantes sont essentiels pour assurer la continuité des\\nservices d’impression et anticiper les besoins de maintenance.\\nDurantcestage,j’aidéveloppéuneapplicationwebcomplètepermettantladécouverte,\\nle monitoring et la gestion des imprimantes connectées au réseau d’entreprise. L’outil offre\\nla possibilité de suivre en temps réel différents indicateurs tels que l’état de connexion\\n(imprimante en ligne ou hors ligne), le nombre total de pages imprimées, le niveau de\\ntoner ainsi que l’état général des périphériques. La découverte des imprimantes se fait en\\nrenseignant l’adresse d’un ou de plusieurs sous-réseaux, ce qui permet d’élargir la portée\\nde la supervision et d’obtenir une vision centralisée du parc d’impression.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 6, 'page_label': '6', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='renseignant l’adresse d’un ou de plusieurs sous-réseaux, ce qui permet d’élargir la portée\\nde la supervision et d’obtenir une vision centralisée du parc d’impression.\\nCe rapport présente en détail le travail réalisé, depuis l’analyse des besoins jusqu’au\\ndéploiement de l’application, en passant par la conception technique et le développement.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 7, 'page_label': '7', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 1\\nContexte et Objectifs du Stage\\n1.1 Présentation de l’Entreprise\\nYazaki est un leader mondial dans la fabrication de systèmes de câblage\\nautomobile et de instruments de mesure. Fondée en 1929 au Japon, l’entreprise\\nemploie plus de 200 000 personnes dans 45 pays et possède une forte présence dans\\nle secteur automobile avec des innovations technologiques constantes.\\nYazaki est une entreprise spécialisée dans la conception et la fabrication de systèmes\\nélectriques et électroniques pour l’industrie automobile. Fondée en 1941 au Japon, elle\\nemploie aujourd’hui plus de 190 000 collaborateurs et possède une présence internationale\\navec des implantations dans plus de 45 pays. Les activités principales de Yazaki incluent la\\nproduction de faisceaux de câblage, de systèmes de distribution électrique, de composants\\nélectroniques et de solutions pour la gestion de l’énergie dans les véhicules.\\nFigure 1.1– Vue d’une desccription de l’histoire de Yazaki\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 8, 'page_label': '8', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 1. CONTEXTE ET OBJECTIFS DU STAGE 8\\n1.2 Contexte du Projet\\nLa gestion du parc d’imprimantes chez Yazaki Meknes représentait un défi\\nopérationnel significatif avec plus de 200 imprimantes réparties sur différents sites,\\nnécessitant une surveillance manuelle fastidieuse et sujette à des oublis ou retards\\ndans la maintenance préventive.\\nAu sein de Yazaki, la gestion du parc d’imprimantes était jusqu’alors réalisée manuel-\\nlement par les équipes informatiques. Cette approche présentait plusieurs limitations :\\n— Absence de centralisation des informations sur l’état des imprimantes\\n— Difficulté à anticiper les pannes et les besoins en consommables\\n— Processus fastidieux pour inventorier les nouvelles imprimantes sur le réseau\\n— Manque de reporting automatisé sur l’utilisation des imprimantes\\nFace à ces constats, il a été décidé de développer une application interne de gestion\\nd’imprimantes réseau permettant de résoudre ces problématiques.\\n1.3 Objectifs du Stage'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 8, 'page_label': '8', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Face à ces constats, il a été décidé de développer une application interne de gestion\\nd’imprimantes réseau permettant de résoudre ces problématiques.\\n1.3 Objectifs du Stage\\nLes objectifs techniques principaux incluaient la maîtrise du protocole SNMP, le\\ndéveloppement full-stack avec des technologies modernes, et la conteneurisation de\\nl’application pour un déploiement simplifié en environnement de production.\\nLes objectifs principaux de ce stage étaient :\\n1. Analyser les besoins fonctionnels et techniques de l’application\\n2. Concevoir l’architecture technique de la solution\\n3. Développer une application web complète avec frontend et backend\\n4. Implémenter le protocole SNMP pour la communication avec les imprimantes\\n5. Réaliser des fonctionnalités de découverte automatique des imprimantes sur le ré-\\nseau\\n6. Mettre en place un système de reporting et d’export de données\\n7. Déployer l’application à l’aide de Docker'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 9, 'page_label': '9', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 2\\nAnalyse des Besoins et Conception\\n2.1 Analyse Fonctionnelle\\nL’analyse fonctionnelle a identifié quatre domaines critiques : la découverte\\nautomatique, la surveillance en temps réel, la gestion des alertes et la génération\\nde rapports, répondant aux besoins spécifiques des équipes techniques de Yazaki.\\nL’analyse des besoins a permis d’identifier les fonctionnalités principales suivantes :\\nFonctionnalités Administratives\\n— Découverte automatique des imprimantes sur le réseau\\n— Inventaire centralisé des imprimantes avec leurs caractéristiques\\n— Surveillance en temps réel des niveaux de toner et des compteurs de pages\\n— Gestion des statuts des imprimantes (en ligne, hors ligne, en impression, etc.)\\nFonctionnalités de Reporting\\n— Tableau de bord avec indicateurs visuels\\n— Export des données au format Excel\\n— Filtres avancés pour la recherche d’imprimantes\\n— Alertes automatiques pour les imprimantes nécessitant une intervention\\n2.2 Analyse Technique'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 9, 'page_label': '9', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='— Export des données au format Excel\\n— Filtres avancés pour la recherche d’imprimantes\\n— Alertes automatiques pour les imprimantes nécessitant une intervention\\n2.2 Analyse Technique\\nL’architecture technique retenue privilégie une approche microservices avec une\\nAPI RESTful, permettant une séparation claire des responsabilités et une\\névolutivité optimale pour les futures extensions du système.\\nL’analyse technique a conduit aux choix suivants :\\nArchitecture\\n— Architecture microservices avec séparation frontend/backend\\n— API RESTful pour la communication entre les composants\\n— Base de données relationnelle pour la persistance des données\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 10, 'page_label': '10', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 10\\nTechnologies\\nLogo Technologie Description\\nSpring Boot (Java) Utilisé pour le backend grâce à sa robustesse et\\nson large écosystème.\\nReact.js + Tail-\\nwind CSS\\nDéveloppement du frontend avec une interface\\nmoderne, responsive et modulaire.\\nPostgreSQL Base de données relationnelle fiable, performante\\net open-source.\\nSNMP Protocole réseau utilisé pour le monitoring et la\\ncommunication avec les imprimantes.\\nDocker Conteneurisation pour simplifier le déploiement\\net isoler les services.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 11, 'page_label': '11', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 12, 'page_label': '12', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 12\\n2.3 Conception UML\\n2.3.1 Diagramme de Cas d’Utilisation\\nFigure 2.1– Diagramme de Cas d’Utilisation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 13, 'page_label': '13', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 2. ANALYSE DES BESOINS ET CONCEPTION 13\\n2.3.2 Diagramme de Classes\\nFigure 2.2– Diagramme de classes'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 14, 'page_label': '14', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 3\\nRéalisation Technique\\n3.1 Environnement de Développement\\nL’environnement de développement a été configuré avec une approche DevOps,\\nintégrant le versioning Git, une pipeline CI/CD naissante, et des outils de qualité\\nde code pour assurer la maintenabilité du projet sur le long terme.\\nL’environnement de développement mis en place comprenait :\\n—IDE: IntelliJ IDEA pour le backend Java, VS Code pour le frontend React\\n—Versioning: Git avec repository GitLab d’entreprise\\n—Gestion de projet: Méthodologie Agile avec des sprints de deux semaines\\n—Tests: JUnit pour le backend, Jest et React Testing Library pour le frontend\\n3.2 Architecture Détaillée\\n3.2.1 Backend Spring Boot\\nL’architecture backend repose sur le pattern MVC avec une couche de service\\nrobuste gérant la logique métier, une couche de repository pour l’accès aux\\ndonnées, et un contrôleur exposant une API RESTful documentée et sécurisée.\\nLe backend a été développé avec Spring Boot, offrant une structure robuste et modu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 14, 'page_label': '14', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='données, et un contrôleur exposant une API RESTful documentée et sécurisée.\\nLe backend a été développé avec Spring Boot, offrant une structure robuste et modu-\\nlaire. Le contrôleur principal expose une API RESTful avec les endpoints suivants :\\n—GET /printers- Lister les imprimantes avec filtres\\n—POST /discoverPrinters- Découvrir de nouvelles imprimantes\\n—POST /refreshAllPrinters- Actualiser les données des imprimantes\\n—GET /printers/download/excel- Exporter les données en Excel\\n—GET /discoveryProgress- Suivre la progression de la découverte\\n3.2.2 Frontend React\\nL’interface utilisateur a été conçue avec une approche component-first, favorisant\\nla réutilisabilité et la maintenabilité. L’utilisation de Tailwind CSS a permis de\\ncréer une interface moderne et responsive sans dépendre à des bibliothèques de\\ncomposants externes.\\nLe frontend utilise React avec une architecture composée de plusieurs modules. L’in-\\nterface utilisateur est divisée en plusieurs vues :'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 14, 'page_label': '14', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='composants externes.\\nLe frontend utilise React avec une architecture composée de plusieurs modules. L’in-\\nterface utilisateur est divisée en plusieurs vues :\\n—Tableau de bord: Vue d’ensemble avec indicateurs visuels\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 15, 'page_label': '15', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 3. RÉALISATION TECHNIQUE 15\\n—Liste des imprimantes: Tableau interactif avec fonctionnalités de filtrage\\n—Découverte: Interface pour lancer la découverte d’imprimantes\\n—Export: Gestion de l’export des données\\n3.2.3 Base de Données PostgreSQL\\nLe schéma de base de données a été optimisé pour les requêtes de reporting tout\\nen maintenant l’intégrité des données. Des index stratégiques ont été implémentés\\nsur les champs fréquemment interrogés comme l’adresse IP et le statut des\\nimprimantes.\\nLa base de données PostgreSQL stocke les informations des imprimantes avec une\\nstructure relationnelle normalisée permettant des requêtes complexes et des jointures ef-\\nficaces pour la génération de rapports.\\n3.3 Implémentation des Fonctionnalités Principales\\n3.3.1 Découverte Automatique des Imprimantes\\nL’algorithme de découverte combine un scan réseau par ping avec une\\ninterrogation SNMP ciblée, utilisant un pool de threads pour paralléliser les'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 15, 'page_label': '15', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='3.3.1 Découverte Automatique des Imprimantes\\nL’algorithme de découverte combine un scan réseau par ping avec une\\ninterrogation SNMP ciblée, utilisant un pool de threads pour paralléliser les\\nrequêtes et réduire significativement le temps de scan d’un réseau complet.\\nLa découverte automatique utilise une combinaison de ping réseau et d’interrogation\\nSNMP. Le processus s’exécute en plusieurs phases : scan du réseau pour identifier les ap-\\npareils actifs, interrogation SNMP pour identifier les imprimantes, et enfin enregistrement\\nen base de données avec toutes les informations techniques recueillies.\\n3.3.2 Communication SNMP\\nL’implémentation SNMP gère les particularités des différents fabricants\\nd’imprimantes via un système d’OIDs (Object Identifiers) spécifiques, avec des\\nmécanismes de fallback pour assurer une compatibilité maximale avec le parc\\nhétérogène d’imprimantes.\\nLe service SNMP implémente la communication avec les imprimantes en utilisant la'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 15, 'page_label': '15', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='mécanismes de fallback pour assurer une compatibilité maximale avec le parc\\nhétérogène d’imprimantes.\\nLe service SNMP implémente la communication avec les imprimantes en utilisant la\\nbibliothèque SNMP4J. Cette implémentation gère les timeouts, les retries et les erreurs de\\ncommunication spécifiques au protocole SNMP, avec une gestion robuste des exceptions.\\n3.3.3 Interface Utilisateur React\\nL’interface utilise des indicateurs visuels avancés avec des graphiques\\nsemi-circulaires pour représenter l’état des consommables, et un système de\\nfiltrage dynamique permettant aux utilisateurs de créer des vues personnalisées\\nsur le parc d’imprimantes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 16, 'page_label': '16', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 3. RÉALISATION TECHNIQUE 16\\nL’interface utilisateur offre une expérience moderne et réactive avec des mises à jour\\nen temps réel de l’état des imprimantes, des indicateurs visuels colorés pour les niveaux\\nde toner critiques, et des mécanismes de tri et filtrage avancés.\\n3.4 Gestion de la Qualité et Tests\\nLa stratégie de test inclut des tests unitaires pour la logique métier, des tests\\nd’intégration pour les appels SNMP, et des tests end-to-end pour les scénarios\\nutilisateurs critiques, avec un objectif de couverture de code supérieur à 80%.\\nDes tests unitaires et d’intégration ont été implémentés pour assurer la qualité du\\ncode. La couverture de test couvre les aspects critiques comme la communication SNMP,\\nla gestion des erreurs réseau, et la génération des rapports Excel.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 17, 'page_label': '17', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 4\\nDéploiement et Exploitation\\n4.1 Conteneurisation avec Docker\\nLa conteneurisation Docker permet un déploiement cohérent entre les\\nenvironnements de développement, test et production, avec une configuration par\\nvariables d’environnement et une orchestration simplifiée des services\\ninterdépendants.\\nL’application a été conteneurisée à l’aide de Docker, avec une configuration multi-\\nconteneurs via Docker Compose. Cette approche permet d’isoler les services, de gérer leurs\\ndépendances respectives et de simplifier le déploiement sur différentes environnements.\\n4.2 Déploiement en Production\\nLe déploiement en production suit une procédure standardisée avec validation en\\npré-production, rollback automatisé en cas d’échec, et monitoring des\\nperformances après déploiement pour détecter rapidement d’éventuels problèmes.\\nLe déploiement en production a suivi les étapes suivantes :\\n1. Construction des images Docker pour chaque service'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 17, 'page_label': '17', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='performances après déploiement pour détecter rapidement d’éventuels problèmes.\\nLe déploiement en production a suivi les étapes suivantes :\\n1. Construction des images Docker pour chaque service\\n2. Configuration des variables d’environnement pour la production\\n3. Déploiement sur un serveur dédié avec Docker Engine\\n4. Configuration du réseau pour permettre l’accès aux imprimantes\\n5. Mise en place de la persistance des données PostgreSQL\\n6. Tests de charge et validation des performances\\n4.3 Monitoring et Maintenance\\nLe système de monitoring inclut la journalisation centralisée des événements, la\\nsurveillance de la santé des services via des endpoints dédiés, et des alertes\\nproactives pour les imprimantes nécessitant une intervention technique préventive.\\nDes mécanismes de monitoring ont été implémentés :\\n— Journalisation centralisée des événements de l’application\\n— Surveillance de la santé des services avec des endpoints de health-check'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 17, 'page_label': '17', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Des mécanismes de monitoring ont été implémentés :\\n— Journalisation centralisée des événements de l’application\\n— Surveillance de la santé des services avec des endpoints de health-check\\n— Alertes automatiques par email pour les imprimantes critiques\\n— Rapports planifiés générés automatiquement\\n17'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 18, 'page_label': '18', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Chapitre 5\\nBilan et Perspectives\\n5.1 Bilan du Projet\\nLe projet a atteint ses objectifs principaux avec une application pleinement\\nfonctionnelle déployée en production, offrant une réduction de 70% du temps\\nconsacré à la gestion manuelle des imprimantes et une meilleure anticipation des\\nbesoins de maintenance.\\nLe projet a permis de développer une application complète de gestion d’imprimantes\\nréseau qui répond aux objectifs initiaux :\\nObjectifs Atteints\\n— Découverte automatique des imprimantes sur le réseau\\n— Surveillance en temps réel des consommables\\n— Interface web moderne et intuitive\\n— Export des données au format Excel\\n— Déploiement containerisé avec Docker\\nRésultats Techniques\\n— Backend Spring Boot robuste avec API RESTful\\n— Frontend React performant avec Tailwind CSS\\n— Base de données PostgreSQL optimisée\\n— Implémentation complète du protocole SNMP\\n— Automatisation du déploiement avec Docker\\n5.2 Difficultés Rencontrées et Solutions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 18, 'page_label': '18', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='— Base de données PostgreSQL optimisée\\n— Implémentation complète du protocole SNMP\\n— Automatisation du déploiement avec Docker\\n5.2 Difficultés Rencontrées et Solutions\\nLes principaux défis techniques ont inclus la gestion de l’hétérogénéité des\\nimplémentations SNMP selon les fabricants, l’optimisation des performances du\\nscan réseau, et la création d’une interface utilisateur intuitive pour une gestion\\nefficace d’un grand parc d’imprimantes.\\nPlusieurs défis techniques ont été rencontrés durant le développement :\\nHétérogénéité des Imprimantes\\n—Problème: Les différentes marques et modèles d’imprimantes implémentent le\\nprotocole SNMP de manière variable\\n18'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 19, 'page_label': '19', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 5. BILAN ET PERSPECTIVES 19\\n—Solution: Implémentation d’un système de détection de modèle avec des OIDs\\nspécifiques pour chaque fabricant\\nPerformances du Scan Réseau\\n—Problème: Le scan séquentiel des adresses IP était trop lent\\n—Solution: Implémentation d’un système multi-threadé avec un pool de threads\\npour paralléliser les requêtes\\nGestion des États dans l’Interface\\n—Problème: Complexité de la gestion d’état entre les différents composants React\\n—Solution: Utilisation du contexte React et custom hooks pour une gestion d’état\\ncentralisée\\n5.3 Perspectives d’Évolution\\nLes évolutions envisagées incluent l’intégration avec les systèmes de ticketing\\nexistants, la mise en œuvre de notifications temps réel, l’ajout de capacités\\nprédictives pour la maintenance, et la migration vers une architecture Kubernetes\\npour une meilleure scalabilité.\\nPlusieurs axes d’amélioration et d’évolution sont envisageables :\\nFonctionnalités à Ajouter'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 19, 'page_label': '19', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='pour une meilleure scalabilité.\\nPlusieurs axes d’amélioration et d’évolution sont envisageables :\\nFonctionnalités à Ajouter\\n— Intégration avec des systèmes de ticketing (ServiceNow, Jira)\\n— Alertes temps réel via notifications push\\n— Historique des consommables et prédiction des besoins\\n— Gestion des droits d’accès multi-utilisateurs\\nAméliorations Techniques\\n— Migration vers une architecture microservices plus découpée\\n— Implémentation de caching Redis pour améliorer les performances\\n— Ajout de tests de charge et d’intégration continus\\n— Refactoring vers une PWA (Progressive Web App) pour une expérience mobile\\nnative\\nDéploiement et DevOps\\n— Mise en place d’une pipeline CI/CD complète\\n— Déploiement sur Kubernetes pour une meilleure scalabilité\\n— Monitoring avancé avec Prometheus et Grafana\\n— Sécurisation renforcée avec gestion centralisée des secrets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 20, 'page_label': '20', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='CHAPITRE 5. BILAN ET PERSPECTIVES 20\\n5.4 Apports Personnels\\nCe stage a permis l’acquisition de compétences techniques pointues en\\ndéveloppement full-stack et en gestion de projet, tout en développant des capacités\\nd’analyse et de résolution de problèmes techniques complexes dans un\\nenvironnement professionnel exigeant.\\nCe stage a été l’occasion de développer plusieurs compétences techniques et profes-\\nsionnelles :\\nCompétences Techniques Acquises\\n— Maîtrise approfondie de Spring Boot et l’écosystème Java\\n— Expérience significative avec React et les hooks avancés\\n— Première expérience avec le protocole SNMP et la communication réseau\\n— Pratique de la conteneurisation avec Docker et Docker Compose\\n— Gestion de base de données PostgreSQL avec Spring Data JPA\\nCompétences Méthodologiques\\n— Gestion de projet selon les méthodologies Agile\\n— Conception technique et architecture logicielle\\n— Rédaction de documentation technique complète'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 20, 'page_label': '20', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Compétences Méthodologiques\\n— Gestion de projet selon les méthodologies Agile\\n— Conception technique et architecture logicielle\\n— Rédaction de documentation technique complète\\n— Travail en équipe et collaboration avec les autres services'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 21, 'page_label': '21', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Conclusion\\nCe stage chez Yazaki a représenté une opportunité exceptionnelle de concevoir et\\ndévelopper une application complète from scratch, en intégrant des technologies\\nmodernes et en répondant à un besoin métier concret. L’application déployée\\napporte une valeur opérationnelle significative et ouvre la voie à de nombreuses\\névolutions futures.\\nCe stage de deux mois au sein de Yazaki a été une expérience extrêmement enrichis-\\nsante tant sur le plan technique que professionnel. Le développement de cette application\\nde gestion d’imprimantes réseau m’a permis de mettre en pratique les connaissances ac-\\nquises durant ma formation et d’acquérir de nouvelles compétences dans des technologies\\nmodernes.\\nLe projet a abouti à la création d’une application complète, allant de la découverte\\nautomatiquedesimprimantessurleréseauàlagénérationderapportsdétaillés,enpassant\\npar une interface utilisateur moderne et intuitive. L’utilisation de Spring Boot pour le'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 21, 'page_label': '21', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='automatiquedesimprimantessurleréseauàlagénérationderapportsdétaillés,enpassant\\npar une interface utilisateur moderne et intuitive. L’utilisation de Spring Boot pour le\\nbackend, React pour le frontend, et Docker pour le déploiement a démontré l’efficacité de\\nces technologies pour développer des applications enterprise robustes et maintenables.\\nLes défis techniques rencontrés, particulièrement concernant l’hétérogénéité des impri-\\nmantes et l’optimisation des performances, ont été formateurs et m’ont appris à rechercher\\net implémenter des solutions adaptées à des problèmes complexes.\\nCette expérience chez Yazaki a confirmé mon intérêt pour le développement d’applica-\\ntions enterprise et m’a donné une vision concrète des enjeux techniques et organisationnels\\ndans un environnement professionnel. Je suis convaincu que les compétences acquises du-\\nrant ce stage seront un atout précieux pour ma future carrière dans le développement\\nlogiciel.\\n21'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 22, 'page_label': '22', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Bibliographie\\n[1] Spring Boot Documentation.\\nhttps://spring.io/projects/spring-boot\\n[2] React Documentation.\\nhttps://reactjs.org/docs/getting-started.html\\n[3] Tailwind CSS Documentation.\\nhttps://tailwindcss.com/docs\\n[4] PostgreSQL Documentation.\\nhttps://www.postgresql.org/docs/\\n[5] Docker Documentation.\\nhttps://docs.docker.com/\\n[6] SNMP4J Documentation.\\nhttp://www.snmp4j.org/\\n22'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 23, 'page_label': '23', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='Annexe A\\nAnnexes\\nA.1 Code Source\\nLe code source complet est disponible dans github avec des procédures de\\ndéploiement pour les environnements de développement et de production.\\nLe code source complet de l’application est disponible dans le lien github suivant :\\nhttps://github.com/medachbab/PrintersManagement\\nA.2 Manuel d’Utilisation\\nLe manuel d’utilisation couvre l’ensemble des fonctionnalités avec des captures\\nd’écran, des procédures pas à pas pour les opérations courantes, et un guide de\\ndépannage pour les problèmes techniques les plus fréquents.\\nUn manuel d’utilisation détaillé a été rédigé pour les administrateurs et utilisateurs\\nfinaux, couvrant :\\n— Installation et configuration de l’application\\n— Guide d’utilisation des différentes fonctionnalités\\n— Dépannage des problèmes courants\\n— Bonnes pratiques pour la gestion des imprimantes\\n23'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 24, 'page_label': '24', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='ANNEXE A. ANNEXES 24\\nA.3 Captures d’Écran\\nFigure A.1– home page de l’application\\nFigure A.2– Accès simplifié aux imprimantes via le tableau de bord'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.27', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-18T15:57:56+00:00', 'author': '', 'keywords': '', 'moddate': '2025-09-18T15:57:56+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.27 (TeX Live 2025) kpathsea version 6.4.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdfs\\\\rapport_de_stage.pdf', 'total_pages': 26, 'page': 25, 'page_label': '25', 'source_file': 'rapport_de_stage.pdf', 'file_type': 'pdf'}, page_content='ANNEXE A. ANNEXES 25\\nFigure A.3– Interface de découverte d’imprimantes\\nFigure A.4– Interface d’export des données')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks= split_documents(all_pdf_docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a92c1fa",
   "metadata": {},
   "source": [
    "#### step3: embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e08b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the embedding model:all-MiniLM-L6-v2\n",
      "the model: all-MiniLM-L6-v2 is loaded successfuly, embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x24378372ba0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model()\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"loading the embedding model:{self.model_name}\")\n",
    "            self.model=SentenceTransformer(self.model_name)\n",
    "            print(f\"the model: {self.model_name} is loaded successfuly, embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading the model: {self.model_name}: {e}\")\n",
    "            raise\n",
    "    def generate_embeddings(self, texts: List[str])->np.ndarray:\n",
    "        \"\"\"\n",
    "        this function, takes a list of text strings to embed\n",
    "        and returns the numpy array of embeddings with shape(len(texts), embedding_dimention(384 in this case))\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"model not loaded\")\n",
    "        print(f\"generating embeddings for {len(texts)} texts\")\n",
    "        embeddings= self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "embeddingManager=EmbeddingManager()\n",
    "embeddingManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00c9d7",
   "metadata": {},
   "source": [
    "#### step4: vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f51267c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f4f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector store initialazed,  collection:pdf_documents\n",
      "existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2432a32a270>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str=\"pdf_documents\", persist_directory: str=\"../data/vector_store\"):\n",
    "        self.collection_name= collection_name\n",
    "        self.persist_directory=persist_directory\n",
    "        self.client=None\n",
    "        self.collection=None\n",
    "        self._initialize_store()\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client=chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            self.collection=self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF documents embeddings for rag\"}\n",
    "            )\n",
    "            print(f\"vector store initialazed,  collection:{self.collection_name}\")\n",
    "            print(f\"existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error in initializing the vector store: {e}\")\n",
    "            raise\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        this function adds the list of langchain docs with there embeddings to the vector store\n",
    "        Arguments:\n",
    "            documents: list of langchain documents\n",
    "            embeddings: correspondng embeddings for documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"the number of documents must match the number of embeddings\")\n",
    "        print(f\"adding{len(documents)} documents to the document store\")\n",
    "\n",
    "        # 1- preparing data for chromadb\n",
    "        ids=[]\n",
    "        metadatas= []\n",
    "        documents_content= []\n",
    "        embedding_list= []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            \n",
    "            #generate a unique id using uuid and the index:\n",
    "            doc_id= f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #metadata:\n",
    "            metadata= dict(doc.metadata)\n",
    "            metadata['doc_index']= i\n",
    "            metadata['doc_length']= len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #document contents:\n",
    "            documents_content.append(doc.page_content)\n",
    "\n",
    "            #embeddings:\n",
    "            embedding_list.append(embedding.tolist())\n",
    "        # 2- adding prepared data to collection:\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embedding_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_content\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"error adding documents to the vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorStore=VectorStore()\n",
    "vectorStore\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e848d1",
   "metadata": {},
   "source": [
    "we covert the chunks texts to embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5729e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating embeddings for 125 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07653fa5510d40b296e99bffb5eb7d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (125, 384)\n",
      "adding125 documents to the document store\n"
     ]
    }
   ],
   "source": [
    "#1- extracting page_content from the chunks documents\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "#2- generating embeddings\n",
    "embeddings= embeddingManager.generate_embeddings(texts)\n",
    "#3- storing the results in the vectore db:\n",
    "vectorStore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed30a40",
   "metadata": {},
   "source": [
    "#### 2- Retrieval pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e326d5",
   "metadata": {},
   "source": [
    "user--> query --> embedded-->similarity search in the vector db --> to get the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "501a899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self, vectorStore: VectorStore, embeddingManager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        initialize the retriever\n",
    "        Args:\n",
    "            vector_store: Vector store containing documments embeddings\n",
    "            embedding_manager: manager to embed the query(users question)\n",
    "        \"\"\"\n",
    "        self.vectorStore= vectorStore\n",
    "        self.embeddingManager=embeddingManager\n",
    "    def retrieve(self, query: str, top_k: int= 5, score_threshold: float=0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevent documents in the vectore store based on the users embedded query\n",
    "\n",
    "        Args:\n",
    "            query: the search query in the str format\n",
    "            top_k: number of top results to return\n",
    "            score_threshold: minimum similarity score threshold \n",
    "        \"\"\"\n",
    "        print(\"retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\")\n",
    "        #step1: embedding the str query:\n",
    "        query_embedding= self.embeddingManager.generate_embeddings([query])[0]\n",
    "        #step2: searching in the vector store:\n",
    "        try:\n",
    "            results= self.vectorStore.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            #process the results:\n",
    "            retrieved_docs=[]\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents= results['documents'][0]\n",
    "                metadatas= results['metadatas'][0]\n",
    "                distances= results['distances'][0]\n",
    "                print(f\"retrieved {len(documents)} documents before filtering\")\n",
    "                ids= results['ids'][0]\n",
    "                for i, (document, metadata, distance, id) in enumerate(zip(documents, metadatas, distances, ids)):\n",
    "                    # we convert the cosine_distance calculated by default in chromadb to the similarity score\n",
    "                    # the smaller the distance the higher the similarity score\n",
    "                    similarity_score= 1-distance\n",
    "                    if similarity_score>=score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                print(f\"retrieved {len(retrieved_docs)} documents after filtering\")\n",
    "            else:\n",
    "                print(\"no corresponding documents found\")\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(\"error retrieving the context from the knowledge base: {e}\") \n",
    "rag_retriever= RAGRetriever(vectorStore, embeddingManager)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce1cfe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x2432a328ec0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f895a078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72e0849075c489b97a0be37192fae16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 5 documents before filtering\n",
      "retrieved 5 documents after filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_d9fe184d_15',\n",
       "  'content': \"Data Journey\\n1 - L'ingestion des données\\nL'ingestion des données est la première étape du cycle de vie des données. C'est à ce stade que les données \\nsont collectées à partir de diverses sources internes telles que les bases de données, les systèmes de gestion de la \\nrelation client (CRM), les systèmes d'information de gestion (ERP), les systèmes existants, les sources externes telles \\nque les enquêtes et les fournisseurs tiers. Il est important de s'assurer que les données acquises sont exactes et à \\njour afin de pouvoir les utiliser efficacement dans les étapes suivantes du cycle.\\nÀ ce stade, les données brutes sont extraites d'une ou de plusieurs sources de données, répliquées, puis intégrées \\ndans un support de stockage d'atterrissage. Ensuite, vous devez prendre en compte les caractéristiques des \\ndonnées que vous souhaitez acquérir pour vous assurer que l'étape d'ingestion des données dispose de la \\ntechnologie et des processus adéquats pour atteindre ses objectifs.\\n11\",\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'creationdate': '2025-02-16T12:01:23+01:00',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'total_pages': 20,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'doc_index': 15,\n",
       "   'page_label': '11',\n",
       "   'page': 10,\n",
       "   'author': 'elmarouani',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'source_file': 'bgDataFundamentals.pdf',\n",
       "   'doc_length': 989},\n",
       "  'similarity_score': 0.2912670373916626,\n",
       "  'distance': 0.7087329626083374,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_7e1fc09e_3',\n",
       "  'content': \"Qu’est ce que les données ?\\n\\uf0a7 Les organisations modernes considèrent les données comme leur actif le plus précieux, car elles fournissent des \\ninformations sur le comportement des clients, les tendances du marché, les performances des produits, etc. qui \\naident à prendre des décisions éclairées sur l'affectation des ressources.\\n\\uf0a7 La théorie de l'information a poussé le concept de données beaucoup plus loin (Shannon, 1948). La théorie de \\nl'information est un domaine d'étude qui cherche à comprendre la nature et l'origine de l'information et, selon \\ncette étude, tout peut être considéré comme des données. Cela inclut les objets physiques et les concepts \\nabstraits tels que les idées ou les émotions. En outre, les données sont définies comme tout ensemble de symboles \\nqui transmettent un sens lorsqu'ils sont interprétés par un récepteur. Par conséquent, tout ce qui a une forme de \\nreprésentation symbolique (par exemple, des séquences d'ADN, des mots, des nombres) peut être classé comme\",\n",
       "  'metadata': {'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'creationdate': '2025-02-16T12:01:23+01:00',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'doc_index': 3,\n",
       "   'doc_length': 998,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'author': 'elmarouani',\n",
       "   'page_label': '4',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'bgDataFundamentals.pdf',\n",
       "   'page': 3,\n",
       "   'total_pages': 20,\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'creator': 'Microsoft® PowerPoint® 2016'},\n",
       "  'similarity_score': 0.22305738925933838,\n",
       "  'distance': 0.7769426107406616,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_0aa626d2_8',\n",
       "  'content': '(homme/femme), la nationalité (marocain/français) et les couleurs (vert/bleu).\\n• Données ordinales : ce type de données est associé à un ordre ou à un classement. \\nLes exemples incluent les classements tels que 1er, 2ème et 3ème ; les notes telles que \\nA+, B- et C/D ; et les notes élevées, moyennes et basses.\\n6',\n",
       "  'metadata': {'creationdate': '2025-02-16T12:01:23+01:00',\n",
       "   'total_pages': 20,\n",
       "   'doc_length': 312,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'doc_index': 8,\n",
       "   'page_label': '6',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'page': 5,\n",
       "   'source_file': 'bgDataFundamentals.pdf',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'author': 'elmarouani'},\n",
       "  'similarity_score': 0.21175503730773926,\n",
       "  'distance': 0.7882449626922607,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_166807fb_12',\n",
       "  'content': \"Caractéristiques des données (les V) \\nDans le domaine du marketing, les experts en la matière ont commencé à utiliser deux caractéristiques \\nsupplémentaires qui ne sont pas innées aux données mais qui peuvent avoir un impact significatif sur les informations \\ngénérées à partir de celles-ci. Ces deux caractéristiques sont les suivantes\\n\\uf0a7 Variabilité : une mesure de la variation des valeurs dans chaque variante de données. Ce concept est lié au contexte des données \\net à la signification qui leur est donnée. Dans une organisation, la signification peut changer constamment, ce qui a un impa ct \\nsignificatif sur l'homogénéisation des données. Ce concept diffère de celui de variété : Imaginez un café qui propose six mél anges \\nde café différents (c'est la variété), mais si vous prenez le même mélange tous les jours. Il a un goût différent chaque jour ; c'est la \\nvariabilité.\",\n",
       "  'metadata': {'doc_length': 882,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'page_label': '9',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'doc_index': 12,\n",
       "   'page': 8,\n",
       "   'creationdate': '2025-02-16T12:01:23+01:00',\n",
       "   'total_pages': 20,\n",
       "   'source_file': 'bgDataFundamentals.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'elmarouani',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016'},\n",
       "  'similarity_score': 0.14942097663879395,\n",
       "  'distance': 0.850579023361206,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_2c1c0c15_2',\n",
       "  'content': \"Qu’est ce que les données ?\\n\\uf0a7 Les données (Data) sont toutes les informations que vous collectez et \\nqui ont été organisées et structurées de manière à pouvoir être \\nanalysées. \\n« Data are a collection of discrete or continuous values that convey \\ninformation, describing the quantity, quality, fact, statistics, other basic \\nunits of meaning, or simply sequences of symbols that may be further \\ninterpreted formally “ - Wikipedia\\n\\uf0a7 Les données sont collectées à chaque fois que vous effectuez un \\nachat, que vous naviguez sur un site web, que vous voyagez, que \\nvous passez un appel téléphonique ou que vous publiez un message \\nsur un site de médias sociaux. \\n\\uf0a7 Les données peuvent provenir de nombreuses sources, notamment \\nde capteurs, d'enquêtes, d'expériences, d'observations ou \\nd'enregistrements existants (données historiques), comme les \\ntransactions financières. \\n 3\",\n",
       "  'metadata': {'title': 'PowerPoint Presentation',\n",
       "   'doc_length': 876,\n",
       "   'creationdate': '2025-02-16T12:01:23+01:00',\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'doc_index': 2,\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'bgDataFundamentals.pdf',\n",
       "   'page': 2,\n",
       "   'author': 'elmarouani',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'page_label': '3',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'total_pages': 20},\n",
       "  'similarity_score': 0.12036353349685669,\n",
       "  'distance': 0.8796364665031433,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Que veut dire les données\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df8d51",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b40439c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f48c03dd20c49849b0e8f53d0ce0ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 5 documents before filtering\n",
      "retrieved 5 documents after filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_c519377b_49',\n",
       "  'content': '18\\nEnvoi asynchrone dans Kafka:\\nL’envoi asynchrone est rapide et efficace, surtout lorsque l’on n’a pas besoin d’attendre les réponses de Kafka.\\n\\uf0a7 Comparaison avec l’envoi synchrone :\\no Si le temps réseau aller-retour est de 10 ms, envoyer 100 messages en attendant à chaque fois prend ~1 \\nseconde.\\no Si on envoie tout sans attendre, c’est quasi instantané.\\no En général, l’application n’a pas besoin de la réponse (topic, partition, offset) mais doit savoir si une erreur \\nest survenue.\\n\\uf0a7 Solution : Ajouter un callback\\no Permet d’envoyer les messages sans blocage tout en gérant les erreurs.\\no Le callback est exécuté à la réception de la réponse Kafka, avec ou sans erreur.\\nProducers: Construction',\n",
       "  'metadata': {'page': 17,\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 50,\n",
       "   'page_label': '18',\n",
       "   'moddate': '2025-05-20T15:25:22+01:00',\n",
       "   'doc_index': 49,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'source_file': 'kafka.pdf',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'author': 'elmarouani',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'doc_length': 700,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\kafka.pdf',\n",
       "   'creationdate': '2025-05-20T15:25:22+01:00'},\n",
       "  'similarity_score': 0.4053112864494324,\n",
       "  'distance': 0.5946887135505676,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_2d367f33_45',\n",
       "  'content': '15\\nProducers: Construction\\nUne fois le producer Kafka instancié, on peut envoyer des messages de trois façons principales :\\n1. Fire-and-Forget (Envoyer sans retour)\\n\\uf0a7 Le message est envoyé sans attendre de confirmation.\\n\\uf0a7 Simple et rapide, mais aucune garantie de livraison en cas d’erreur non-récupérable ou de timeout.\\n\\uf0a7 Kafka est très disponible, donc cela fonctionne bien dans la majorité des cas.\\n\\uf0a7 Les erreurs ne sont pas remontées à l’application.\\n2. Synchronous Send (Envoi synchrone)\\n\\uf0a7 Le producer reste asynchrone en interne, mais on appelle .get() sur le Future retourné par send() pour \\nattendre le résultat.\\n\\uf0a7 Cela permet de savoir si le message a bien été délivré avant de passer au suivant.\\n\\uf0a7 Fiable, mais plus lent que l’envoi asynchrone.\\n3. Asynchronous Send with Callback (Envoi asynchrone avec rappel)\\n\\uf0a7 On appelle send() en fournissant une fonction de rappel (callback).\\n\\uf0a7 Cette fonction est exécutée dès que Kafka répond (succès ou échec).',\n",
       "  'metadata': {'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'doc_length': 960,\n",
       "   'page': 14,\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'elmarouani',\n",
       "   'page_label': '15',\n",
       "   'moddate': '2025-05-20T15:25:22+01:00',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\kafka.pdf',\n",
       "   'creationdate': '2025-05-20T15:25:22+01:00',\n",
       "   'total_pages': 50,\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'source_file': 'kafka.pdf',\n",
       "   'doc_index': 45},\n",
       "  'similarity_score': 0.40333348512649536,\n",
       "  'distance': 0.5966665148735046,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_4cecf940_48',\n",
       "  'content': '17\\nEnvoi synchrone d’un message Kafka:\\nL’envoi synchrone permet au producer :\\n\\uf0a7 de capturer les exceptions (ex. : erreurs de Kafka ou échec après plusieurs tentatives),\\n\\uf0a7 mais il présente un inconvénient majeur : la performance.\\n\\uf0a7 Inconvénient principal :\\no Le thread reste bloqué en attendant la réponse du broker Kafka (de 2 ms à plusieurs secondes selon la \\ncharge).\\no Pendant ce temps, il ne peut rien faire d’autre (pas même envoyer d’autres messages).\\no Cela entraîne une performance très faible, raison pour laquelle cette méthode n’est pas utilisée en \\nproduction, mais très fréquente dans les exemples pédagogiques.\\nProducers: Construction',\n",
       "  'metadata': {'doc_length': 648,\n",
       "   'total_pages': 50,\n",
       "   'page': 16,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'author': 'elmarouani',\n",
       "   'doc_index': 48,\n",
       "   'page_label': '17',\n",
       "   'source_file': 'kafka.pdf',\n",
       "   'creationdate': '2025-05-20T15:25:22+01:00',\n",
       "   'moddate': '2025-05-20T15:25:22+01:00',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\kafka.pdf',\n",
       "   'title': 'PowerPoint Presentation'},\n",
       "  'similarity_score': 0.23483741283416748,\n",
       "  'distance': 0.7651625871658325,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_3e8021ff_50',\n",
       "  'content': '19\\nProducers: Temps de livraison des messages\\nDepuis Apache Kafka 2.1, ce temps est divisé en deux phases distinctes :\\n\\uf0a7 Temps jusqu’au retour de l’appel asynchrone à send() :\\no Pendant cette phase, le thread de l’application est bloqué (le temps de mise en lot du message, par ex.).\\n\\uf0a7 Temps entre le retour de send() et le déclenchement du callback :\\no Correspond au délai entre le moment où le message a été placé dans un batch à envoyer, et la réponse\\ndu broker Kafka :\\n\\uf0fc succès ,\\n\\uf0fc erreur non-récupérable ,\\n\\uf0fc ou expiration du délai d’envoi maximal configuré.',\n",
       "  'metadata': {'source_file': 'kafka.pdf',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'page': 18,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\kafka.pdf',\n",
       "   'doc_index': 50,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'creationdate': '2025-05-20T15:25:22+01:00',\n",
       "   'author': 'elmarouani',\n",
       "   'total_pages': 50,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'doc_length': 562,\n",
       "   'moddate': '2025-05-20T15:25:22+01:00',\n",
       "   'page_label': '19'},\n",
       "  'similarity_score': 0.12265574932098389,\n",
       "  'distance': 0.8773442506790161,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_c75cacb9_42',\n",
       "  'content': '12\\nKafka: Producers\\nÉtapes principales :\\n1. Sérialisation :\\n\\uf0a7 Kafka convertit la clé et la valeur en tableaux d’octets pour les transmettre via le réseau.\\n2. Choix de la partition :\\n\\uf0a7 Si aucune partition n’est définie, un partitionneur détermine automatiquement à quelle partition envoyer\\nle message (souvent basé sur la clé).\\n3. Batching :\\n\\uf0a7 Kafka groupe les messages destinés à la même partition dans un batch avant de les envoyer.\\n4. Transmission :\\n\\uf0a7 Un thread séparé se charge d’envoyer ces batches aux brokers Kafka.\\n5. Réponse du broker :\\n\\uf0a7 En cas de succès → le broker renvoie un objet RecordMetadata (contenant le topic, la partition et \\nl’offset).\\n\\uf0a7 En cas d’échec → le broker renvoie une erreur, et le producer peut effectuer des tentatives de renvoi\\n(retries) avant d’abandonner.',\n",
       "  'metadata': {'page_label': '12',\n",
       "   'source_file': 'kafka.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'page': 11,\n",
       "   'author': 'elmarouani',\n",
       "   'total_pages': 50,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'doc_index': 42,\n",
       "   'moddate': '2025-05-20T15:25:22+01:00',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\kafka.pdf',\n",
       "   'creationdate': '2025-05-20T15:25:22+01:00',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'doc_length': 790},\n",
       "  'similarity_score': 0.121090829372406,\n",
       "  'distance': 0.878909170627594,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"envoi asynchrone kafka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9efbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74274e3cd86446a986986f1c807c1b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 5 documents before filtering\n",
      "retrieved 3 documents after filtering\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_6106c830_0',\n",
       "  'content': 'Mohamed El Marouani\\nTDIA 2\\nLes fondements du Big Data',\n",
       "  'metadata': {'source_file': 'bgDataFundamentals.pdf',\n",
       "   'creationdate': '2025-02-16T12:01:23+01:00',\n",
       "   'doc_index': 0,\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'author': 'elmarouani',\n",
       "   'total_pages': 20,\n",
       "   'doc_length': 53,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'page_label': '1',\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'file_type': 'pdf',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'page': 0},\n",
       "  'similarity_score': 0.2098078727722168,\n",
       "  'distance': 0.7901921272277832,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_4b4c0ea6_26',\n",
       "  'content': \"Evolution du Big Data\\nBien que le concept de Big Data soit relativement nouveau, la nécessité de gérer des jeux de données volumineux \\nremonte aux années 1960 et 70, avec les premiers data centers et le développement des bases de données \\nrelationnelles.\\n\\uf0a7 Passé: En 2005, on assista à une prise de conscience de la quantité de données que les utilisateurs généraient sur \\nFacebook, YouTube et autres services en ligne. Apache Hadoop, une infrastructure open source créée \\nspécifiquement pour stocker et analyser de grands jeux de données, fut développé cette même année. NoSQL\\ncommença également à être de plus en plus utilisé à cette époque.\\n\\uf0a7 Présent: Le développement d’infrastructures open source, telles qu'Apache Hadoop et, plus récemment, Apache \\nSpark, a été primordial pour la croissance du Big Data, car celles-ci facilitent l’utilisation du Big Data et réduisent les \\ncoûts de stockage. Depuis, le volume du Big Data a explosé. Les utilisateurs génèrent toujours d’énormes quantités\",\n",
       "  'metadata': {'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'source_file': 'bgDataFundamentals.pdf',\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'doc_index': 26,\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'elmarouani',\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'total_pages': 20,\n",
       "   'page': 18,\n",
       "   'doc_length': 994,\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'page_label': '19',\n",
       "   'creationdate': '2025-02-16T12:01:23+01:00'},\n",
       "  'similarity_score': 0.1659451127052307,\n",
       "  'distance': 0.8340548872947693,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_77f82488_29',\n",
       "  'content': 'Le paysage Big Data\\n20',\n",
       "  'metadata': {'page': 19,\n",
       "   'title': 'PowerPoint Presentation',\n",
       "   'creationdate': '2025-02-16T12:01:23+01:00',\n",
       "   'total_pages': 20,\n",
       "   'source': '..\\\\data\\\\pdfs\\\\bgDataFundamentals.pdf',\n",
       "   'page_label': '20',\n",
       "   'source_file': 'bgDataFundamentals.pdf',\n",
       "   'creator': 'Microsoft® PowerPoint® 2016',\n",
       "   'moddate': '2025-02-16T12:01:23+01:00',\n",
       "   'producer': 'Microsoft® PowerPoint® 2016',\n",
       "   'file_type': 'pdf',\n",
       "   'author': 'elmarouani',\n",
       "   'doc_index': 29,\n",
       "   'doc_length': 22},\n",
       "  'similarity_score': 0.04160571098327637,\n",
       "  'distance': 0.9583942890167236,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"what is big data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b78367",
   "metadata": {},
   "source": [
    "#### 3- Augmented generation: giving the context with the query and a prompt to the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a3ac687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "groq_api_key=os.getenv(\"groq_api_key\")\n",
    "\n",
    "llm= ChatGroq(groq_api_key=groq_api_key, \n",
    "              model_name=\"openai/gpt-oss-20b\", \n",
    "              temperature=0.1,\n",
    "              max_tokens=1024)\n",
    "\n",
    "def rag_simple(query, retriever: RAGRetriever, llm, top_k=3):\n",
    "    results=retriever.retrieve(query, top_k=top_k)\n",
    "    context= \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"no relevent context found\"\n",
    "    #generating answer using groq llm:\n",
    "    prompt=f\"\"\"use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {query}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "    response= llm.invoke([prompt.format(context=context, query= query)])\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbc3833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5abd81520f42a3814c3732fa67ff5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 3 documents before filtering\n",
      "retrieved 3 documents after filtering\n",
      "**Types de données :**\n",
      "\n",
      "1. **Données qualitatives (catégorielles)**  \n",
      "   - **Nominales** : catégories sans ordre (ex. sexe, nationalité, couleur).  \n",
      "   - **Ordinales** : catégories avec un ordre ou un classement (ex. classements, notes, niveaux de satisfaction).\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"Quelles sont les differents Types des données\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "813023b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534f26452fdf4357986e3e8eab533a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 3 documents before filtering\n",
      "retrieved 3 documents after filtering\n",
      "Les 5 caractéristiques clés du Big Data :  \n",
      "\n",
      "1. **Volume** – quantités massives de données.  \n",
      "2. **Vitesse (Velocity)** – génération et traitement rapides.  \n",
      "3. **Variété** – diversité des formats (texte, image, vidéo, etc.).  \n",
      "4. **Veracité** – qualité et fiabilité des données.  \n",
      "5. **Valeur (Value)** – capacité à produire des insights utiles.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"donne moi les 5 caracteristiques de la big data\", rag_retriever, llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d8a73",
   "metadata": {},
   "source": [
    "### Enhanced rag returning answer with the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "855b8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advenced(query, retriever: RAGRetriever, llm, top_k=5, min_score=0.0, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features which returns the answer with the sources documents:\n",
    "    Returns answer, sources, confidence score, and full context if required   \n",
    "    \"\"\"\n",
    "    results= retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'no relevent context found in the provided files', 'sources':[], 'confidence': 0.0, 'context': \"\"}\n",
    "    context= \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources=[{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:120]+\"...\"\n",
    "    } for doc in results]\n",
    "    confidence= max([doc['similarity_score'] for doc in results])\n",
    "    #generating the answer:\n",
    "    prompt=f\"\"\"Use this following context to answer the question concisely and precisely\\nContext:\\n{context}\\nQuestion:\\n{query}\\n\\nAnswer:\"\"\"\n",
    "    response=llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output={\n",
    "        'answer': response.content,\n",
    "        'source': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context']=context\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4db62b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccb04c7c01943469cce34a5dd81d9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 3 documents before filtering\n",
      "retrieved 3 documents after filtering\n",
      "answer: Les 5 caractéristiques classiques du Big Data — les « V » — sont :\n",
      "\n",
      "1. **Volume** – quantité massive de données.  \n",
      "2. **Velocity** – vitesse de génération et de traitement.  \n",
      "3. **Variety** – diversité des formats (texte, image, vidéo, etc.).  \n",
      "4. **Veracity** – qualité, fiabilité et précision des données.  \n",
      "5. **Value** – valeur ajoutée et insights que l’on peut en extraire.\n",
      "sources:[{'source': 'bgDataFundamentals.pdf', 'page': 0, 'score': 0.5941737592220306, 'preview': 'Mohamed El Marouani\\nTDIA 2\\nLes fondements du Big Data...'}, {'source': 'bgDataFundamentals.pdf', 'page': 1, 'score': 0.35692936182022095, 'preview': '2\\n1. Qu’est ce que les données?\\n2. Types des données\\n3. Impact des données\\n4. Caractéristiques des données (les V)\\n5. Da...'}, {'source': 'bgDataFundamentals.pdf', 'page': 18, 'score': 0.34568339586257935, 'preview': 'Evolution du Big Data\\nBien que le concept de Big Data soit relativement nouveau, la nécessité de gérer des jeux de donné...'}]\n",
      "confidence:0.5941737592220306\n",
      "context prev:Mohamed El Marouani\n",
      "TDIA 2\n",
      "Les fondements du Big Data\n",
      "\n",
      "2\n",
      "1. Qu’est ce que les données?\n",
      "2. Types des données\n",
      "3. Impact des données\n",
      "4. Caractéristiques des données (les V)\n",
      "5. Data Journey\n",
      "6. Qu’est ce que Big Data?\n",
      "7. Big Data: cas d’utilisation\n",
      "8. Evolution du Big Data\n",
      "9. Paysage du Big Data\n",
      "\n",
      "Evolution du Big Data\n",
      "Bien que le concept de Big Data soit relativement nouveau, la nécessité de gérer des jeux de données volumineux \n",
      "remonte aux années 1960 et 70, avec les premiers data centers et le développement des bases de données \n",
      "relationnelles.\n",
      " Passé: En 2005, on assista à une prise de conscience de la quantité de données que les utilisateurs généraient sur \n",
      "Facebook, YouTube et autres services en ligne. Apache Hadoop, une infrastructure open source créée \n",
      "spécifiquement pour stocker et analyser de grands jeux de données, fut développé cette même année. NoSQL\n",
      "commença également à être de plus en plus utilisé à cette époque.\n",
      " Présent: Le développement d’infrastructures open source, telles qu'Apache Hadoop et, plus récemment, Apache \n",
      "Spark, a été primordial pour la croissance du Big Data, car celles-ci facilitent l’utilisation du Big Data et réduisent les \n",
      "coûts de stockage. Depuis, le volume du Big Data a explosé. Les utilisateurs génèrent toujours d’énormes quantités\n"
     ]
    }
   ],
   "source": [
    "result=rag_advenced(\"donne moi les 5 caracteristiques de la big data\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "\n",
    "print(f\"answer: {result['answer']}\")\n",
    "print(f\"sources:{result['source']}\")\n",
    "print(f\"confidence:{result['confidence']}\")\n",
    "print(f\"context prev:{result['context']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32c8f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f40c7115444472bcf95a947883390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 3 documents before filtering\n",
      "retrieved 2 documents after filtering\n",
      "answer: Le stage a duré **deux mois**.\n",
      "sources:[{'source': 'rapport_de_stage.pdf', 'page': 7, 'score': 0.06437516212463379, 'preview': 'Chapitre 1\\nContexte et Objectifs du Stage\\n1.1 Présentation de l’Entreprise\\nYazaki est un leader mondial dans la fabricat...'}, {'source': 'rapport_de_stage.pdf', 'page': 6, 'score': -0.028636693954467773, 'preview': 'Introduction\\nCe stage de deux mois réalisé au sein de Yazaki Meknes avait pour objectif\\nprincipal le développement d’une...'}]\n",
      "confidence:0.06437516212463379\n",
      "context prev:Chapitre 1\n",
      "Contexte et Objectifs du Stage\n",
      "1.1 Présentation de l’Entreprise\n",
      "Yazaki est un leader mondial dans la fabrication de systèmes de câblage\n",
      "automobile et de instruments de mesure. Fondée en 1929 au Japon, l’entreprise\n",
      "emploie plus de 200 000 personnes dans 45 pays et possède une forte présence dans\n",
      "le secteur automobile avec des innovations technologiques constantes.\n",
      "Yazaki est une entreprise spécialisée dans la conception et la fabrication de systèmes\n",
      "électriques et électroniques pour l’industrie automobile. Fondée en 1941 au Japon, elle\n",
      "emploie aujourd’hui plus de 190 000 collaborateurs et possède une présence internationale\n",
      "avec des implantations dans plus de 45 pays. Les activités principales de Yazaki incluent la\n",
      "production de faisceaux de câblage, de systèmes de distribution électrique, de composants\n",
      "électroniques et de solutions pour la gestion de l’énergie dans les véhicules.\n",
      "Figure 1.1– Vue d’une desccription de l’histoire de Yazaki\n",
      "7\n",
      "\n",
      "Introduction\n",
      "Ce stage de deux mois réalisé au sein de Yazaki Meknes avait pour objectif\n",
      "principal le développement d’une application complète de gestion d’imprimantes\n",
      "réseau. Dans un contexte où la gestion des ressources d’impression constitue un\n",
      "enjeu opérationnel majeur, cette application vise à centraliser le monitoring, la\n",
      "découverte automatique et la gestion des imprimantes connectées au réseau de\n",
      "l’entreprise.\n",
      "Dans le cadre de ma formation en Transformation Digitale et Intelligence Artificielle,\n",
      "j’ai effectué un stage de deux mois au sein de l’entreprise Yazaki, spécialisée dans la\n",
      "conception et la fabrication de systèmes électriques et électroniques pour l’industrie au-\n",
      "tomobile. Ce stage avait pour objectif principal la conception et le développement d’une\n",
      "application de gestion d’imprimantes réseau.\n",
      "Lagestionduparcd’imprimantesreprésenteunenjeuimportantpourlesentreprisesde\n",
      "taille moyenne à grande. Le suivi des compteurs de pages, de la connectivitée, des niveaux\n"
     ]
    }
   ],
   "source": [
    "result=rag_advenced(\"quelle est la durrée du stage effectué au sein de l'entreprise YAZAKI\", rag_retriever, llm, top_k=3, min_score=-0.1, return_context=True)\n",
    "\n",
    "print(f\"answer: {result['answer']}\")\n",
    "print(f\"sources:{result['source']}\")\n",
    "print(f\"confidence:{result['confidence']}\")\n",
    "print(f\"context prev:{result['context']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c27be253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving documents for the query: '{query}' with top k: {top_k}, and the score threshold of similarity: {score_threshold}\n",
      "generating embeddings for 1 texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c61bf5a19c34fefba013ecf2c6e9968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated embeddings with shape: (1, 384)\n",
      "retrieved 3 documents before filtering\n",
      "retrieved 3 documents after filtering\n",
      "answer: **Présentation de l’entreprise : Yazaki**\n",
      "\n",
      "Yazaki est un fournisseur mondial de composants et de systèmes automobiles.  \n",
      "Spécialisée dans les câblages, les connecteurs, les systèmes de gestion de l’énergie et les solutions de communication embarquée, l’entreprise conçoit, fabrique et distribue des pièces destinées aux constructeurs automobiles et aux équipementiers. Elle opère à l’échelle internationale, avec des sites de production et de R&D répartis dans le monde entier, et se distingue par son engagement en matière d’innovation, de qualité et de durabilité.\n",
      "sources:[{'source': 'rapport_de_stage.pdf', 'page': 6, 'score': -0.1468815803527832, 'preview': 'renseignant l’adresse d’un ou de plusieurs sous-réseaux, ce qui permet d’élargir la portée\\nde la supervision et d’obteni...'}, {'source': 'rapport_de_stage.pdf', 'page': 1, 'score': -0.25188279151916504, 'preview': 'Remerciements\\nJe tiens à exprimer ma profonde gratitude à toutes les personnes qui ont contribué,\\nde près ou de loin, à ...'}, {'source': 'rapport_de_stage.pdf', 'page': 2, 'score': -0.27975690364837646, 'preview': 'Table des matières\\nRemerciements 1\\nListe des figures 4\\nListe des tableaux 5\\nIntroduction 6\\n1 Contexte et Objectifs du St...'}]\n",
      "confidence:-0.1468815803527832\n",
      "context prev:renseignant l’adresse d’un ou de plusieurs sous-réseaux, ce qui permet d’élargir la portée\n",
      "de la supervision et d’obtenir une vision centralisée du parc d’impression.\n",
      "Ce rapport présente en détail le travail réalisé, depuis l’analyse des besoins jusqu’au\n",
      "déploiement de l’application, en passant par la conception technique et le développement.\n",
      "6\n",
      "\n",
      "Remerciements\n",
      "Je tiens à exprimer ma profonde gratitude à toutes les personnes qui ont contribué,\n",
      "de près ou de loin, à la réussite de ce stage et à la réalisation de ce projet.\n",
      "Mes remerciements les plus sincères s’adressent à mon encadrantMr. Bouya\n",
      "Aminepour son accompagnement constant, ses conseils précieux et sa patience.\n",
      "Sa supervision a été déterminante pour l’accomplissement de mon projet et\n",
      "l’acquisition de nouvelles compétences.\n",
      "J’exprime ma gratitude à l’ensemble du personnel de l’entreprise Yazaki pour leur\n",
      "accueil chaleureux, leur disponibilité et les connaissances qu’ils ont bien voulu partager\n",
      "avec moi.\n",
      "Enfin, un grand merci à ma famille et à mes amis pour leur soutien et leurs encoura-\n",
      "gements tout au long de ce parcours.\n",
      "1\n",
      "\n",
      "Table des matières\n",
      "Remerciements 1\n",
      "Liste des figures 4\n",
      "Liste des tableaux 5\n",
      "Introduction 6\n",
      "1 Contexte et Objectifs du Stage 7\n",
      "1.1 Présentation de l’Entreprise . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "1.2 Contexte du Projet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "1.3 Objectifs du Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n",
      "2 Analyse des Besoins et Conception 9\n",
      "2.1 Analyse Fonctionnelle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Analyse Technique . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.3 Conception UML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "2.3.1 Diagramme de Cas d’Utilisation . . . . . . . . . . . . . . . . . . . . 12\n",
      "2.3.2 Diagramme de Classes . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "3 Réalisation Technique 14\n",
      "3.1 Environnement de Développement . . . . . . . . . . . . . . . . . . . . . . . 14\n"
     ]
    }
   ],
   "source": [
    "result=rag_advenced(\"Présentation de l’Entreprise\", rag_retriever, llm, top_k=3, min_score=-1, return_context=True)\n",
    "\n",
    "print(f\"answer: {result['answer']}\")\n",
    "print(f\"sources:{result['source']}\")\n",
    "print(f\"confidence:{result['confidence']}\")\n",
    "print(f\"context prev:{result['context']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec11df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
